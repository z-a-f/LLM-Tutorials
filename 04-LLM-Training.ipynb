{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f5f76d-ebe8-49e1-9835-11a6e0024797",
   "metadata": {},
   "source": [
    "# Session 4, Part 1 – Training a Transformer-Based Text Generator\n",
    "\n",
    "In this session, we will gain **hands-on experience** training a **Transformer-based text generator** (e.g., a GPT-like model) on a small dataset. We’ll cover:\n",
    "\n",
    "- **Model Architecture**: building a minimal decoder-only Transformer with causal masking.  \n",
    "- **Training Routines**: batch scheduling, gradient accumulation, and optimization.  \n",
    "- **Inference Techniques**: *greedy decoding*, *temperature scaling*, and *top-k* sampling.\n",
    "\n",
    "By the end, you’ll be able to **train** a basic text-generating Transformer and **experiment** with different hyperparameters and inference strategies to observe how they affect text quality.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Text Generation](#introduction)\n",
    "2. [Dataset Preparation](#dataset-prep)\n",
    "3. [Building a GPT-like Model](#gpt-model)\n",
    "   - [Causal Masking](#causal-masking)\n",
    "   - [Model Definition](#model-def)\n",
    "   - [Training Routine (Optimization & Strategies)](#training-routine)\n",
    "4. [Inference Techniques](#inference)\n",
    "   - [Greedy Decoding vs. Random Sampling](#greedy-random)\n",
    "   - [Temperature Scaling](#temperature)\n",
    "   - [Top-k Sampling](#topk)\n",
    "5. [Practical Exercises](#exercises)\n",
    "6. [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d36e4bf-8b64-4541-b40a-2d2e54fb71c8",
   "metadata": {},
   "source": [
    "## <a id=\"introduction\"></a>1. Introduction to Text Generation\n",
    "\n",
    "**Text generation** is a fundamental language modeling task. We model $P(x_{t} \\mid x_{0}, \\dots, x_{t-1})$ to predict the next token given the previous tokens. **Transformer**-based **decoder-only** architectures (like GPT) excel at *autoregressive* generation for tasks like writing paragraphs, summarizing, or code generation.\n",
    "\n",
    "**Key Insight**:  \n",
    "- We mask out future tokens so that at each time step $t$, the model can’t “see” tokens $t+1, t+2, \\dots$.  \n",
    "- This is often referred to as **causal language modeling** or **autoregressive** language modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d69ba0-515e-4415-bdd1-ff82e5f5acfc",
   "metadata": {},
   "source": [
    "\n",
    "### Why Use a Transformer (GPT-like) for Generation?\n",
    "\n",
    "1. **Parallel Computation** of the hidden states (although we generate tokens one-by-one at inference time, training can process full sequences in parallel).\n",
    "2. **Long-range Dependencies**: Self-attention can capture wide contexts better than many RNNs.\n",
    "3. **Scalability**: We can scale up to large models (e.g., GPT-2, GPT-3) and achieve impressive quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2bb79-fb2c-4f08-87bd-ea13c7c82626",
   "metadata": {},
   "source": [
    "## <a id=\"dataset-prep\"></a>2. Dataset Preparation\n",
    "\n",
    "To demonstrate, we’ll use a **small text dataset**: **Tiny Shakespeare**. You can adapt these steps to any text corpus.\n",
    "\n",
    "### 2.1 Download and Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0e96986-d5aa-4be1-8aab-dfb1773e2665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394\n",
      "Sample:\n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Download the tiny shakespeare dataset if not present\n",
    "if not os.path.exists(\"tiny_shakespeare.txt\"):\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    r = requests.get(url)\n",
    "    with open(\"tiny_shakespeare.txt\", \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "with open(\"tiny_shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "print(\"Length of text:\", len(text_data))\n",
    "print(\"Sample:\\n\", text_data[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa97ab-8864-4ac7-852f-be6e35f3b3b5",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Character-Level Tokenization\n",
    "\n",
    "We’ll keep it simple: **character-level** tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43bf897e-ca4c-44b6-99e7-ec27067f2691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 65\n",
      "Chars: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k']\n",
      "Example of mapped indices: [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56]\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text_data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Chars:\", chars[:50])\n",
    "\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for ch, i in char2idx.items()}\n",
    "\n",
    "# Convert entire text to indices\n",
    "data_indices = [char2idx[ch] for ch in text_data]\n",
    "print(\"Example of mapped indices:\", data_indices[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ef22a-6d7a-4943-a88c-24f04973c687",
   "metadata": {},
   "source": [
    "### 2.3 Creating (x, y) Sequences\n",
    "\n",
    "We want to split data into sequences of a fixed length (`seq_length`), with target `y` being the next character for each position in `x`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b72eb9f6-d155-47fa-9bc4-6a369812a07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 544\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=64):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.num_samples = len(self.data) // self.seq_len - 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.seq_len\n",
    "        x = self.data[start_idx : start_idx + self.seq_len]\n",
    "        y = self.data[start_idx + 1 : start_idx + self.seq_len + 1]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "seq_length = 64\n",
    "dataset = CharDataset(data_indices, seq_len=seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "print(\"Number of batches:\", len(dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659bdd2e-5139-4423-ac83-bbe92f52196f",
   "metadata": {},
   "source": [
    "## <a id=\"gpt-model\"></a>3. Building a GPT-like Model\n",
    "\n",
    "### <a id=\"causal-masking\"></a>3.1 Causal Masking\n",
    "\n",
    "A **causal mask** ensures each position in the sequence can only attend to itself and previous positions. This is typically implemented with a **triangular** (lower-triangular) matrix.\n",
    "\n",
    "At each time step $t$, the model sees positions $\\leq t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2279f5f4-398c-46c8-b7fa-bf8be8d7df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "def generate_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Returns a (seq_len, seq_len) mask\n",
    "    where positions j>i are set to False, preventing 'future' attention.\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask == 1  # boolean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a795e783-9a8b-4ea5-b025-dcb4167bba09",
   "metadata": {},
   "source": [
    "### <a id=\"model-def\"></a>3.2 Model Definition\n",
    "\n",
    "**GPT** is essentially:\n",
    "1. An **embedding** layer (tokens + optional positions).\n",
    "2. A stack of **decoder blocks** with causal self-attention.\n",
    "3. A final linear layer that outputs a distribution over the vocabulary.\n",
    "\n",
    "#### 3.2.1 Self-Attention (Causal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "114c856c-2c74-4529-b8ac-e66e17ed39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadCausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.shape\n",
    "        # Project to queries, keys, values\n",
    "        q = self.Wq(x).view(B, T, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
    "        k = self.Wk(x).view(B, T, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
    "        v = self.Wv(x).view(B, T, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
    "        \n",
    "        # Scaled dot-product\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            # mask shape => (T, T), broadcast to (B, num_heads, T, T)\n",
    "            # positions with mask = False => set to -inf\n",
    "            scores = scores.masked_fill(~mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        \n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        \n",
    "        # reshape back\n",
    "        out = out.permute(0,2,1,3).contiguous().view(B, T, C)\n",
    "        out = self.out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ecf5a2-a2c7-4109-a094-ffd8ff7c3f7b",
   "metadata": {},
   "source": [
    "#### 3.2.2 Decoder Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c705ac8-fbee-4116-ae10-98f9abb5cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadCausalSelfAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, d_model),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout(ff_out)\n",
    "        x = self.norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c0631-6ef1-45f1-8db3-9cd408ea742f",
   "metadata": {},
   "source": [
    "#### 3.2.3 GPT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78d80671-a84a-43f3-9c2b-3f595f44d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, num_layers=4, num_heads=4, ff_dim=1024, max_len=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(d_model, num_heads, ff_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        # Token + positional embeddings\n",
    "        tok_emb = self.embedding(x)\n",
    "        positions = torch.arange(0, T, device=x.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embedding(positions)\n",
    "        \n",
    "        hidden = tok_emb + pos_emb\n",
    "        \n",
    "        mask = generate_causal_mask(T).to(x.device)\n",
    "        for block in self.blocks:\n",
    "            hidden = block(hidden, mask=mask)\n",
    "        \n",
    "        hidden = self.ln_f(hidden)\n",
    "        logits = self.fc_out(hidden)  # shape (B, T, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5df89c-2ca3-470f-b133-59d0119293c8",
   "metadata": {},
   "source": [
    "## <a id=\"training-routine\"></a>3.3 Training Routine (Optimization & Strategies)\n",
    "\n",
    "We’ll use **AdamW** and **CrossEntropy** (typical for LM). For demonstration, we’ll run a few epochs on the small dataset.\n",
    "\n",
    "\n",
    "**Possible Training Extensions**:\n",
    "- **Gradient Accumulation** if memory is limited (accumulate gradients across multiple batches before `optimizer.step()`).\n",
    "- **Learning Rate Schedules** (cosine decay, linear warmup, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dde93607-672d-40cf-af27-a5f807a6497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/5 - Loss: 2.0577\n",
      "Epoch 2/5 - Loss: 1.6229\n",
      "Epoch 3/5 - Loss: 1.5208\n",
      "Epoch 4/5 - Loss: 1.4666\n",
      "Epoch 5/5 - Loss: 1.4302\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = GPT(vocab_size, d_model=256, num_layers=4, num_heads=4).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_batch)  # shape (B, T, vocab_size)\n",
    "        \n",
    "        # Flatten\n",
    "        B, T, V = logits.shape\n",
    "        loss = criterion(logits.view(B*T, V), y_batch.view(B*T))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197dae6-dcb9-4e1a-9099-462b9d605c50",
   "metadata": {},
   "source": [
    "## <a id=\"inference\"></a>4. Inference Techniques\n",
    "\n",
    "Once trained, we can generate text. Key methods:\n",
    "\n",
    "1. **Greedy Decoding**: pick `argmax` at each step.  \n",
    "2. **Random Sampling**: sample from the probability distribution (adds variety).  \n",
    "3. **Temperature Scaling**: modifies distribution sharpness or spread.  \n",
    "4. **Top-k** (or **Top-p**) sampling: restricts sampling to top-k probable tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a2fc1-56d2-43c3-8688-6308e87d7e21",
   "metadata": {},
   "source": [
    "### <a id=\"greedy-random\"></a>4.1 Greedy Decoding vs. Random Sampling\n",
    "\n",
    "- **Greedy Decoding**:  \n",
    "  - At each time step, select the token with the highest probability.  \n",
    "  - Pros: consistent, high-probability outcome.  \n",
    "  - Cons: can get stuck in repetitive loops.\n",
    "\n",
    "- **Random Sampling**:  \n",
    "  - Sample next token according to the predicted probability distribution.  \n",
    "  - Pros: more diverse text, can escape repetitive loops.  \n",
    "  - Cons: might lead to incoherent tangents if probabilities are too spread out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222a549-3909-436f-9b12-699f89812fb6",
   "metadata": {},
   "source": [
    "### <a id=\"temperature\"></a>4.2 Temperature Scaling\n",
    "\n",
    "$$\n",
    "p_i^\\text{(scaled)} \\propto \\exp\\left(\\frac{\\log p_i}{\\text{temp}}\\right)\n",
    "$$\n",
    "\n",
    "- `temp < 1.0` => more confident (peaky distribution).  \n",
    "- `temp > 1.0` => more creativity and randomness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904ba7d-5383-4a5b-9404-b958d18cd766",
   "metadata": {},
   "source": [
    "### <a id=\"topk\"></a>4.3 Top-k Sampling\n",
    "\n",
    "- Sort the logits by probability.\n",
    "- Keep only the top k tokens, set others to 0 (or -inf in log space).\n",
    "- Renormalize and sample.\n",
    "\n",
    "*(**Top-p** / nucleus sampling is similar but chooses a dynamic set of tokens until their cumulative probability >= p.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0b9a8-9eff-49d8-91e5-3d4f0b0e253c",
   "metadata": {},
   "source": [
    "### Sample Generation Code\n",
    "\n",
    "**Observations**:\n",
    "- Vary **temperature**: higher => more diverse but less coherent.\n",
    "- Vary **top_k**: small k => conservative, large k => more variety.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6773fbc-1784-451d-a618-0415250869f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " ROMEO:\n",
      "The more sometime and man.\n",
      "I cannot to the married of thisenckene oronous ouner thes sthe warous therereres, me ale ones ouchest te thalen wishes thaldous wanigedoumathere sthare w winereranous alere\n"
     ]
    }
   ],
   "source": [
    "###### import torch.nn.functional as F\n",
    "\n",
    "def generate_text(\n",
    "    model, \n",
    "    start_text=\"ROMEO:\", \n",
    "    max_new_tokens=100, \n",
    "    temperature=1.0, \n",
    "    top_k=None\n",
    "):\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert start text to indices\n",
    "    input_ids = torch.tensor([char2idx[ch] for ch in start_text], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Forward pass\n",
    "        logits = model(input_ids)  # shape: (1, current_len, vocab_size)\n",
    "        logits = logits[:, -1, :]  # last timestep => shape (1, vocab_size)\n",
    "        \n",
    "        # Scale by temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # (Optional) top-k\n",
    "        if top_k is not None:\n",
    "            v, ix = torch.topk(logits, top_k)\n",
    "            probs = torch.zeros_like(logits).scatter_(1, ix, torch.softmax(v, dim=-1))\n",
    "        else:\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Sample from distribution\n",
    "        next_id = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "        # Append to input\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[next_id]], device=device)], dim=1)\n",
    "    \n",
    "    out_seq = input_ids[0].tolist()\n",
    "    return \"\".join(idx2char[idx] for idx in out_seq)\n",
    "\n",
    "# Example usage after training:\n",
    "generated = generate_text(model, start_text=\"ROMEO:\", max_new_tokens=200, temperature=0.8, top_k=3)\n",
    "print(\"Generated Text:\\n\", generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded9e92-caa5-4c8f-ad38-f8a64e5742f8",
   "metadata": {},
   "source": [
    "## <a id=\"exercises\"></a>5. Practical Exercises\n",
    "\n",
    "### Exercise 1: Hyperparameter Tweaks\n",
    "1. Change `seq_length`, `d_model`, `num_layers`, `num_heads`.  \n",
    "2. Observe how training speed and memory usage are affected.  \n",
    "3. Compare generated text for different settings.\n",
    "\n",
    "### Exercise 2: Gradient Accumulation\n",
    "1. If you have limited GPU memory, implement a small gradient accumulation loop (accumulate gradients over N mini-batches before calling `optimizer.step()`).  \n",
    "2. Verify you get similar results to a larger batch size (without accumulation).\n",
    "\n",
    "### Exercise 3: Inference Experiments\n",
    "1. Generate text using **greedy decoding**. Save a sample.  \n",
    "2. Generate text using **random sampling** with `temperature=1.0`. Compare the style.  \n",
    "3. Try `temperature=0.6` vs. `temperature=1.2` to see how it changes coherence and creativity.  \n",
    "4. Set `top_k=5` vs. `top_k=50`, observe differences in repetition or diversity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d9fd7-180c-463b-ac99-6c0a819b6675",
   "metadata": {},
   "source": [
    "## <a id=\"conclusion\"></a>6. Conclusion\n",
    "\n",
    "**Summary**:  \n",
    "In this **Session 4, Part 1**, we built a **Transformer-based text generator** from scratch, focusing on:\n",
    "- **Decoder-only architecture** with **causal masking**.\n",
    "- **Autoregressive training** using `(x,y)` pairs shifted by one token.\n",
    "- **Inference methods** (greedy vs. random sampling, temperature, top-k) for controlling generation style.\n",
    "\n",
    "**Key Takeaways**:\n",
    "1. The **causal mask** is critical for ensuring the model only attends to past tokens.  \n",
    "2. **Hyperparameters** (hidden size, layers, heads) and training routines (optimizer, batch size) can drastically affect both training speed and generated text quality.  \n",
    "3. **Inference sampling** parameters can move your model from repetitive to more creative outputs.\n",
    "\n",
    "**Next Steps**:\n",
    "- If you’d like to refine generation quality, train longer or on a bigger dataset. \n",
    "- For advanced features, see **Session 4, Part 2** on **large-scale pretraining** and **fine-tuning** strategies (pipeline parallelism, mixed precision, etc.).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69a561-c73b-4a12-8563-b4ffe48d63c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
