{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803b7e6f-657c-41de-aaba-6d2e2138aeb2",
   "metadata": {},
   "source": [
    "# Session 1: Data Processing and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec3ac8-8799-47c8-a33c-8226ef6027f4",
   "metadata": {},
   "source": [
    "Fundamental NLP data preprocessing and tokenization techniques.\n",
    "\n",
    "<img src=\"img/sentence_example_2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dda4a2-77c7-4caf-b023-3f8499a9cd87",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Generally, it is always a good idea to preprocess the text to make it more readable.\n",
    "The preprocessing steps is intended to make it less noisy.\n",
    "\n",
    "**Lowercasing**\n",
    "\n",
    "The most obvious step is to convert the characters to a lowercase.\n",
    "That preprocessing step seems logical, as words (often) don't care if it's uppercase or not\n",
    "\n",
    "<img src=\"img/hello_world_lowercase.png\"/>\n",
    "\n",
    "**Removing punctuation**\n",
    "\n",
    "Punctuation is important in language.\n",
    "However, often times it adds to the complexity, with little to no added semantic value.\n",
    "We can remove all the punctuation, making the complexity much lower.\n",
    "\n",
    "<img src=\"img/hello_world_no_punctuation.png\"/>\n",
    "\n",
    "\n",
    "**Stemming**\n",
    "\n",
    "Conversion of the words to their \"base\" forms by removing suffixes (and sometimes prefixes).\n",
    "<img src=\"img/stemming.png\"/>\n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "Conversion of the words to their \"base\" form using some morphological rules: $\\{\\text{am}, \\text{are}, \\text{is}\\} \\Rightarrow \\text{be}$\n",
    "<img src=\"img/lemmatization.png\"/>\n",
    "\n",
    "**Tokenization**\n",
    "\n",
    "Splitting the words/sentences into smaller components based on some rules.\n",
    "Note that the rules might be a combination of all other methods, so this terminology is definitely overloaded.\n",
    "\n",
    "In general, when we talk about tokenization, we refer to a set of rules that are applied to the text in order to split it into smaller pieces.\n",
    "Depending on the rules, the tokenized text would change as\n",
    "\n",
    "<img src=\"img/tokenization_bpe_simple.png\"/>\n",
    "\n",
    "For the purpose of this setup, we will call all of the preprocessing steps as just **Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2150b86-1f6b-4d04-a222-7a5c1de07ed7",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Once the text is cleaned up, there are many different ways you can tokenize it.\n",
    "In general, everything boils down to how you would **split** the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd3e53c-a4f2-4469-b132-b3242165bf2f",
   "metadata": {},
   "source": [
    "### Word-Level Tokenization\n",
    "\n",
    "One of the simplest ways to tokenize is to just split the text into words.\n",
    "Depending on the rules, you might split the text using spaces (blank characters) or punctuation:\n",
    "\n",
    "<img src=\"img/tokenization_words.png\"/>\n",
    "\n",
    "This tokenization is capable of distibguishing each individual word.\n",
    "However, the issue is that it might require a very large dictionary to store the words and their numerical representations.\n",
    "\n",
    "### Character-Level Tokenization\n",
    "\n",
    "Another extreme is to split the text by individual characters.\n",
    "\n",
    "<img src=\"img/tokenization_chars.png\"/>\n",
    "\n",
    "This tokenization mechanism has relatively small dictionary size.\n",
    "However, it relies on the model to understand the difference between different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6606dc21-2595-4332-ac6d-8252a0922ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello, World! NLP is amazing.\n",
      "Processed: hello world nlp is amazing\n",
      "Word Tokens: ['hello', 'world', 'nlp', 'is', 'amazing']\n",
      "Character Tokens: ['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 'n', 'l', 'p', ' ', 'i', 's', ' ', 'a', 'm', 'a', 'z', 'i', 'n', 'g']\n",
      "\n",
      "\n",
      "\n",
      "BPE Tokens: ['This', 'is', 'an', 'example', 'sentence', '.']\n",
      "Tokenized Dataset: [['This', 'is', 'an', 'example', 'sentence', '.'], ['Another', 'sentence', 'goes', 'here', '.']]\n"
     ]
    }
   ],
   "source": [
    "### Session 1: Data Processing and Tokenization\n",
    "\n",
    "# Introduction\n",
    "# This notebook covers fundamental NLP data preprocessing and tokenization techniques.\n",
    "# It includes both theoretical explanations and practical exercises.\n",
    "\n",
    "## Section 1: Understanding Text Preprocessing\n",
    "\n",
    "### What is Text Preprocessing?\n",
    "# Text preprocessing is a crucial step in NLP to ensure data consistency, reduce noise, and optimize model performance.\n",
    "\n",
    "# Common preprocessing steps:\n",
    "# - Lowercasing\n",
    "# - Removing punctuation\n",
    "# - Tokenization (splitting text into smaller units)\n",
    "# - Removing stopwords (optional, task-dependent)\n",
    "# - Normalization (e.g., stemming, lemmatization)\n",
    "\n",
    "# Let's implement some basic text preprocessing:\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def basic_preprocessing(text: str) -> str:\n",
    "    \"\"\"Performs basic text preprocessing.\"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "text_sample = \"Hello, World! NLP is amazing.\"\n",
    "print(\"Original:\", text_sample)\n",
    "print(\"Processed:\", basic_preprocessing(text_sample))\n",
    "\n",
    "## Section 2: Tokenization\n",
    "\n",
    "### What is Tokenization?\n",
    "# Tokenization is the process of breaking text into smaller units (tokens).\n",
    "# These can be words, subwords, or characters.\n",
    "\n",
    "# Common types of tokenization:\n",
    "# - Word-based tokenization (splitting on spaces)\n",
    "# - Character-level tokenization\n",
    "# - Subword tokenization (e.g., Byte-Pair Encoding, WordPiece, SentencePiece)\n",
    "\n",
    "# Let's implement simple word-based and character-level tokenization:\n",
    "\n",
    "def word_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Splits text into words.\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "def char_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Splits text into characters.\"\"\"\n",
    "    return list(text)\n",
    "\n",
    "# Example usage:\n",
    "processed_text = basic_preprocessing(text_sample)\n",
    "print(\"Word Tokens:\", word_tokenize(processed_text))\n",
    "print(\"Character Tokens:\", char_tokenize(processed_text))\n",
    "\n",
    "## Section 3: Subword Tokenization\n",
    "\n",
    "### What is Subword Tokenization?\n",
    "# Subword tokenization helps handle rare words and reduce vocabulary size.\n",
    "# Examples: Byte-Pair Encoding (BPE), WordPiece, and SentencePiece.\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "\n",
    "def train_bpe_tokenizer(texts: List[str]):\n",
    "    \"\"\"Trains a simple BPE tokenizer on a given text corpus.\"\"\"\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "    tokenizer.train_from_iterator(texts, trainer)\n",
    "    return tokenizer\n",
    "\n",
    "# Example usage with Tiny Shakespeare dataset (or any small dataset you provide)\n",
    "dataset = [\"This is an example sentence.\", \"Another sentence goes here.\"]\n",
    "bpe_tokenizer = train_bpe_tokenizer(dataset)\n",
    "\n",
    "# Encoding a sample sentence\n",
    "encoded = bpe_tokenizer.encode(\"This is an example sentence.\")\n",
    "print(\"BPE Tokens:\", encoded.tokens)\n",
    "\n",
    "## Section 4: Preparing a Dataset for Future Use\n",
    "\n",
    "### Creating a Tokenized Dataset\n",
    "# We will process a small dataset and tokenize it for use in future sessions.\n",
    "\n",
    "def prepare_tokenized_dataset(texts: List[str], tokenizer):\n",
    "    \"\"\"Tokenizes and encodes a dataset using the given tokenizer.\"\"\"\n",
    "    return [tokenizer.encode(text).tokens for text in texts]\n",
    "\n",
    "# Tokenizing our dataset\n",
    "tokenized_dataset = prepare_tokenized_dataset(dataset, bpe_tokenizer)\n",
    "print(\"Tokenized Dataset:\", tokenized_dataset)\n",
    "\n",
    "# Saving the tokenizer for reuse\n",
    "bpe_tokenizer.save(\"bpe_tokenizer.json\")\n",
    "\n",
    "# Conclusion: In this session, we covered:\n",
    "# - Basic text preprocessing\n",
    "# - Different types of tokenization\n",
    "# - Implementing word, character, and subword tokenization\n",
    "# - Creating a small dataset for future use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf037295-a488-4a39-9aba-d8ab007631ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
