{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803b7e6f-657c-41de-aaba-6d2e2138aeb2",
   "metadata": {},
   "source": [
    "# Session 1: Data Processing and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec3ac8-8799-47c8-a33c-8226ef6027f4",
   "metadata": {},
   "source": [
    "Fundamental NLP data preprocessing and tokenization techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dda4a2-77c7-4caf-b023-3f8499a9cd87",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Consider an example sentence: `\"Hello World! NLP is amazing.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6606dc21-2595-4332-ac6d-8252a0922ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello, World! NLP is amazing.\n",
      "Processed: hello world nlp is amazing\n",
      "Word Tokens: ['hello', 'world', 'nlp', 'is', 'amazing']\n",
      "Character Tokens: ['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 'n', 'l', 'p', ' ', 'i', 's', ' ', 'a', 'm', 'a', 'z', 'i', 'n', 'g']\n",
      "\n",
      "\n",
      "\n",
      "BPE Tokens: ['This', 'is', 'an', 'example', 'sentence', '.']\n",
      "Tokenized Dataset: [['This', 'is', 'an', 'example', 'sentence', '.'], ['Another', 'sentence', 'goes', 'here', '.']]\n"
     ]
    }
   ],
   "source": [
    "### Session 1: Data Processing and Tokenization\n",
    "\n",
    "# Introduction\n",
    "# This notebook covers fundamental NLP data preprocessing and tokenization techniques.\n",
    "# It includes both theoretical explanations and practical exercises.\n",
    "\n",
    "## Section 1: Understanding Text Preprocessing\n",
    "\n",
    "### What is Text Preprocessing?\n",
    "# Text preprocessing is a crucial step in NLP to ensure data consistency, reduce noise, and optimize model performance.\n",
    "\n",
    "# Common preprocessing steps:\n",
    "# - Lowercasing\n",
    "# - Removing punctuation\n",
    "# - Tokenization (splitting text into smaller units)\n",
    "# - Removing stopwords (optional, task-dependent)\n",
    "# - Normalization (e.g., stemming, lemmatization)\n",
    "\n",
    "# Let's implement some basic text preprocessing:\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def basic_preprocessing(text: str) -> str:\n",
    "    \"\"\"Performs basic text preprocessing.\"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "text_sample = \"Hello, World! NLP is amazing.\"\n",
    "print(\"Original:\", text_sample)\n",
    "print(\"Processed:\", basic_preprocessing(text_sample))\n",
    "\n",
    "## Section 2: Tokenization\n",
    "\n",
    "### What is Tokenization?\n",
    "# Tokenization is the process of breaking text into smaller units (tokens).\n",
    "# These can be words, subwords, or characters.\n",
    "\n",
    "# Common types of tokenization:\n",
    "# - Word-based tokenization (splitting on spaces)\n",
    "# - Character-level tokenization\n",
    "# - Subword tokenization (e.g., Byte-Pair Encoding, WordPiece, SentencePiece)\n",
    "\n",
    "# Let's implement simple word-based and character-level tokenization:\n",
    "\n",
    "def word_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Splits text into words.\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "def char_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Splits text into characters.\"\"\"\n",
    "    return list(text)\n",
    "\n",
    "# Example usage:\n",
    "processed_text = basic_preprocessing(text_sample)\n",
    "print(\"Word Tokens:\", word_tokenize(processed_text))\n",
    "print(\"Character Tokens:\", char_tokenize(processed_text))\n",
    "\n",
    "## Section 3: Subword Tokenization\n",
    "\n",
    "### What is Subword Tokenization?\n",
    "# Subword tokenization helps handle rare words and reduce vocabulary size.\n",
    "# Examples: Byte-Pair Encoding (BPE), WordPiece, and SentencePiece.\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "\n",
    "def train_bpe_tokenizer(texts: List[str]):\n",
    "    \"\"\"Trains a simple BPE tokenizer on a given text corpus.\"\"\"\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "    tokenizer.train_from_iterator(texts, trainer)\n",
    "    return tokenizer\n",
    "\n",
    "# Example usage with Tiny Shakespeare dataset (or any small dataset you provide)\n",
    "dataset = [\"This is an example sentence.\", \"Another sentence goes here.\"]\n",
    "bpe_tokenizer = train_bpe_tokenizer(dataset)\n",
    "\n",
    "# Encoding a sample sentence\n",
    "encoded = bpe_tokenizer.encode(\"This is an example sentence.\")\n",
    "print(\"BPE Tokens:\", encoded.tokens)\n",
    "\n",
    "## Section 4: Preparing a Dataset for Future Use\n",
    "\n",
    "### Creating a Tokenized Dataset\n",
    "# We will process a small dataset and tokenize it for use in future sessions.\n",
    "\n",
    "def prepare_tokenized_dataset(texts: List[str], tokenizer):\n",
    "    \"\"\"Tokenizes and encodes a dataset using the given tokenizer.\"\"\"\n",
    "    return [tokenizer.encode(text).tokens for text in texts]\n",
    "\n",
    "# Tokenizing our dataset\n",
    "tokenized_dataset = prepare_tokenized_dataset(dataset, bpe_tokenizer)\n",
    "print(\"Tokenized Dataset:\", tokenized_dataset)\n",
    "\n",
    "# Saving the tokenizer for reuse\n",
    "bpe_tokenizer.save(\"bpe_tokenizer.json\")\n",
    "\n",
    "# Conclusion: In this session, we covered:\n",
    "# - Basic text preprocessing\n",
    "# - Different types of tokenization\n",
    "# - Implementing word, character, and subword tokenization\n",
    "# - Creating a small dataset for future use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf037295-a488-4a39-9aba-d8ab007631ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
