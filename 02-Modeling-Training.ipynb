{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c286d0-48dc-4707-bb04-10cc87727c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e996f26d-f6cf-444c-bac2-511c70ef9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed3798-5270-4bd3-b0d4-c7bfd6b42a8f",
   "metadata": {},
   "source": [
    "# Session 2 – NLP Models and Training Basics (RNN)\n",
    "  \n",
    "In this notebook, we will dive into fundamental sequence models such as **RNNs** and **LSTMs**. We’ll also cover basic neural embeddings, training objectives, and see how to implement and train a **simple text generator** using an LSTM.\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction and Overview](#introduction)\n",
    "2. [Recurrent Neural Networks (RNNs)](#rnns)\n",
    "   - [The RNN Cell](#rnn-cell)\n",
    "   - [Vanishing and Exploding Gradients](#vanishing)\n",
    "3. [Long Short-Term Memory (LSTM)](#lstm)\n",
    "   - [Key Intuition Behind LSTM Gates](#lstm-gates)\n",
    "4. [Embeddings](#embeddings)\n",
    "5. [Basic Training Objectives in Language Modeling](#training-objectives)\n",
    "   - [Next Token Prediction](#next-token-pred)\n",
    "   - [Perplexity](#perplexity)\n",
    "6. [Implementing a Simple LSTM Text Generator in PyTorch](#implementation)\n",
    "   - [Data Preparation](#data-prep)\n",
    "   - [Model Definition](#model-def)\n",
    "   - [Training Loop](#training-loop)\n",
    "   - [Generating Text](#generate-text)\n",
    "\n",
    "Each section will be followed by one or more **Exercises** to help you practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b50b1-32ad-4ce9-a35f-4f4e2d8bb4f0",
   "metadata": {},
   "source": [
    "# <a id=\"overview\"></a>1. Overview and Setup\n",
    "\n",
    "This tutorial assumes you have:\n",
    "\n",
    "- **Basic Python** knowledge.\n",
    "- A local or cloud environment (e.g., Jupyter, Colab) with **PyTorch** installed.\n",
    "  - If needed, install PyTorch via `pip install torch` or follow instructions at [pytorch.org](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "No prior reading of other sessions is required; we’ll present all the essentials here.\n",
    "\n",
    "### Quick Setup Check\n",
    "```python\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "```\n",
    "\n",
    "Ensure you see a version number (e.g., `2.0.0` or similar) printed. If you get an error, please install or update PyTorch before continuing.\n",
    "\n",
    "- We’ll focus on **RNNs** and **LSTMs**.  \n",
    "- We’ll learn **why** they are powerful for sequential data.  \n",
    "- We’ll cover **basic training objectives** (like next-token prediction) for language modeling.  \n",
    "- Finally, we’ll implement a small **LSTM-based text generator**.\n",
    "\n",
    "**By the end of this session**, you should be able to:\n",
    "1. Understand how an RNN cell and LSTM cell process sequential data.  \n",
    "2. Implement an **LSTM** in a deep learning framework (here, PyTorch).  \n",
    "3. Train and evaluate a **text-generation** model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d5d4f43-f69f-4efc-9c2f-6eafe5fca19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a964978-2cdb-4568-9f2b-97b81c07b33a",
   "metadata": {},
   "source": [
    "# 2. Recurrent Neural Networks (RNNs)<a id=\"rnns\"></a>\n",
    "\n",
    "Recurrent Neural Networks are designed to handle **sequential data** by maintaining a hidden state that captures information about previous time steps. \n",
    "\n",
    "## Key Idea\n",
    "At each time step $t$:\n",
    "1. The RNN takes an input $x_t$ and the hidden state from the previous time step $h_{t-1}$.\n",
    "2. It produces a new hidden state $h_t$.\n",
    "\n",
    "Mathematically, a very **basic** RNN can be written as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h) \\\\\n",
    "y_t &= W_{hy} h_t + b_y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $h_t$ is the updated hidden state.\n",
    "- $y_t$ is the output at time step $t$ (used for tasks like classification or next-token prediction).\n",
    "- $W_{hh}, W_{xh}, W_{hy}$ are learned weight matrices.\n",
    "\n",
    "**Rearrangement of Terms**\n",
    "\n",
    "Notice that the term $W_{hh} h_{t-1} + W_{xh} x_t$ uses two matrix multiplications and an addition.\n",
    "Unless compiled, these two multiplications will be performed sequentially.\n",
    "We can gain a slight improvement if we concatenate $h$ and $x$, and use a single matrix multiplication by a larger weight matrix:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\tanh(W_h H_t + b_h) \\\\\n",
    "y_t &= W_{hy} h_t + b_y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $H_t = [h_{t-1}||x_t]$ is the concatenation of $h$ and $x$\n",
    "- $W_{h} = [W_{hh}||W_{xh}]$ is the cconcatenaation of $W_{hh}, W_{xh}$\n",
    "\n",
    "\n",
    "<img src=\"img_src/RNNs.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf63a95-efb9-43c0-ad96-b8731e927366",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"rnn-cell\"></a>The RNN Cell\n",
    "\n",
    "The **RNN cell** is the fundamental computational unit. At time step $t$:\n",
    "1. **Input**: current token (often embedded) + previous hidden state.\n",
    "2. **Output**: updated hidden state + optional output vector.\n",
    "\n",
    "If you unroll this cell over time for $T$ steps, you get a **computation graph** that looks like a chain, where each link is an RNN cell.\n",
    "\n",
    "<img src=\"img_src/RNN-folded.svg\"/>\n",
    "<img src=\"img_src/RNN-unfolded.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281f15f-52e2-4038-9fa1-116d35e884fa",
   "metadata": {},
   "source": [
    "### Exercise: Implement a Toy RNN Cell\n",
    "**Goal**:  \n",
    "1. Write a Python function that computes a single time-step of an RNN.  \n",
    "1. Use NumPy or PyTorch (in NumPy style) to do the matrix multiplication and a `tanh` activation.  \n",
    "1. Test it on a small input (e.g., input dimension of 5, hidden dimension of 3).\n",
    "\n",
    "*(Keep it simple—focus on the concept, not a full RNN unrolled over time.)*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0561a7ed-91c1-4cca-81f9-bf7e38c6a8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def rnn_step(x_t, h_prev, Wxh, Whh, bh):\n",
    "    \"\"\"Simple RNN step\n",
    "\n",
    "    Args:\n",
    "        x_t: shape (batch_size, input_dim)\n",
    "        h_prev: shape (batch_size, hidden_dim)\n",
    "        Wxh: shape (input_dim, hidden_dim)\n",
    "        Whh: shape (hidden_dim, hidden_dim)\n",
    "        bh: shape (hidden_dim,)\n",
    "    Returns:\n",
    "        h_t: shape (batch_size, hidden_dim) \n",
    "    \"\"\"\n",
    "    weighted_h = h_prev @ Whh  # shape: (batch_size, hidden_dim)\n",
    "    weighted_x = x_t @ Wxh  # shape: (batch_size, hidden_dim)\n",
    "    linear_hx = weighted_h + weighted_x + bh\n",
    "    nonlinear_hx = torch.tanh(linear_hx)\n",
    "    return nonlinear_hx\n",
    "\n",
    "# Tests -- we only consider shapes here\n",
    "N = (1, 2, 5)  # Batch sizes\n",
    "hidden_dims = (1, 2, 5)  # Hidden sizes\n",
    "input_dims = (1, 2, 5)  # Input sizes\n",
    "\n",
    "failed_cases = []\n",
    "for batch_size, hdim, xdim in itertools.product(N, hidden_dims, input_dims):\n",
    "    x = torch.ones(batch_size, xdim)\n",
    "    h = torch.zeros(batch_size, hdim)\n",
    "    Wxh = torch.ones(xdim, hdim)\n",
    "    Whh = torch.ones(hdim, hdim)\n",
    "    bh = torch.zeros(hdim)\n",
    "    expect_shape = (batch_size, hdim)\n",
    "    with torch.no_grad():\n",
    "        h_next = rnn_step(x, h, Wxh, Whh, bh)\n",
    "    if h_next.shape != expect_shape:\n",
    "        print('x', end='')\n",
    "        failed_cases.append((h_next.shape, expect_shape))\n",
    "    else:\n",
    "        print('.', end='')\n",
    "print()\n",
    "for got, expected in failed_cases:\n",
    "    print(f\"{expected} vs. {got}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb613ff0-dde2-4208-a71a-63bccf0a62c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_dim, state_dim):\n",
    "        super().__init__(data_dim, state_dim)\n",
    "\n",
    "        # Define parameters to train\n",
    "        self.input_linear = nn.Linear(  # Takes [x||h_prev] and produces h_next\n",
    "            in_features=self.data_dim + self.state_dim,\n",
    "            out_features=self.state_dim,\n",
    "            bias=True)\n",
    "        self.output_linear = nn.Linear(  # Takes h_next and produces y\n",
    "            in_features=self.state_dim,\n",
    "            out_features=self.data_dim,\n",
    "            bias=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        # Concatenate x and hidden_state\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.shape[0], self.state_dim, device=x.device, dtype=x.dtype)\n",
    "        xh = torch.hstack([x, h])\n",
    "\n",
    "        # Compute new hidden state\n",
    "        xh = self.input_linear(xh)\n",
    "        h_next = self.tanh(xh)\n",
    "\n",
    "        # Compute output\n",
    "        y = self.output_linear(h_next)\n",
    "        return y, h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113605e-5e6d-4762-909e-a0a218010c32",
   "metadata": {},
   "source": [
    "## <a id=\"vanishing\"></a>Vanishing and Exploding Gradients\n",
    "\n",
    "**Problem**: Simple RNNs often struggle with **long-term dependencies** due to **vanishing** or **exploding gradients**. That means:\n",
    "- When sequences are long, the gradient that flows backward through time either becomes extremely small (**vanishes**) or extremely large (**explodes**).\n",
    "- This makes training unstable or ineffective for capturing long-range context.\n",
    "\n",
    "**Solution**: Specialized RNN variants like **LSTM** or **GRU** mitigate these issues by incorporating gating mechanisms.\n",
    "\n",
    "### Research Note: let's invent a GRU (Gated Recurrent Unit)\n",
    "\n",
    "**RNN** : $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$\n",
    "\n",
    "* **Problem:** To compute the gradient of $h_1$ (or any early token), we need to multiply the gradients by small values in $W_h$, thus **vanishing** it.\n",
    "* **Solution:** Intelligently choose the previous memory: $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$ or $h_t = h_{t-1}$\n",
    "\n",
    "**RNN with no vanishing** : $h_t = \\alpha\\odot\\hat{h}_t + (1-\\alpha)\\odot h_{t-1}$, where $\\hat{h}_t=\\phi(W_hh_{t-1} + W_xx_{t})$\n",
    "\n",
    "* **Problem:** To compute the gradient of $h_1$ (or any early token), we need to multiply the gradients by large values in $W_h$, thus **exploding** it.\n",
    "* **Solution:** Intelligently choose to set the previous memory to zero before multiplying it by the weights: $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$ or $h_t = \\phi(W_xx_{t})$\n",
    "\n",
    "**RNN with no explosion** : $h_t = \\phi(W_h(\\beta \\odot h_{t-1}) + W_xx_{t})$\n",
    "\n",
    "* **Problem:** How do we decide on the values of $\\alpha$ and $\\beta$?\n",
    "* **Solution:** Don't! Let the data decide (learning)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\overbrace{\\alpha\\odot\\underbrace{\\phi\\left(W_h(\\beta \\odot h_{t-1}) + W_xx_{t}\\right)}_{\\text{no explosion}} + (1-\\alpha)\\odot h_{t-1}}^\\text{no vanishing} \\\\\n",
    "\\text{where}\\\\\n",
    "\\alpha &= \\sigma\\left(Ah_{t-1} + Bx_t\\right) &&\\text{Memory Update Gate}\\\\\n",
    "\\beta &= \\sigma\\left(Ch_{t-1} + Dx_t\\right) &&\\text{Memory Reset Gate}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Congratulations**, you have just invented a **Gated Recurrent Unit** (GRU)!\n",
    "\n",
    "*An earlier version of a gated recurrent network is [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory), which follows very similar logic for preserving the long-term context infromation.*\n",
    "\n",
    "| Network | Complexity | Long-Term Relationship | Gradient Issues |\n",
    "|---------|--------------|------|----|\n",
    "| RNN (tanh) | (++) | None | (-) |\n",
    "| GRU | (+) | (+)<br/>Single state | (++) |\n",
    "| LSTM | (--) | (++)<br/>Separate state for long and short terms | (++) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b445c-237d-4f50-a24c-b8cc229f8e8d",
   "metadata": {},
   "source": [
    "# <a id=\"lstm\"></a>3. Long Short-Term Memory (LSTM)\n",
    "\n",
    "A **Long Short-Term Memory (LSTM)** network is a type of RNN specifically designed to better capture **long-range dependencies**. It addresses the vanishing/exploding gradient problem through gates that control the flow of information.\n",
    "\n",
    "### <a id=\"lstm-gates\"></a>Key Intuition Behind LSTM Gates\n",
    "\n",
    "Typical LSTM equations:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "f_t &= \\sigma(W_f [h_{t-1}, x_t] + b_f)\n",
    "&\\quad(\\text{Forget Gate}) \\\\\n",
    "i_t &= \\sigma(W_i [h_{t-1}, x_t] + b_i)\n",
    "&\\quad(\\text{Input Gate}) \\\\\n",
    "\\tilde{C_t} &= \\tanh(W_C [h_{t-1}, x_t] + b_C)\n",
    "&\\quad(\\text{Candidate Values}) \\\\\n",
    "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C_t}\n",
    "&\\quad(\\text{Cell State Update}) \\\\\n",
    "o_t &= \\sigma(W_o [h_{t-1}, x_t] + b_o)\n",
    "&\\quad(\\text{Output Gate}) \\\\\n",
    "h_t &= o_t \\odot \\tanh(C_t)\n",
    "&\\quad(\\text{New Hidden State})\n",
    "\\end{aligned}$$\n",
    "\n",
    "- **Forget Gate** ($f_t$): decides how much old state to keep.\n",
    "- **Input Gate** ($i_t$): decides how much new information to add.\n",
    "- **Candidate** ($\\tilde{C_t}$): proposed update to the cell state.\n",
    "- **Output Gate** ($o_t$): decides how much cell state to output as hidden state.\n",
    "\n",
    "This gating mechanism helps **preserve gradients** across many time steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f51e6-2a10-4cec-aff8-e581bc4b072f",
   "metadata": {},
   "source": [
    "### Exercise: Compare RNN and GRU Outputs (or LSTM if you prefer)\n",
    "1. Create a synthetic sequence of length 20.  \n",
    "2. Feed it into a small **Vanilla RNN** and a small **GRU** (in PyTorch).  \n",
    "3. Compare the final hidden states after feeding all time steps. Are they similar? If you vary the length from 20 to 50 to 100, how do the hidden states change?\n",
    "\n",
    "*Hint*: This is a conceptual experiment. You can use random inputs, then measure how the hidden states drift over longer sequences. \n",
    "\n",
    "*Hint*: **If you really want**, you can use the utilities in the `utils.py` file to generate simple synthetic sequences (`generate_synthetic_sequences`).\n",
    "\n",
    "*Hint*: You don't have to train the network, but if you want to you can use `utils.py` (`train_recurrent`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8212c123-1c29-414d-b919-0625d7673f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden output shapes: h_rnn.shape=torch.Size([1, 3, 2]), h_gru.shape=torch.Size([1, 3, 2]), h_lstm.shape=torch.Size([1, 3, 2]), c_lstm.shape=torch.Size([1, 3, 2])\n",
      "Hidden state norms: 9.96e-01, 2.00e+00, 2.91e+00\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "batch_size = 3\n",
    "sequence_length = 10\n",
    "xdim = 3\n",
    "hdim = 2\n",
    "\n",
    "x = torch.ones(sequence_length, batch_size, xdim)  # Input sequence\n",
    "h = torch.zeros(1, batch_size, hdim)  # Initial hidden state / initial memory\n",
    "\n",
    "rnn_model = nn.RNN(input_size=xdim, hidden_size=hdim, num_layers=1, batch_first=False, bidirectional=False)\n",
    "gru_model = nn.GRU(input_size=xdim, hidden_size=hdim, num_layers=1, batch_first=False, bidirectional=False)\n",
    "lstm_model = nn.LSTM(input_size=xdim, hidden_size=hdim, batch_first=False)\n",
    "\n",
    "rnn_model.zero_grad()\n",
    "gru_model.zero_grad()\n",
    "lstm_model.zero_grad()\n",
    "\n",
    "y_rnn, h_rnn = rnn_model(x)\n",
    "y_gru, h_gru = gru_model(x)\n",
    "y_lstm, (h_lstm, c_lstm) = lstm_model(x)\n",
    "\n",
    "print(f\"Hidden output shapes: {h_rnn.shape=}, {h_gru.shape=}, {h_lstm.shape=}, {c_lstm.shape=}\")\n",
    "\n",
    "# Very basic error -- just minimizing the norm of the memory\n",
    "rnn_error = h_rnn.norm()\n",
    "gru_error = h_gru.norm()\n",
    "lstm_error = h_lstm.norm() + c_lstm.norm()\n",
    "\n",
    "rnn_error.backward()\n",
    "gru_error.backward()\n",
    "lstm_error.backward()\n",
    "\n",
    "print(f\"Hidden state norms: {rnn_error:.2e}, {gru_error:.2e}, {lstm_error:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ab50badc-ac31-43ff-a469-da174722fe5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5695, 0.3378],\n",
       "         [0.5443, 0.3228]]),\n",
       " tensor([[-0.0997, -0.1173],\n",
       "         [ 0.0308,  0.0358],\n",
       "         [ 0.0012,  0.0012],\n",
       "         [ 0.0363,  0.0388],\n",
       "         [ 0.3305,  0.3887],\n",
       "         [ 0.0720,  0.0834]]),\n",
       " tensor([[ 0.1479, -0.1351],\n",
       "         [ 0.2204, -0.1990],\n",
       "         [ 0.2406, -0.2199],\n",
       "         [ 0.2446, -0.2221],\n",
       "         [ 0.5650, -0.5166],\n",
       "         [-0.4403,  0.3976],\n",
       "         [ 0.1588, -0.1445],\n",
       "         [ 0.0152, -0.0163]]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.weight_hh_l0.grad, gru_model.weight_hh_l0.grad, lstm_model.weight_hh_l0.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b2473-c806-405e-b0c3-647151ebdcba",
   "metadata": {},
   "source": [
    "# <a id=\"embeddings\"></a>4. Embeddings\n",
    "\n",
    "**TODO: Embedding and Latent Space Explanation**\n",
    "\n",
    "When dealing with text, each word or token is usually mapped to an **embedding** vector rather than a large one-hot vector.\n",
    "\n",
    "- **Embedding Layer**: A learnable matrix that maps token indices to dense vectors of fixed dimension $d$.\n",
    "- This helps the model learn **semantic relationships** between words.\n",
    "\n",
    "For example:\n",
    "- Word “hello” → index 5 → embedding vector $\\mathbf{e} \\in \\mathbb{R}^d$.\n",
    "\n",
    "Most frameworks (like PyTorch) provide a built-in layer, `nn.Embedding(vocab_size, embed_dim)`, that handles this.\n",
    "\n",
    "\n",
    "### Exercise 3: Custom Embedding Lookup\n",
    "1. Create a small vocabulary of 5 tokens.  \n",
    "2. Initialize a random embedding matrix of shape $(5, d)$.  \n",
    "3. Write a function that takes a token index and returns the corresponding embedding row.  \n",
    "4. Compare with `nn.Embedding` in PyTorch for the same matrix initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "060618e9-54bd-4212-8fd4-e1bf674ca2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom lookup vector: tensor([ 2.2082, -0.6380,  0.4617])\n",
      "nn.Embedding lookup vector: tensor([ 2.2082, -0.6380,  0.4617])\n",
      "Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "embed_dim = 3\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Step 1: create a random embedding matrix\n",
    "embedding_matrix = torch.randn(vocab_size, embed_dim)\n",
    "\n",
    "def custom_embed_lookup(token_idx, embedding_matrix):\n",
    "    \"\"\"\n",
    "    token_idx: int index (0 <= token_idx < vocab_size)\n",
    "    embedding_matrix: shape (vocab_size, embed_dim)\n",
    "    returns: torch.Tensor of shape (embed_dim,)\n",
    "    \"\"\"\n",
    "    return embedding_matrix[token_idx]\n",
    "\n",
    "# Pick a test token index\n",
    "test_idx = 2\n",
    "custom_vec = custom_embed_lookup(test_idx, embedding_matrix)\n",
    "print(\"Custom lookup vector:\", custom_vec)\n",
    "\n",
    "# Step 2: Compare with nn.Embedding\n",
    "embed_layer = nn.Embedding(vocab_size, embed_dim)\n",
    "# Overwrite the embedding_layer's weights with our random matrix\n",
    "with torch.no_grad():\n",
    "    embed_layer.weight.copy_(embedding_matrix)\n",
    "\n",
    "# Now let's see if it matches:\n",
    "with torch.no_grad():\n",
    "    torch_vec = embed_layer(torch.tensor([test_idx]))\n",
    "print(\"nn.Embedding lookup vector:\", torch_vec.squeeze(0))\n",
    "\n",
    "# They should be (almost) identical\n",
    "print(\"Difference:\", (custom_vec - torch_vec.squeeze(0)).abs().sum().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e7f84-2962-4b60-9176-e30ef3d352ae",
   "metadata": {},
   "source": [
    "# <a id=\"training-objectives\"></a>5. Training Objectives in Language Modeling\n",
    "\n",
    "In language modeling, a typical goal is **next-token prediction**: given the previous tokens, predict the next one. We often use **cross-entropy loss** and measure model performance with **perplexity**.\n",
    "\n",
    "### <a id=\"next-token-prediction\"></a>Next Token Prediction\n",
    "\n",
    "For a vocabulary of size $V$, the model outputs a probability distribution over the next token:\n",
    "$$\n",
    "P(x_t \\mid x_{t-1}, x_{t-2}, \\ldots, x_1)\n",
    "$$\n",
    "The training loss for a sequence might be:\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t}\\log P(\\hat{x}_t = x_t)\n",
    "$$\n",
    "where $ x_t $ is the ground truth and $\\hat{x}_t$ is the predicted distribution.\n",
    "\n",
    "\n",
    "### <a id=\"perplexity\"></a>Perplexity\n",
    "\n",
    "**Perplexity (PPL)** is a common metric for language models:\n",
    "$$\n",
    "\\text{PPL} = \\exp\\left(-\\frac{1}{N}\\sum_{t=1}^{N} \\log P(x_t)\\right),\n",
    "$$\n",
    "where $N$ is the total number of tokens in the test set. Lower PPL typically means a better language model.\n",
    "\n",
    "\n",
    "### Exercise 4: Manual Cross-Entropy\n",
    "- Let your model output a probability vector $[0.2, 0.3, 0.1, 0.4]$ for a 4-word vocabulary.  \n",
    "- Suppose the correct label is index 3. Manually compute cross-entropy.  \n",
    "- Compare with `torch.nn.functional.cross_entropy` to confirm your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f4a59189-4da6-43f3-b525-53b24e77e6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual cross-entropy: 0.916290716972994\n",
      "PyTorch cross-entropy: 0.9162907004356384\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "probs = torch.tensor([0.2, 0.3, 0.1, 0.4])\n",
    "true_label = 3  # index 3 is the correct label\n",
    "\n",
    "# 1) Manual cross-entropy\n",
    "manual_ce = -math.log(probs[true_label].item())\n",
    "\n",
    "# 2) Using PyTorch (note that F.cross_entropy expects logits, not probabilities!)\n",
    "# So we need to convert probabilities => logits with log-softmax inverse => logit = log(p_i / 1)\n",
    "# But simpler is to do cross_entropy on log(prob) by building a single \"batch\" example:\n",
    "logits = torch.log(probs).unsqueeze(0)  # shape (1, 4)\n",
    "targets = torch.tensor([true_label])    # shape (1,)\n",
    "\n",
    "ce_torch = F.nll_loss(logits, targets)  # nll_loss expects log-probabilities\n",
    "# or equivalently: ce_torch = F.cross_entropy(logits, targets) if we interpret logits as log-probs\n",
    "\n",
    "print(\"Manual cross-entropy:\", manual_ce)\n",
    "print(\"PyTorch cross-entropy:\", ce_torch.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b643c3-dfed-4493-a590-67e5a8c35f06",
   "metadata": {},
   "source": [
    "# <a id=\"implementation\"></a>6. Implementing a Simple LSTM Text Generator in PyTorch\n",
    "\n",
    "Let’s build a small example that:\n",
    "1. **Prepares a tiny text dataset**.\n",
    "2. Splits it into input–target pairs for next-token prediction.\n",
    "3. Defines and trains an LSTM-based model.\n",
    "4. **Generates** text from the trained model.\n",
    "\n",
    "### <a id=\"data-prep\"></a>Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "17cf4560-45b6-48cd-b578-6b6ce53618d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['pytorch', 'world', 'again', 'hello']\n",
      "Mapping (word -> idx): {'pytorch': 0, 'world': 1, 'again': 2, 'hello': 3}\n",
      "Vocab size: 4\n",
      "Input sequences shape: torch.Size([4, 3])\n",
      "Target words shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Example small text\n",
    "text = \"hello world hello pytorch hello world again\"\n",
    "\n",
    "# Tokenize (word-level for simplicity)\n",
    "words = text.split()\n",
    "vocab = list(set(words))\n",
    "vocab_size = len(vocab)\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Mapping (word -> idx):\", word2idx)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# Convert words to indices\n",
    "indices = [word2idx[w] for w in words]\n",
    "\n",
    "# We'll choose a sequence length\n",
    "seq_length = 3\n",
    "\n",
    "# Prepare training data\n",
    "input_sequences = []\n",
    "target_words = []\n",
    "\n",
    "for i in range(len(indices) - seq_length):\n",
    "    input_seq = indices[i:i+seq_length]   # 3 words\n",
    "    target = indices[i+seq_length]        # the 4th word is the label\n",
    "    input_sequences.append(input_seq)\n",
    "    target_words.append(target)\n",
    "\n",
    "input_sequences = torch.tensor(input_sequences, dtype=torch.long)\n",
    "target_words = torch.tensor(target_words, dtype=torch.long)\n",
    "\n",
    "print(\"Input sequences shape:\", input_sequences.shape)\n",
    "print(\"Target words shape:\", target_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b8569230-bddd-4cc6-8585-25fd1c0e551c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 3] => 0\n",
      "  ['pytorch', 'hello', 'world'] => pytorch\n",
      "[1 3 0] => 3\n",
      "  ['pytorch', 'hello', 'world'] => hello\n",
      "[3 0 3] => 1\n",
      "  ['pytorch', 'hello', 'world'] => world\n",
      "[0 3 1] => 2\n",
      "  ['pytorch', 'hello', 'world'] => again\n"
     ]
    }
   ],
   "source": [
    "for seq, targ in zip(input_sequences, target_words):\n",
    "    seq = seq.numpy()\n",
    "    targ = targ.item()\n",
    "    seq_detokenized = list(map(idx2word.get, seq))\n",
    "    targ_detokenized = idx2word.get(targ)\n",
    "    print(f\"{seq} => {targ}\")\n",
    "    print(f\"  {input_sequences_detokenized} => {targ_detokenized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527d7b4-87ad-475e-aee0-86eaf23edb82",
   "metadata": {},
   "source": [
    "### <a id=\"model-def\"></a>Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "af099acf-c0d2-4136-af58-4f9e401fc993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        # 1) Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # 2) LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        # 3) Linear output layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)                # (batch_size, seq_length, embed_dim)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embedded)  # (batch_size, seq_length, hidden_dim)\n",
    "        final_hidden = lstm_out[:, -1, :]           # last time step\n",
    "        logits = self.fc(final_hidden)              # (batch_size, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a9d58-cc34-4a2f-8d13-798bf443ad5b",
   "metadata": {},
   "source": [
    "### <a id=\"training-loop\"></a>Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a67dcc63-f36a-4f6d-a383-ee3a29013102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/200], Loss: 0.0115\n",
      "Epoch [100/200], Loss: 0.0030\n",
      "Epoch [150/200], Loss: 0.0018\n",
      "Epoch [200/200], Loss: 0.0012\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 8\n",
    "hidden_dim = 16\n",
    "learning_rate = 0.01\n",
    "num_epochs = 200\n",
    "\n",
    "model = SimpleLSTM(vocab_size, embed_dim, hidden_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_sequences)  # shape: (batch_size, vocab_size)\n",
    "    loss = criterion(logits, target_words)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35484b03-2b3d-499a-9677-cad88a37fdd0",
   "metadata": {},
   "source": [
    "### <a id=\"generate-text\"></a>Generating Text\n",
    "\n",
    "We can now generate text by **sampling** the model’s predictions iteratively.\n",
    "\n",
    "Feel free to experiment with:\n",
    "- **Different seeds**.\n",
    "- **Different sampling strategies** (e.g., greedy vs. top-k).  \n",
    "- A **larger corpus** (like Tiny Shakespeare)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "049323c0-a716-4c73-83b9-083df7d48bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: hello world hello pytorch hello world again pytorch\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_words, num_words=5):\n",
    "    model.eval()\n",
    "    words_generated = seed_words[:]\n",
    "    \n",
    "    # Convert seed_words to indices\n",
    "    current_seq = [word2idx[w] for w in seed_words]\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        inp = torch.tensor([current_seq], dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            logits = model(inp)  # shape: (1, vocab_size)\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()  # shape: (vocab_size,)\n",
    "        \n",
    "        # Sample from probability distribution\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        next_word = idx2word[next_idx]\n",
    "        words_generated.append(next_word)\n",
    "        \n",
    "        # Slide the window (drop the first index, append new index)\n",
    "        current_seq = current_seq[1:] + [next_idx]\n",
    "    \n",
    "    return \" \".join(words_generated)\n",
    "\n",
    "# Let's try generating with a seed of length = seq_length (3)\n",
    "seed = [\"hello\", \"world\", \"hello\"]  # must be in vocab\n",
    "generated_text = generate_text(model, seed, num_words=5)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6267e-67ac-4047-b6a0-ceef19c69c5d",
   "metadata": {},
   "source": [
    "### Exercise: Experiment with the Generator\n",
    "1. Change the `num_words` to 10 or 20 and see if your text generation forms any repetitive patterns.  \n",
    "2. Try a **larger** dataset if you have one. Compare the coherence of the generated text.  \n",
    "3. Print out intermediate hidden states if you’re curious about how the model’s representation changes over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d6dbfa72-a426-479f-ab2d-ea7c7ef65039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " hello world hello pytorch hello world again pytorch hello world again\n",
      "Intermediate hidden states shapes:\n",
      " Step 1: (1, 16)\n",
      " Step 2: (1, 16)\n",
      " Step 3: (1, 16)\n",
      " Step 4: (1, 16)\n",
      " Step 5: (1, 16)\n",
      " Step 6: (1, 16)\n",
      " Step 7: (1, 16)\n",
      " Step 8: (1, 16)\n"
     ]
    }
   ],
   "source": [
    "# Suppose 'model' is our trained LSTM model, 'word2idx' and 'idx2word' are our mappings.\n",
    "\n",
    "def generate_text_with_hidden(model, seed_words, num_words=10):\n",
    "    \"\"\"\n",
    "    Generate text from the model, returning the hidden states as well.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    words_generated = seed_words[:]\n",
    "    hidden_states = []   # store hidden states at each step\n",
    "    \n",
    "    current_seq = [word2idx[w] for w in seed_words]\n",
    "    \n",
    "    # Hidden and cell state, if needed\n",
    "    # We'll assume 1-layer LSTM, batch_size=1\n",
    "    h, c = None, None\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        inp = torch.tensor([current_seq], dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            # Modify forward pass to capture intermediate hidden states\n",
    "            # We can do this by running the embedding + LSTM manually:\n",
    "            embedded = model.embedding(inp)  # shape (1, seq_length, embed_dim)\n",
    "            # We pass in (h, c) if they exist, otherwise let the LSTM init them\n",
    "            lstm_out, (h, c) = model.lstm(embedded, (h, c) if h is not None else None)\n",
    "            \n",
    "            # final time step\n",
    "            final_hidden = lstm_out[:, -1, :] \n",
    "            \n",
    "            # For debugging: store the hidden state in a list\n",
    "            hidden_states.append(final_hidden.detach().cpu().numpy())\n",
    "            \n",
    "            logits = model.fc(final_hidden)\n",
    "            probs = torch.softmax(logits, dim=-1).squeeze()\n",
    "            next_idx = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "        next_word = idx2word[next_idx]\n",
    "        words_generated.append(next_word)\n",
    "        current_seq = current_seq[1:] + [next_idx]\n",
    "        \n",
    "    return \" \".join(words_generated), hidden_states\n",
    "\n",
    "# Example usage\n",
    "seed = [\"hello\", \"world\", \"hello\"]\n",
    "generated_text, h_states = generate_text_with_hidden(model, seed, num_words=8)\n",
    "print(\"Generated Text:\\n\", generated_text)\n",
    "print(\"Intermediate hidden states shapes:\")\n",
    "for i, hs in enumerate(h_states):\n",
    "    print(f\" Step {i+1}: {hs.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01d168-505a-40e2-9daf-69039934e4af",
   "metadata": {},
   "source": [
    "# Fun Things -- Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f5a665-da25-4aff-ab81-f893698e08ae",
   "metadata": {},
   "source": [
    "### Step 1: Pre-process the textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "054925f7-8c8f-4631-82c1-721a8bc497e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_PATH = os.path.abspath(\"./shakespeare\")\n",
    "filelist = os.listdir(DATA_PATH)\n",
    "filelist = list(map(lambda f: os.path.join(DATA_PATH, f), filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dd96c502-75fc-4493-964b-c117c557de81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of combined text: 5283837\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_shakespeare_texts(filelist):\n",
    "    \"\"\"\n",
    "    Loads all text from filelist and returns all texts concatenated\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    \n",
    "    for file_path in filelist:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            all_text += f.read() + \"\\n\"  # add a newline at the end of each file\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "full_text = load_shakespeare_texts(filelist)\n",
    "print(\"Total length of combined text:\", len(full_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e4c93-8968-4b5c-96cd-31eb211e9dac",
   "metadata": {},
   "source": [
    "**Character-Level Tokenization**\n",
    "\n",
    "Since this is a character-level model, our “tokens” are just unique characters found in the text:\n",
    "\n",
    "* Identify the unique set of characters.\n",
    "* Map each character to a unique integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "846dae5c-8a8d-4bb8-bafb-75e1645e8d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chars found: 80\n",
      "Example of characters: ['\\t', '\\n', ' ', '!', '$', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create vocabulary of unique characters\n",
    "chars = sorted(list(set(full_text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Unique chars found:\", vocab_size)\n",
    "print(\"Example of characters:\", chars[:50])\n",
    "\n",
    "# Create mapping from character to index (and reverse)\n",
    "_char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "_idx2char = {i: ch for ch, i in _char2idx.items()}\n",
    "\n",
    "# Add special characters\n",
    "for special_token in [\"<|UNK|>\"]:\n",
    "    k = len(_char2idx)\n",
    "    _char2idx[special_token] = k\n",
    "    _idx2char[k] = special_token\n",
    "\n",
    "# Utility functions\n",
    "def char2idx(ch):\n",
    "    return [_char2idx.get(c, \"<|UNK|>\") for c in ch]\n",
    "def idx2char(idx):\n",
    "    if isinstance(idx, torch.Tensor):\n",
    "        return idx2char(idx.detach().cpu().numpy())\n",
    "    if isinstance(idx, np.ndarray):\n",
    "        return idx2char(idx.tolist())\n",
    "    if isinstance(idx, int):\n",
    "        return _idx2char[idx]\n",
    "    return [idx2char(i) for i in idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821ff33-b6cb-4083-9b5a-ca0f537faf20",
   "metadata": {},
   "source": [
    "**Convert Text to Indices**\n",
    "\n",
    "Convert the entire text into a list (or array) of integer indices. This will make it easier to feed into PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "53f7f340-1ac9-4b56-b579-7692d2eeb667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_tensor shape: torch.Size([5283837])\n"
     ]
    }
   ],
   "source": [
    "# Convert all text to indices\n",
    "data_as_indices = char2idx(full_text)  # [_char2idx[ch] for ch in full_text]\n",
    "data_tensor = torch.tensor(data_as_indices, dtype=torch.long)\n",
    "print(\"data_tensor shape:\", data_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f3014-1fea-420b-abcb-b2af333ecbda",
   "metadata": {},
   "source": [
    "**Create Training Sequences**\n",
    "\n",
    "For character-level language modeling, a common approach is:\n",
    "\n",
    "* Pick a sequence length, e.g. seq_length = 100.\n",
    "* For each sequence of seq_length characters, the target is the next character.\n",
    "\n",
    "We can use PyTorch's `Dataset`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2c288993-0ed4-48ae-9ed7-8750e1afd69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 52837\n",
      "Example X (indices): tensor([ 0, 44, 32, 29,  2, 37, 29, 42, 42, 49])\n",
      "Example Y (index): tensor([44, 32, 29,  2, 37, 29, 42, 42, 49,  2])\n",
      "Example X (decoded): ['\\t', 'T', 'H', 'E', ' ', 'M', 'E', 'R', 'R', 'Y']\n",
      "Example Y (decoded): ['T', 'H', 'E', ' ', 'M', 'E', 'R', 'R', 'Y', ' ']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data_tensor, seq_length):\n",
    "        self.data = data_tensor\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        # We can form this many sequences (minus 1 for the target)\n",
    "        return len(self.data) // self.seq_length - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_length\n",
    "        x_seq = self.data[start : start + self.seq_length]\n",
    "        # Targets are the subsequent seq_length characters\n",
    "        y_seq = self.data[start+1 : start + self.seq_length + 1]\n",
    "        return x_seq, y_seq\n",
    "\n",
    "seq_length = 100\n",
    "dataset = CharDataset(data_tensor, seq_length=seq_length)\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "\n",
    "# For demonstration, let's get one example\n",
    "example_x, example_y = dataset[0]\n",
    "print(\"Example X (indices):\", example_x[:10])\n",
    "print(\"Example Y (index):\", example_y[:10])\n",
    "print(\"Example X (decoded):\", idx2char(example_x[:10]))# \"\".join(idx2char(i.item()) for i in example_x[:30]))\n",
    "print(\"Example Y (decoded):\", idx2char(example_y[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2b56412e-ef4a-4e0d-a4d2-74d0f9dfc495",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbc683-5192-4fab-a1a0-9e784f8922d6",
   "metadata": {},
   "source": [
    "### Step 2: Model Definition (LSTM)\n",
    "\n",
    "We’ll define a character-level LSTM model:\n",
    "\n",
    "1. Embedding: maps integer character indices to dense vectors (optional, but often helps).\n",
    "1. LSTM: one or more LSTM layers that process the embedded sequence.\n",
    "1. Linear: output layer to predict the next character’s index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1ec456ca-b740-4252-8300-79cd7b3e1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_length)\n",
    "        hidden_state: tuple (h, c) for LSTM hidden/cell states (if you want to pass it in)\n",
    "        Returns: logits (batch_size, seq_length, vocab_size), updated_hidden_state\n",
    "        \"\"\"\n",
    "        # 1) Embedding\n",
    "        embedded = self.embedding(x)  # shape: (batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        # 2) LSTM\n",
    "        if hidden_state is None:\n",
    "            out, (h, c) = self.lstm(embedded)  # out: (batch_size, seq_length, hidden_dim)\n",
    "        else:\n",
    "            out, (h, c) = self.lstm(embedded, hidden_state)\n",
    "        \n",
    "        # 3) Fully connected (we want to produce a prediction at each time step)\n",
    "        logits = self.fc(out)  # shape: (batch_size, seq_length, vocab_size)\n",
    "        \n",
    "        return logits, (h, c)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Utility to initialize the hidden state (h, c) to zeros.\n",
    "        Returns: h0, c0 (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        return (h0, c0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c75f8-ed65-4855-9ae6-dc30e1ae879e",
   "metadata": {},
   "source": [
    "### Step 3: Training Routine\n",
    "\n",
    "**Training Setup**\n",
    "\n",
    "We define:\n",
    "\n",
    "* A loss function (CrossEntropyLoss), typical for next-character prediction.\n",
    "* An optimizer (e.g., Adam or RMSprop).\n",
    "* Possibly device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "39ef7dfe-4f5c-402a-9d60-8c52e30d210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = CharLSTM(vocab_size, embed_dim=128, hidden_dim=256, num_layers=2)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb32ce-a9dc-42bc-9de1-2b97821b5b48",
   "metadata": {},
   "source": [
    "**Training Loop**\n",
    "\n",
    "At each iteration:\n",
    "\n",
    "1. Get a batch (x, y) from the dataloader. Here, x is of shape (batch_size, seq_length) and y of shape (batch_size,).\n",
    "1. Model outputs logits of shape (batch_size, seq_length, vocab_size).\n",
    "1. We actually want to predict the character that comes after each character in x. So we can shift by 1 step or simply note that y at index i is the final character of the sequence. But if we want a prediction at each time step (not just the last one), we might create labels of shape (batch_size, seq_length)—one label per input character.\n",
    "\n",
    "In the example below, we do the simplest approach: each sequence’s final character is the label. This means we use only the last time step’s logits to compute the loss. Alternatively, if you want to predict the next character at every time step, you’ll need to shift the labels accordingly. (We’ll show the typical approach of every time step.)\n",
    "\n",
    "**Case: Predict next char at every time step**\n",
    "\n",
    "We shift our target by 1 inside the dataset or handle it here. Let’s assume we do it at the dataset level for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7d20bcc1-b463-4cd6-b2f4-2fa04d38b624",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [100] at entry 0 and [37] at entry 9",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[198], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      5\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_seq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_seq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Reset gradients\u001b[39;49;00m\n",
      "File \u001b[0;32m~/mamba/envs/llm/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/mamba/envs/llm/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/mamba/envs/llm/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mamba/envs/llm/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mamba/envs/llm/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/mamba/envs/llm/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/mamba/envs/llm/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [100] at entry 0 and [37] at entry 9"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for x_seq, y_seq in dataloader:\n",
    "        x_seq, y_seq = x_seq.to(device), y_seq.to(device)\n",
    "        \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden_state = model.init_hidden(batch_size=x_seq.size(0))\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, hidden_state = model(x_seq, hidden_state)  \n",
    "        # logits: (batch_size, seq_length, vocab_size)\n",
    "        \n",
    "        # Reshape logits and targets for cross-entropy\n",
    "        # We want CE across all time steps\n",
    "        logits_reshaped = logits.view(-1, vocab_size)   # (batch_size*seq_length, vocab_size)\n",
    "        targets_reshaped = y_seq.view(-1)               # (batch_size*seq_length,)\n",
    "        \n",
    "        loss = criterion(logits_reshaped, targets_reshaped)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader_full)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb72f797-9330-4422-a5e9-f27c58d250c0",
   "metadata": {},
   "source": [
    "### Step 4: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b930d01a-0fdb-48d1-990f-de520f54cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string=\"ROMEO:\", length=500, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generates text one character at a time.\n",
    "    - start_string: initial prompt\n",
    "    - length: number of characters to generate\n",
    "    - temperature: sampling diversity (1.0 => neutral, >1 => more random)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert start string to indices\n",
    "    input_indices = [char2idx[ch] for ch in start_string]\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    hidden_state = model.init_hidden(batch_size=1)\n",
    "    \n",
    "    # \"Warm up\" the model with the start string\n",
    "    for i in range(len(start_string) - 1):\n",
    "        # feed each char except the last one\n",
    "        _, hidden_state = model(input_tensor[:, i:i+1], hidden_state)\n",
    "    \n",
    "    # The last character in start_string\n",
    "    last_char_idx = input_tensor[:, -1]\n",
    "    output_text = start_string\n",
    "    \n",
    "    # Now generate 'length' more characters\n",
    "    for _ in range(length):\n",
    "        logits, hidden_state = model(last_char_idx.unsqueeze(1), hidden_state)\n",
    "        # logits shape: (1, 1, vocab_size)\n",
    "        logits = logits[:, -1, :]  # take the last time step => shape (1, vocab_size)\n",
    "        \n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()  # shape (vocab_size,)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "        # Append to output\n",
    "        next_char = idx2char[next_idx]\n",
    "        output_text += next_char\n",
    "        \n",
    "        # Update last_char_idx\n",
    "        last_char_idx = torch.tensor([next_idx], device=device)\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "# Example usage after training:\n",
    "generated = generate_text(model, start_string=\"ROMEO:\", length=300, temperature=0.8)\n",
    "print(\"Generated text:\\n\", generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31be407-67f7-495d-9204-9aa8b975fcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
