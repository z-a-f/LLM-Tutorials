{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ed3798-5270-4bd3-b0d4-c7bfd6b42a8f",
   "metadata": {},
   "source": [
    "# Session 2 – NLP Models and Training Basics (RNN)\n",
    "  \n",
    "In this notebook, we will dive into fundamental sequence models such as **RNNs** and **LSTMs**. We’ll also cover basic neural embeddings, training objectives, and see how to implement and train a **simple text generator** using an LSTM.\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction and Overview](#introduction)\n",
    "2. [Recurrent Neural Networks (RNNs)](#rnns)\n",
    "   - [The RNN Cell](#rnn-cell)\n",
    "   - [Vanishing and Exploding Gradients](#vanishing)\n",
    "3. [Long Short-Term Memory (LSTM)](#lstm)\n",
    "   - [Key Intuition Behind LSTM Gates](#lstm-gates)\n",
    "4. [Embeddings](#embeddings)\n",
    "5. [Basic Training Objectives in Language Modeling](#training-objectives)\n",
    "   - [Next Token Prediction](#next-token-pred)\n",
    "   - [Perplexity](#perplexity)\n",
    "6. [Implementing a Simple LSTM Text Generator in PyTorch](#implementation)\n",
    "   - [Data Preparation](#data-prep)\n",
    "   - [Model Definition](#model-def)\n",
    "   - [Training Loop](#training-loop)\n",
    "   - [Generating Text](#generate-text)\n",
    "7. [Conclusion](#conclusion)\n",
    "\n",
    "Each section will be followed by one or more **Exercises** to help you practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b50b1-32ad-4ce9-a35f-4f4e2d8bb4f0",
   "metadata": {},
   "source": [
    "# <a id=\"overview\"></a>1. Overview and Setup\n",
    "\n",
    "This tutorial assumes you have:\n",
    "\n",
    "- **Basic Python** knowledge.\n",
    "- A local or cloud environment (e.g., Jupyter, Colab) with **PyTorch** installed.\n",
    "  - If needed, install PyTorch via `pip install torch` or follow instructions at [pytorch.org](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "No prior reading of other sessions is required; we’ll present all the essentials here.\n",
    "\n",
    "### Quick Setup Check\n",
    "```python\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "```\n",
    "\n",
    "Ensure you see a version number (e.g., `2.0.0` or similar) printed. If you get an error, please install or update PyTorch before continuing.\n",
    "\n",
    "- We’ll focus on **RNNs** and **LSTMs**.  \n",
    "- We’ll learn **why** they are powerful for sequential data.  \n",
    "- We’ll cover **basic training objectives** (like next-token prediction) for language modeling.  \n",
    "- Finally, we’ll implement a small **LSTM-based text generator**.\n",
    "\n",
    "**By the end of this session**, you should be able to:\n",
    "1. Understand how an RNN cell and LSTM cell process sequential data.  \n",
    "2. Implement an **LSTM** in a deep learning framework (here, PyTorch).  \n",
    "3. Train and evaluate a **text-generation** model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d5d4f43-f69f-4efc-9c2f-6eafe5fca19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "PyTorch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a964978-2cdb-4568-9f2b-97b81c07b33a",
   "metadata": {},
   "source": [
    "# 2. Recurrent Neural Networks (RNNs)<a id=\"rnns\"></a>\n",
    "\n",
    "Recurrent Neural Networks are designed to handle **sequential data** by maintaining a hidden state that captures information about previous time steps. \n",
    "\n",
    "## Key Idea\n",
    "At each time step $t$:\n",
    "1. The RNN takes an input $x_t$ and the hidden state from the previous time step $h_{t-1}$.\n",
    "2. It produces a new hidden state $h_t$.\n",
    "\n",
    "Mathematically, a very **basic** RNN can be written as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h) \\\\\n",
    "y_t &= W_{hy} h_t + b_y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $h_t$ is the updated hidden state.\n",
    "- $y_t$ is the output at time step $t$ (used for tasks like classification or next-token prediction).\n",
    "- $W_{hh}, W_{xh}, W_{hy}$ are learned weight matrices.\n",
    "\n",
    "**Rearrangement of Terms**\n",
    "\n",
    "Notice that the term $W_{hh} h_{t-1} + W_{xh} x_t$ uses two matrix multiplications and an addition.\n",
    "Unless compiled, these two multiplications will be performed sequentially.\n",
    "We can gain a slight improvement if we concatenate $h$ and $x$, and use a single matrix multiplication by a larger weight matrix:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\tanh(W_h H_t + b_h) \\\\\n",
    "y_t &= W_{hy} h_t + b_y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $H_t = [h_{t-1}||x_t]$ is the concatenation of $h$ and $x$\n",
    "- $W_{h} = [W_{hh}||W_{xh}]$ is the cconcatenaation of $W_{hh}, W_{xh}$\n",
    "\n",
    "\n",
    "<img src=\"img/RNNs.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf63a95-efb9-43c0-ad96-b8731e927366",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"rnn-cell\"></a>The RNN Cell\n",
    "\n",
    "The **RNN cell** is the fundamental computational unit. At time step $t$:\n",
    "1. **Input**: current token (often embedded) + previous hidden state.\n",
    "2. **Output**: updated hidden state + optional output vector.\n",
    "\n",
    "If you unroll this cell over time for $T$ steps, you get a **computation graph** that looks like a chain, where each link is an RNN cell.\n",
    "\n",
    "<img src=\"img/RNN-folded.png\"/>\n",
    "<img src=\"img/RNN-unfolded.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113605e-5e6d-4762-909e-a0a218010c32",
   "metadata": {},
   "source": [
    "## <a id=\"vanishing\"></a>Vanishing and Exploding Gradients\n",
    "\n",
    "**Problem**: Simple RNNs often struggle with **long-term dependencies** due to **vanishing** or **exploding gradients**. That means:\n",
    "- When sequences are long, the gradient that flows backward through time either becomes extremely small (**vanishes**) or extremely large (**explodes**).\n",
    "- This makes training unstable or ineffective for capturing long-range context.\n",
    "\n",
    "**Solution**: Specialized RNN variants like **LSTM** or **GRU** mitigate these issues by incorporating gating mechanisms.\n",
    "\n",
    "\n",
    "### Research Note: let's invent a GRU\n",
    "\n",
    "**RNN** : $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$\n",
    "\n",
    "* **Problem:** To compute the gradient of $h_1$ (or any early token), we need to multiply the gradients by small values in $W_h$, thus **vanishing** it.\n",
    "* **Solution:** Intelligently choose the previous memory: $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$ or $h_t = h_{t-1}$\n",
    "\n",
    "**RNN with no vanishing** : $h_t = \\alpha\\odot\\hat{h}_t + (1-\\alpha)\\odot h_{t-1}$, where $\\hat{h}_t=\\phi(W_hh_{t-1} + W_xx_{t})$\n",
    "\n",
    "* **Problem:** To compute the gradient of $h_1$ (or any early token), we need to multiply the gradients by large values in $W_h$, thus **exploding** it.\n",
    "* **Solution:** Intelligently choose to set the previous memory to zero before multiplying it by the weights: $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$ or $h_t = \\phi(W_xx_{t})$\n",
    "\n",
    "**RNN with no explosion** : $h_t = \\phi(W_h(\\beta \\odot h_{t-1}) + W_xx_{t})$\n",
    "\n",
    "* **Problem:** How do we decide on the values of $\\alpha$ and $\\beta$?\n",
    "* **Solution:** Don't! Let the data decide (learning)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\overbrace{\\alpha\\odot\\underbrace{\\phi\\left(W_h(\\beta \\odot h_{t-1}) + W_xx_{t}\\right)}_{\\text{no explosion}} + (1-\\alpha)\\odot h_{t-1}}^\\text{no vanishing} \\\\\n",
    "\\text{where}\\\\\n",
    "\\alpha &= \\sigma\\left(Ah_{t-1} + Bx_t\\right) &&\\text{Reset Gate}\\\\\n",
    "\\beta &= \\sigma\\left(Ch_{t-1} + Dx_t\\right) &&\\text{Update Gate}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Congratulations**, you have just invented a **Gated Recurrent Network**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281f15f-52e2-4038-9fa1-116d35e884fa",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Exercise 2: Implement a Toy RNN Cell\n",
    "**Goal**:  \n",
    "1. Write a Python function that computes a single time-step of an RNN.  \n",
    "2. Use NumPy or PyTorch (in NumPy style) to do the matrix multiplication and a `tanh` activation.  \n",
    "3. Test it on a small input (e.g., input dimension of 5, hidden dimension of 3).  \n",
    "\n",
    "**Hint**:  \n",
    "```python\n",
    "import torch\n",
    "\n",
    "def rnn_step(x_t, h_prev, Wxh, Whh, bh):\n",
    "    # x_t: shape (batch_size, input_dim)\n",
    "    # h_prev: shape (batch_size, hidden_dim)\n",
    "    # Wxh: shape (input_dim, hidden_dim)\n",
    "    # Whh: shape (hidden_dim, hidden_dim)\n",
    "    # bh: shape (hidden_dim,)\n",
    "    # return h_t: shape (batch_size, hidden_dim)\n",
    "    pass\n",
    "```\n",
    "*(Keep it simple—focus on the concept, not a full RNN unrolled over time.)*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561a7ed-91c1-4cca-81f9-bf7e38c6a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
