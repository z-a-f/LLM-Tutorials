{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c286d0-48dc-4707-bb04-10cc87727c3e",
   "metadata": {
    "id": "01c286d0-48dc-4707-bb04-10cc87727c3e"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e996f26d-f6cf-444c-bac2-511c70ef9425",
   "metadata": {
    "id": "e996f26d-f6cf-444c-bac2-511c70ef9425"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed3798-5270-4bd3-b0d4-c7bfd6b42a8f",
   "metadata": {
    "id": "00ed3798-5270-4bd3-b0d4-c7bfd6b42a8f"
   },
   "source": [
    "# Session 2 – NLP Models and Training Basics (RNN)\n",
    "  \n",
    "In this notebook, we will dive into fundamental sequence models such as **RNNs** and **LSTMs**. We’ll also cover basic neural embeddings, training objectives, and see how to implement and train a **simple text generator** using an LSTM.\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction and Overview](#introduction)\n",
    "2. [Recurrent Neural Networks (RNNs)](#rnns)\n",
    "   - [The RNN Cell](#rnn-cell)\n",
    "   - [Vanishing and Exploding Gradients](#vanishing)\n",
    "3. [Long Short-Term Memory (LSTM)](#lstm)\n",
    "   - [Key Intuition Behind LSTM Gates](#lstm-gates)\n",
    "4. [Embeddings](#embeddings)\n",
    "5. [Basic Training Objectives in Language Modeling](#training-objectives)\n",
    "   - [Next Token Prediction](#next-token-pred)\n",
    "   - [Perplexity](#perplexity)\n",
    "6. [Implementing a Simple LSTM Text Generator in PyTorch](#implementation)\n",
    "   - [Data Preparation](#data-prep)\n",
    "   - [Model Definition](#model-def)\n",
    "   - [Training Loop](#training-loop)\n",
    "   - [Generating Text](#generate-text)\n",
    "\n",
    "Each section will be followed by one or more **Exercises** to help you practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b50b1-32ad-4ce9-a35f-4f4e2d8bb4f0",
   "metadata": {
    "id": "bc7b50b1-32ad-4ce9-a35f-4f4e2d8bb4f0"
   },
   "source": [
    "# <a id=\"overview\"></a>1. Overview and Setup\n",
    "\n",
    "This tutorial assumes you have:\n",
    "\n",
    "- **Basic Python** knowledge.\n",
    "- A local or cloud environment (e.g., Jupyter, Colab) with **PyTorch** installed.\n",
    "  - If needed, install PyTorch via `pip install torch` or follow instructions at [pytorch.org](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "No prior reading of other sessions is required; we’ll present all the essentials here.\n",
    "\n",
    "### Quick Setup Check\n",
    "```python\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "```\n",
    "\n",
    "Ensure you see a version number (e.g., `2.0.0` or similar) printed. If you get an error, please install or update PyTorch before continuing.\n",
    "\n",
    "- We’ll focus on **RNNs** and **LSTMs**.  \n",
    "- We’ll learn **why** they are powerful for sequential data.  \n",
    "- We’ll cover **basic training objectives** (like next-token prediction) for language modeling.  \n",
    "- Finally, we’ll implement a small **LSTM-based text generator**.\n",
    "\n",
    "**By the end of this session**, you should be able to:\n",
    "1. Understand how an RNN cell and LSTM cell process sequential data.  \n",
    "2. Implement an **LSTM** in a deep learning framework (here, PyTorch).  \n",
    "3. Train and evaluate a **text-generation** model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d5d4f43-f69f-4efc-9c2f-6eafe5fca19b",
   "metadata": {
    "id": "1d5d4f43-f69f-4efc-9c2f-6eafe5fca19b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a964978-2cdb-4568-9f2b-97b81c07b33a",
   "metadata": {
    "id": "0a964978-2cdb-4568-9f2b-97b81c07b33a"
   },
   "source": [
    "# 2. Recurrent Neural Networks (RNNs)<a id=\"rnns\"></a>\n",
    "\n",
    "Recurrent Neural Networks are designed to handle **sequential data** by maintaining a hidden state that captures information about previous time steps.\n",
    "\n",
    "## Key Idea\n",
    "At each time step $t$:\n",
    "1. The RNN takes an input $x_t$ and the hidden state from the previous time step $h_{t-1}$.\n",
    "2. It produces a new hidden state $h_t$.\n",
    "\n",
    "Mathematically, a very **basic** RNN can be written as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h) \\\\\n",
    "y_t &= W_{hy} h_t + b_y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $h_t$ is the updated hidden state.\n",
    "- $y_t$ is the output at time step $t$ (used for tasks like classification or next-token prediction).\n",
    "- $W_{hh}, W_{xh}, W_{hy}$ are learned weight matrices.\n",
    "\n",
    "**Rearrangement of Terms**\n",
    "\n",
    "Notice that the term $W_{hh} h_{t-1} + W_{xh} x_t$ uses two matrix multiplications and an addition.\n",
    "Unless compiled, these two multiplications will be performed sequentially.\n",
    "We can gain a slight improvement if we concatenate $h$ and $x$, and use a single matrix multiplication by a larger weight matrix:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\tanh(W_h H_t + b_h) \\\\\n",
    "y_t &= W_{hy} h_t + b_y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $H_t = [h_{t-1}||x_t]$ is the concatenation of $h$ and $x$\n",
    "- $W_{h} = [W_{hh}||W_{xh}]$ is the cconcatenaation of $W_{hh}, W_{xh}$\n",
    "\n",
    "\n",
    "<img src=\"img_src/RNNs.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf63a95-efb9-43c0-ad96-b8731e927366",
   "metadata": {
    "id": "6bf63a95-efb9-43c0-ad96-b8731e927366"
   },
   "source": [
    "\n",
    "## <a id=\"rnn-cell\"></a>The RNN Cell\n",
    "\n",
    "The **RNN cell** is the fundamental computational unit. At time step $t$:\n",
    "1. **Input**: current token (often embedded) + previous hidden state.\n",
    "2. **Output**: updated hidden state + optional output vector.\n",
    "\n",
    "If you unroll this cell over time for $T$ steps, you get a **computation graph** that looks like a chain, where each link is an RNN cell.\n",
    "\n",
    "<img src=\"img_src/RNN-folded.svg\"/>\n",
    "<img src=\"img_src/RNN-unfolded.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281f15f-52e2-4038-9fa1-116d35e884fa",
   "metadata": {
    "id": "b281f15f-52e2-4038-9fa1-116d35e884fa"
   },
   "source": [
    "### Exercise: Implement a Toy RNN Cell\n",
    "**Goal**:  \n",
    "1. Write a Python function that computes a single time-step of an RNN.  \n",
    "1. Use NumPy or PyTorch (in NumPy style) to do the matrix multiplication and a `tanh` activation.  \n",
    "1. Test it on a small input (e.g., input dimension of 5, hidden dimension of 3).\n",
    "\n",
    "*(Keep it simple—focus on the concept, not a full RNN unrolled over time.)*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0561a7ed-91c1-4cca-81f9-bf7e38c6a8da",
   "metadata": {
    "id": "0561a7ed-91c1-4cca-81f9-bf7e38c6a8da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def rnn_step(x_t, h_prev, Wxh, Whh, bh):\n",
    "    \"\"\"Simple RNN step\n",
    "\n",
    "    Args:\n",
    "        x_t: shape (batch_size, input_dim)\n",
    "        h_prev: shape (batch_size, hidden_dim)\n",
    "        Wxh: shape (input_dim, hidden_dim)\n",
    "        Whh: shape (hidden_dim, hidden_dim)\n",
    "        bh: shape (hidden_dim,)\n",
    "    Returns:\n",
    "        h_t: shape (batch_size, hidden_dim)\n",
    "    \"\"\"\n",
    "    weighted_h = h_prev @ Whh  # shape: (batch_size, hidden_dim)\n",
    "    weighted_x = x_t @ Wxh  # shape: (batch_size, hidden_dim)\n",
    "    linear_hx = weighted_h + weighted_x + bh\n",
    "    nonlinear_hx = torch.tanh(linear_hx)\n",
    "    return nonlinear_hx\n",
    "\n",
    "# Tests -- we only consider shapes here\n",
    "N = (1, 2, 5)  # Batch sizes\n",
    "hidden_dims = (1, 2, 5)  # Hidden sizes\n",
    "input_dims = (1, 2, 5)  # Input sizes\n",
    "\n",
    "failed_cases = []\n",
    "for batch_size, hdim, xdim in itertools.product(N, hidden_dims, input_dims):\n",
    "    x = torch.ones(batch_size, xdim)\n",
    "    h = torch.zeros(batch_size, hdim)\n",
    "    Wxh = torch.ones(xdim, hdim)\n",
    "    Whh = torch.ones(hdim, hdim)\n",
    "    bh = torch.zeros(hdim)\n",
    "    expect_shape = (batch_size, hdim)\n",
    "    with torch.no_grad():\n",
    "        h_next = rnn_step(x, h, Wxh, Whh, bh)\n",
    "    if h_next.shape != expect_shape:\n",
    "        print('x', end='')\n",
    "        failed_cases.append((h_next.shape, expect_shape))\n",
    "    else:\n",
    "        print('.', end='')\n",
    "print()\n",
    "for got, expected in failed_cases:\n",
    "    print(f\"{expected} vs. {got}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb613ff0-dde2-4208-a71a-63bccf0a62c8",
   "metadata": {
    "id": "cb613ff0-dde2-4208-a71a-63bccf0a62c8"
   },
   "outputs": [],
   "source": [
    "# PyTorch implementation\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_dim, state_dim):\n",
    "        super().__init__(data_dim, state_dim)\n",
    "\n",
    "        # Define parameters to train\n",
    "        self.input_linear = nn.Linear(  # Takes [x||h_prev] and produces h_next\n",
    "            in_features=self.data_dim + self.state_dim,\n",
    "            out_features=self.state_dim,\n",
    "            bias=True)\n",
    "        self.output_linear = nn.Linear(  # Takes h_next and produces y\n",
    "            in_features=self.state_dim,\n",
    "            out_features=self.data_dim,\n",
    "            bias=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        # Concatenate x and hidden_state\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.shape[0], self.state_dim, device=x.device, dtype=x.dtype)\n",
    "        xh = torch.hstack([x, h])\n",
    "\n",
    "        # Compute new hidden state\n",
    "        xh = self.input_linear(xh)\n",
    "        h_next = self.tanh(xh)\n",
    "\n",
    "        # Compute output\n",
    "        y = self.output_linear(h_next)\n",
    "        return y, h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113605e-5e6d-4762-909e-a0a218010c32",
   "metadata": {
    "id": "a113605e-5e6d-4762-909e-a0a218010c32"
   },
   "source": [
    "## <a id=\"vanishing\"></a>Vanishing and Exploding Gradients\n",
    "\n",
    "**Problem**: Simple RNNs often struggle with **long-term dependencies** due to **vanishing** or **exploding gradients**. That means:\n",
    "- When sequences are long, the gradient that flows backward through time either becomes extremely small (**vanishes**) or extremely large (**explodes**).\n",
    "- This makes training unstable or ineffective for capturing long-range context.\n",
    "\n",
    "**Solution**: Specialized RNN variants like **LSTM** or **GRU** mitigate these issues by incorporating gating mechanisms.\n",
    "\n",
    "### Research Note: let's invent a GRU (Gated Recurrent Unit)\n",
    "\n",
    "**RNN** : $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$\n",
    "\n",
    "* **Problem:** To compute the gradient of $h_1$ (or any early token), we need to multiply the gradients by small values in $W_h$, thus **vanishing** it.\n",
    "* **Solution:** Intelligently choose the previous memory: $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$ or $h_t = h_{t-1}$\n",
    "\n",
    "**RNN with no vanishing** : $h_t = \\alpha\\odot\\hat{h}_t + (1-\\alpha)\\odot h_{t-1}$, where $\\hat{h}_t=\\phi(W_hh_{t-1} + W_xx_{t})$\n",
    "\n",
    "* **Problem:** To compute the gradient of $h_1$ (or any early token), we need to multiply the gradients by large values in $W_h$, thus **exploding** it.\n",
    "* **Solution:** Intelligently choose to set the previous memory to zero before multiplying it by the weights: $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$ or $h_t = \\phi(W_xx_{t})$\n",
    "\n",
    "**RNN with no explosion** : $h_t = \\phi(W_h(\\beta \\odot h_{t-1}) + W_xx_{t})$\n",
    "\n",
    "* **Problem:** How do we decide on the values of $\\alpha$ and $\\beta$?\n",
    "* **Solution:** Don't! Let the data decide (learning)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\overbrace{\\alpha\\odot\\underbrace{\\phi\\left(W_h(\\beta \\odot h_{t-1}) + W_xx_{t}\\right)}_{\\text{no explosion}} + (1-\\alpha)\\odot h_{t-1}}^\\text{no vanishing} \\\\\n",
    "\\text{where}\\\\\n",
    "\\alpha &= \\sigma\\left(Ah_{t-1} + Bx_t\\right) &&\\text{Memory Update Gate}\\\\\n",
    "\\beta &= \\sigma\\left(Ch_{t-1} + Dx_t\\right) &&\\text{Memory Reset Gate}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Congratulations**, you have just invented a **Gated Recurrent Unit** (GRU)!\n",
    "\n",
    "*An earlier version of a gated recurrent network is [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory), which follows very similar logic for preserving the long-term context infromation.*\n",
    "\n",
    "| Network | Complexity | Long-Term Relationship | Gradient Issues |\n",
    "|---------|--------------|------|----|\n",
    "| RNN (tanh) | (++) | None | (-) |\n",
    "| GRU | (+) | (+)<br/>Single state | (++) |\n",
    "| LSTM | (--) | (++)<br/>Separate state for long and short terms | (++) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b445c-237d-4f50-a24c-b8cc229f8e8d",
   "metadata": {
    "id": "ba8b445c-237d-4f50-a24c-b8cc229f8e8d"
   },
   "source": [
    "# <a id=\"lstm\"></a>3. Long Short-Term Memory (LSTM)\n",
    "\n",
    "A **Long Short-Term Memory (LSTM)** network is a type of RNN specifically designed to better capture **long-range dependencies**. It addresses the vanishing/exploding gradient problem through gates that control the flow of information.\n",
    "\n",
    "### <a id=\"lstm-gates\"></a>Key Intuition Behind LSTM Gates\n",
    "\n",
    "Typical LSTM equations:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "f_t &= \\sigma(W_f [h_{t-1}, x_t] + b_f)\n",
    "&\\quad(\\text{Forget Gate}) \\\\\n",
    "i_t &= \\sigma(W_i [h_{t-1}, x_t] + b_i)\n",
    "&\\quad(\\text{Input Gate}) \\\\\n",
    "\\tilde{C_t} &= \\tanh(W_C [h_{t-1}, x_t] + b_C)\n",
    "&\\quad(\\text{Candidate Values}) \\\\\n",
    "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C_t}\n",
    "&\\quad(\\text{Cell State Update}) \\\\\n",
    "o_t &= \\sigma(W_o [h_{t-1}, x_t] + b_o)\n",
    "&\\quad(\\text{Output Gate}) \\\\\n",
    "h_t &= o_t \\odot \\tanh(C_t)\n",
    "&\\quad(\\text{New Hidden State})\n",
    "\\end{aligned}$$\n",
    "\n",
    "- **Forget Gate** ($f_t$): decides how much old state to keep.\n",
    "- **Input Gate** ($i_t$): decides how much new information to add.\n",
    "- **Candidate** ($\\tilde{C_t}$): proposed update to the cell state.\n",
    "- **Output Gate** ($o_t$): decides how much cell state to output as hidden state.\n",
    "\n",
    "This gating mechanism helps **preserve gradients** across many time steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f51e6-2a10-4cec-aff8-e581bc4b072f",
   "metadata": {
    "id": "296f51e6-2a10-4cec-aff8-e581bc4b072f"
   },
   "source": [
    "### Exercise: Compare RNN and GRU Outputs (or LSTM if you prefer)\n",
    "1. Create a synthetic sequence of length 20.  \n",
    "2. Feed it into a small **Vanilla RNN** and a small **GRU** (in PyTorch).  \n",
    "3. Compare the final hidden states after feeding all time steps. Are they similar? If you vary the length from 20 to 50 to 100, how do the hidden states change?\n",
    "\n",
    "*Hint*: This is a conceptual experiment. You can use random inputs, then measure how the hidden states drift over longer sequences.\n",
    "\n",
    "*Hint*: **If you really want**, you can use the utilities in the `utils.py` file to generate simple synthetic sequences (`generate_synthetic_sequences`).\n",
    "\n",
    "*Hint*: You don't have to train the network, but if you want to you can use `utils.py` (`train_recurrent`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8212c123-1c29-414d-b919-0625d7673f88",
   "metadata": {
    "id": "8212c123-1c29-414d-b919-0625d7673f88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden output shapes: h_rnn.shape=torch.Size([1, 3, 2]), h_gru.shape=torch.Size([1, 3, 2]), h_lstm.shape=torch.Size([1, 3, 2]), c_lstm.shape=torch.Size([1, 3, 2])\n",
      "Hidden state norms: 4.28e-01, 1.77e+00, 3.74e+00\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "batch_size = 3\n",
    "sequence_length = 1000\n",
    "xdim = 3\n",
    "hdim = 2\n",
    "\n",
    "x = torch.ones(sequence_length, batch_size, xdim)  # Input sequence\n",
    "h = torch.zeros(1, batch_size, hdim)  # Initial hidden state / initial memory\n",
    "\n",
    "rnn_model = nn.RNN(input_size=xdim, hidden_size=hdim, num_layers=1, batch_first=False, bidirectional=False)\n",
    "gru_model = nn.GRU(input_size=xdim, hidden_size=hdim, num_layers=1, batch_first=False, bidirectional=False)\n",
    "lstm_model = nn.LSTM(input_size=xdim, hidden_size=hdim, batch_first=False)\n",
    "\n",
    "rnn_model.zero_grad()\n",
    "gru_model.zero_grad()\n",
    "lstm_model.zero_grad()\n",
    "\n",
    "y_rnn, h_rnn = rnn_model(x)\n",
    "y_gru, h_gru = gru_model(x)\n",
    "y_lstm, (h_lstm, c_lstm) = lstm_model(x)\n",
    "\n",
    "print(f\"Hidden output shapes: {h_rnn.shape=}, {h_gru.shape=}, {h_lstm.shape=}, {c_lstm.shape=}\")\n",
    "\n",
    "# Very basic error -- just minimizing the norm of the memory\n",
    "rnn_error = h_rnn.norm()\n",
    "gru_error = h_gru.norm()\n",
    "lstm_error = h_lstm.norm() + c_lstm.norm()\n",
    "\n",
    "rnn_error.backward()\n",
    "gru_error.backward()\n",
    "lstm_error.backward()\n",
    "\n",
    "print(f\"Hidden state norms: {rnn_error:.2e}, {gru_error:.2e}, {lstm_error:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab50badc-ac31-43ff-a469-da174722fe5c",
   "metadata": {
    "id": "ab50badc-ac31-43ff-a469-da174722fe5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2607, 0.2397],\n",
       "         [0.2731, 0.2510]]),\n",
       " tensor([[ 5.2188e-02, -3.5770e-02],\n",
       "         [-2.9516e-02,  2.0231e-02],\n",
       "         [-1.4009e-07,  9.6021e-08],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.8071e-01, -1.2386e-01],\n",
       "         [-3.6439e-01,  2.4975e-01]]),\n",
       " tensor([[ 0.0130,  0.1285],\n",
       "         [ 0.0406,  0.4024],\n",
       "         [ 0.0134,  0.1326],\n",
       "         [ 0.1050,  1.0418],\n",
       "         [ 0.0546,  0.5411],\n",
       "         [ 0.1515,  1.5027],\n",
       "         [ 0.0110,  0.1088],\n",
       "         [-0.0063, -0.0625]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.weight_hh_l0.grad, gru_model.weight_hh_l0.grad, lstm_model.weight_hh_l0.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b2473-c806-405e-b0c3-647151ebdcba",
   "metadata": {
    "id": "bd0b2473-c806-405e-b0c3-647151ebdcba"
   },
   "source": [
    "# <a id=\"embeddings\"></a>4. Embeddings\n",
    "\n",
    "**TODO: Embedding and Latent Space Explanation**\n",
    "\n",
    "When dealing with text, each word or token is usually mapped to an **embedding** vector rather than a large one-hot vector.\n",
    "\n",
    "- **Embedding Layer**: A learnable matrix that maps token indices to dense vectors of fixed dimension $d$.\n",
    "- This helps the model learn **semantic relationships** between words.\n",
    "\n",
    "For example:\n",
    "- Word “hello” → index 5 → embedding vector $\\mathbf{e} \\in \\mathbb{R}^d$.\n",
    "\n",
    "Most frameworks (like PyTorch) provide a built-in layer, `nn.Embedding(vocab_size, embed_dim)`, that handles this.\n",
    "\n",
    "\n",
    "### Exercise 3: Custom Embedding Lookup\n",
    "1. Create a small vocabulary of 5 tokens.  \n",
    "2. Initialize a random embedding matrix of shape $(5, d)$.  \n",
    "3. Write a function that takes a token index and returns the corresponding embedding row.  \n",
    "4. Compare with `nn.Embedding` in PyTorch for the same matrix initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060618e9-54bd-4212-8fd4-e1bf674ca2f4",
   "metadata": {
    "id": "060618e9-54bd-4212-8fd4-e1bf674ca2f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom lookup vector: tensor([ 2.2082, -0.6380,  0.4617])\n",
      "nn.Embedding lookup vector: tensor([ 2.2082, -0.6380,  0.4617])\n",
      "Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "embed_dim = 3\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Step 1: create a random embedding matrix\n",
    "embedding_matrix = torch.randn(vocab_size, embed_dim)\n",
    "\n",
    "def custom_embed_lookup(token_idx, embedding_matrix):\n",
    "    \"\"\"\n",
    "    token_idx: int index (0 <= token_idx < vocab_size)\n",
    "    embedding_matrix: shape (vocab_size, embed_dim)\n",
    "    returns: torch.Tensor of shape (embed_dim,)\n",
    "    \"\"\"\n",
    "    return embedding_matrix[token_idx]\n",
    "\n",
    "# Pick a test token index\n",
    "test_idx = 2\n",
    "custom_vec = custom_embed_lookup(test_idx, embedding_matrix)\n",
    "print(\"Custom lookup vector:\", custom_vec)\n",
    "\n",
    "# Step 2: Compare with nn.Embedding\n",
    "embed_layer = nn.Embedding(vocab_size, embed_dim)\n",
    "# Overwrite the embedding_layer's weights with our random matrix\n",
    "with torch.no_grad():\n",
    "    embed_layer.weight.copy_(embedding_matrix)\n",
    "\n",
    "# Now let's see if it matches:\n",
    "with torch.no_grad():\n",
    "    torch_vec = embed_layer(torch.tensor([test_idx]))\n",
    "print(\"nn.Embedding lookup vector:\", torch_vec.squeeze(0))\n",
    "\n",
    "# They should be (almost) identical\n",
    "print(\"Difference:\", (custom_vec - torch_vec.squeeze(0)).abs().sum().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e7f84-2962-4b60-9176-e30ef3d352ae",
   "metadata": {
    "id": "ff3e7f84-2962-4b60-9176-e30ef3d352ae"
   },
   "source": [
    "# <a id=\"training-objectives\"></a>5. Training Objectives in Language Modeling\n",
    "\n",
    "In language modeling, a typical goal is **next-token prediction**: given the previous tokens, predict the next one. We often use **cross-entropy loss** and measure model performance with **perplexity**.\n",
    "\n",
    "### <a id=\"next-token-prediction\"></a>Next Token Prediction\n",
    "\n",
    "For a vocabulary of size $V$, the model outputs a probability distribution over the next token:\n",
    "$$\n",
    "P(x_t \\mid x_{t-1}, x_{t-2}, \\ldots, x_1)\n",
    "$$\n",
    "The training loss for a sequence might be:\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t}\\log P(\\hat{x}_t = x_t)\n",
    "$$\n",
    "where $ x_t $ is the ground truth and $\\hat{x}_t$ is the predicted distribution.\n",
    "\n",
    "\n",
    "### <a id=\"perplexity\"></a>Perplexity\n",
    "\n",
    "**Perplexity (PPL)** is a common metric for language models:\n",
    "$$\n",
    "\\text{PPL} = \\exp\\left(-\\frac{1}{N}\\sum_{t=1}^{N} \\log P(x_t)\\right),\n",
    "$$\n",
    "where $N$ is the total number of tokens in the test set. Lower PPL typically means a better language model.\n",
    "\n",
    "\n",
    "### Exercise 4: Manual Cross-Entropy\n",
    "- Let your model output a probability vector $[0.2, 0.3, 0.1, 0.4]$ for a 4-word vocabulary.  \n",
    "- Suppose the correct label is index 3. Manually compute cross-entropy.  \n",
    "- Compare with `torch.nn.functional.cross_entropy` to confirm your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4a59189-4da6-43f3-b525-53b24e77e6d4",
   "metadata": {
    "id": "f4a59189-4da6-43f3-b525-53b24e77e6d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual cross-entropy: 0.916290716972994\n",
      "PyTorch cross-entropy: 0.9162907004356384\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "probs = torch.tensor([0.2, 0.3, 0.1, 0.4])\n",
    "true_label = 3  # index 3 is the correct label\n",
    "\n",
    "# 1) Manual cross-entropy\n",
    "manual_ce = -math.log(probs[true_label].item())\n",
    "\n",
    "# 2) Using PyTorch (note that F.cross_entropy expects logits, not probabilities!)\n",
    "# So we need to convert probabilities => logits with log-softmax inverse => logit = log(p_i / 1)\n",
    "# But simpler is to do cross_entropy on log(prob) by building a single \"batch\" example:\n",
    "logits = torch.log(probs).unsqueeze(0)  # shape (1, 4)\n",
    "targets = torch.tensor([true_label])    # shape (1,)\n",
    "\n",
    "ce_torch = F.nll_loss(logits, targets)  # nll_loss expects log-probabilities\n",
    "# or equivalently: ce_torch = F.cross_entropy(logits, targets) if we interpret logits as log-probs\n",
    "\n",
    "print(\"Manual cross-entropy:\", manual_ce)\n",
    "print(\"PyTorch cross-entropy:\", ce_torch.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b643c3-dfed-4493-a590-67e5a8c35f06",
   "metadata": {
    "id": "63b643c3-dfed-4493-a590-67e5a8c35f06"
   },
   "source": [
    "# <a id=\"implementation\"></a>6. Implementing a Simple LSTM Text Generator in PyTorch\n",
    "\n",
    "Let’s build a small example that:\n",
    "1. **Prepares a tiny text dataset**.\n",
    "2. Splits it into input–target pairs for next-token prediction.\n",
    "3. Defines and trains an LSTM-based model.\n",
    "4. **Generates** text from the trained model.\n",
    "\n",
    "### <a id=\"data-prep\"></a>Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17cf4560-45b6-48cd-b578-6b6ce53618d1",
   "metadata": {
    "id": "17cf4560-45b6-48cd-b578-6b6ce53618d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['again', 'pytorch', 'world', 'hello']\n",
      "Mapping (word -> idx): {'again': 0, 'pytorch': 1, 'world': 2, 'hello': 3}\n",
      "Vocab size: 4\n",
      "Input sequences shape: torch.Size([4, 3])\n",
      "Target words shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Example small text\n",
    "text = \"hello world hello pytorch hello world again\"\n",
    "\n",
    "# Tokenize (word-level for simplicity)\n",
    "words = text.split()\n",
    "vocab = list(set(words))\n",
    "vocab_size = len(vocab)\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Mapping (word -> idx):\", word2idx)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# Convert words to indices\n",
    "indices = [word2idx[w] for w in words]\n",
    "\n",
    "# We'll choose a sequence length\n",
    "seq_length = 3\n",
    "\n",
    "# Prepare training data\n",
    "input_sequences = []\n",
    "target_words = []\n",
    "\n",
    "for i in range(len(indices) - seq_length):\n",
    "    input_seq = indices[i:i+seq_length]   # 3 words\n",
    "    target = indices[i+seq_length]        # the 4th word is the label\n",
    "    input_sequences.append(input_seq)\n",
    "    target_words.append(target)\n",
    "\n",
    "input_sequences = torch.tensor(input_sequences, dtype=torch.long)\n",
    "target_words = torch.tensor(target_words, dtype=torch.long)\n",
    "\n",
    "print(\"Input sequences shape:\", input_sequences.shape)\n",
    "print(\"Target words shape:\", target_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8569230-bddd-4cc6-8585-25fd1c0e551c",
   "metadata": {
    "id": "b8569230-bddd-4cc6-8585-25fd1c0e551c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 3] => 1\n",
      "  ['hello', 'world', 'hello'] => pytorch\n",
      "[2 3 1] => 3\n",
      "  ['world', 'hello', 'pytorch'] => hello\n",
      "[3 1 3] => 2\n",
      "  ['hello', 'pytorch', 'hello'] => world\n",
      "[1 3 2] => 0\n",
      "  ['pytorch', 'hello', 'world'] => again\n"
     ]
    }
   ],
   "source": [
    "for seq, targ in zip(input_sequences, target_words):\n",
    "    seq = seq.numpy()\n",
    "    targ = targ.item()\n",
    "    seq_detokenized = list(map(idx2word.get, seq))\n",
    "    targ_detokenized = idx2word.get(targ)\n",
    "    print(f\"{seq} => {targ}\")\n",
    "    print(f\"  {seq_detokenized} => {targ_detokenized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527d7b4-87ad-475e-aee0-86eaf23edb82",
   "metadata": {
    "id": "9527d7b4-87ad-475e-aee0-86eaf23edb82"
   },
   "source": [
    "### <a id=\"model-def\"></a>Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af099acf-c0d2-4136-af58-4f9e401fc993",
   "metadata": {
    "id": "af099acf-c0d2-4136-af58-4f9e401fc993"
   },
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        # 1) Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # 2) LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        # 3) Linear output layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)                # (batch_size, seq_length, embed_dim)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embedded)  # (batch_size, seq_length, hidden_dim)\n",
    "        final_hidden = lstm_out[:, -1, :]           # last time step\n",
    "        logits = self.fc(final_hidden)              # (batch_size, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a9d58-cc34-4a2f-8d13-798bf443ad5b",
   "metadata": {
    "id": "b66a9d58-cc34-4a2f-8d13-798bf443ad5b"
   },
   "source": [
    "### <a id=\"training-loop\"></a>Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a67dcc63-f36a-4f6d-a383-ee3a29013102",
   "metadata": {
    "id": "a67dcc63-f36a-4f6d-a383-ee3a29013102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/200], Loss: 0.0101\n",
      "Epoch [100/200], Loss: 0.0032\n",
      "Epoch [150/200], Loss: 0.0019\n",
      "Epoch [200/200], Loss: 0.0013\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 8\n",
    "hidden_dim = 16\n",
    "learning_rate = 0.01\n",
    "num_epochs = 200\n",
    "\n",
    "model = SimpleLSTM(vocab_size, embed_dim, hidden_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_sequences)  # shape: (batch_size, vocab_size)\n",
    "    loss = criterion(logits, target_words)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35484b03-2b3d-499a-9677-cad88a37fdd0",
   "metadata": {
    "id": "35484b03-2b3d-499a-9677-cad88a37fdd0"
   },
   "source": [
    "### <a id=\"generate-text\"></a>Generating Text\n",
    "\n",
    "We can now generate text by **sampling** the model’s predictions iteratively.\n",
    "\n",
    "Feel free to experiment with:\n",
    "- **Different seeds**.\n",
    "- **Different sampling strategies** (e.g., greedy vs. top-k).  \n",
    "- A **larger corpus** (like Tiny Shakespeare)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "049323c0-a716-4c73-83b9-083df7d48bbe",
   "metadata": {
    "id": "049323c0-a716-4c73-83b9-083df7d48bbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: hello world hello pytorch hello world again pytorch\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_words, num_words=5):\n",
    "    model.eval()\n",
    "    words_generated = seed_words[:]\n",
    "\n",
    "    # Convert seed_words to indices\n",
    "    current_seq = [word2idx[w] for w in seed_words]\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        inp = torch.tensor([current_seq], dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            logits = model(inp)  # shape: (1, vocab_size)\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()  # shape: (vocab_size,)\n",
    "\n",
    "        # Sample from probability distribution\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        next_word = idx2word[next_idx]\n",
    "        words_generated.append(next_word)\n",
    "\n",
    "        # Slide the window (drop the first index, append new index)\n",
    "        current_seq = current_seq[1:] + [next_idx]\n",
    "\n",
    "    return \" \".join(words_generated)\n",
    "\n",
    "# Let's try generating with a seed of length = seq_length (3)\n",
    "seed = [\"hello\", \"world\", \"hello\"]  # must be in vocab\n",
    "generated_text = generate_text(model, seed, num_words=5)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6267e-67ac-4047-b6a0-ceef19c69c5d",
   "metadata": {
    "id": "33f6267e-67ac-4047-b6a0-ceef19c69c5d"
   },
   "source": [
    "### Exercise: Experiment with the Generator\n",
    "1. Change the `num_words` to 10 or 20 and see if your text generation forms any repetitive patterns.  \n",
    "2. Try a **larger** dataset if you have one. Compare the coherence of the generated text.  \n",
    "3. Print out intermediate hidden states if you’re curious about how the model’s representation changes over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6dbfa72-a426-479f-ab2d-ea7c7ef65039",
   "metadata": {
    "id": "d6dbfa72-a426-479f-ab2d-ea7c7ef65039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " hello world hello pytorch hello world again pytorch hello world again\n",
      "Intermediate hidden states shapes:\n",
      " Step 1: (1, 16)\n",
      " Step 2: (1, 16)\n",
      " Step 3: (1, 16)\n",
      " Step 4: (1, 16)\n",
      " Step 5: (1, 16)\n",
      " Step 6: (1, 16)\n",
      " Step 7: (1, 16)\n",
      " Step 8: (1, 16)\n"
     ]
    }
   ],
   "source": [
    "# Suppose 'model' is our trained LSTM model, 'word2idx' and 'idx2word' are our mappings.\n",
    "\n",
    "def generate_text_with_hidden(model, seed_words, num_words=10):\n",
    "    \"\"\"\n",
    "    Generate text from the model, returning the hidden states as well.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    words_generated = seed_words[:]\n",
    "    hidden_states = []   # store hidden states at each step\n",
    "\n",
    "    current_seq = [word2idx[w] for w in seed_words]\n",
    "\n",
    "    # Hidden and cell state, if needed\n",
    "    # We'll assume 1-layer LSTM, batch_size=1\n",
    "    h, c = None, None\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        inp = torch.tensor([current_seq], dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            # Modify forward pass to capture intermediate hidden states\n",
    "            # We can do this by running the embedding + LSTM manually:\n",
    "            embedded = model.embedding(inp)  # shape (1, seq_length, embed_dim)\n",
    "            # We pass in (h, c) if they exist, otherwise let the LSTM init them\n",
    "            lstm_out, (h, c) = model.lstm(embedded, (h, c) if h is not None else None)\n",
    "\n",
    "            # final time step\n",
    "            final_hidden = lstm_out[:, -1, :]\n",
    "\n",
    "            # For debugging: store the hidden state in a list\n",
    "            hidden_states.append(final_hidden.detach().cpu().numpy())\n",
    "\n",
    "            logits = model.fc(final_hidden)\n",
    "            probs = torch.softmax(logits, dim=-1).squeeze()\n",
    "            next_idx = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        next_word = idx2word[next_idx]\n",
    "        words_generated.append(next_word)\n",
    "        current_seq = current_seq[1:] + [next_idx]\n",
    "\n",
    "    return \" \".join(words_generated), hidden_states\n",
    "\n",
    "# Example usage\n",
    "seed = [\"hello\", \"world\", \"hello\"]\n",
    "generated_text, h_states = generate_text_with_hidden(model, seed, num_words=8)\n",
    "print(\"Generated Text:\\n\", generated_text)\n",
    "print(\"Intermediate hidden states shapes:\")\n",
    "for i, hs in enumerate(h_states):\n",
    "    print(f\" Step {i+1}: {hs.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01d168-505a-40e2-9daf-69039934e4af",
   "metadata": {
    "id": "eb01d168-505a-40e2-9daf-69039934e4af"
   },
   "source": [
    "# Fun Things -- Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f5a665-da25-4aff-ab81-f893698e08ae",
   "metadata": {
    "id": "25f5a665-da25-4aff-ab81-f893698e08ae"
   },
   "source": [
    "### Step 1: Pre-process the textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "054925f7-8c8f-4631-82c1-721a8bc497e1",
   "metadata": {
    "id": "054925f7-8c8f-4631-82c1-721a8bc497e1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_PATH = os.path.abspath(\"./shakespeare\")\n",
    "filelist = os.listdir(DATA_PATH)\n",
    "filelist = list(map(lambda f: os.path.join(DATA_PATH, f), filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd96c502-75fc-4493-964b-c117c557de81",
   "metadata": {
    "id": "dd96c502-75fc-4493-964b-c117c557de81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of combined text: 5283837\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_shakespeare_texts(filelist):\n",
    "    \"\"\"\n",
    "    Loads all text from filelist and returns all texts concatenated\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "\n",
    "    for file_path in filelist:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            all_text += f.read() + \"\\n\"  # add a newline at the end of each file\n",
    "\n",
    "    return all_text\n",
    "\n",
    "full_text = load_shakespeare_texts(filelist)\n",
    "print(\"Total length of combined text:\", len(full_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e4c93-8968-4b5c-96cd-31eb211e9dac",
   "metadata": {
    "id": "e26e4c93-8968-4b5c-96cd-31eb211e9dac"
   },
   "source": [
    "**Character-Level Tokenization**\n",
    "\n",
    "Since this is a character-level model, our “tokens” are just unique characters found in the text:\n",
    "\n",
    "* Identify the unique set of characters.\n",
    "* Map each character to a unique integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "846dae5c-8a8d-4bb8-bafb-75e1645e8d84",
   "metadata": {
    "id": "846dae5c-8a8d-4bb8-bafb-75e1645e8d84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chars found: 80\n",
      "Example of characters: ['\\t', '\\n', ' ', '!', '$', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create vocabulary of unique characters\n",
    "chars = sorted(list(set(full_text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Unique chars found:\", vocab_size)\n",
    "print(\"Example of characters:\", chars[:50])\n",
    "\n",
    "# Create mapping from character to index (and reverse)\n",
    "_char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "_idx2char = {i: ch for ch, i in _char2idx.items()}\n",
    "\n",
    "# Add special characters\n",
    "for special_token in [\"<|UNK|>\"]:\n",
    "    k = len(_char2idx)\n",
    "    _char2idx[special_token] = k\n",
    "    _idx2char[k] = special_token\n",
    "\n",
    "# Utility functions\n",
    "def char2idx(ch):\n",
    "    return [_char2idx.get(c, \"<|UNK|>\") for c in ch]\n",
    "def idx2char(idx):\n",
    "    if isinstance(idx, torch.Tensor):\n",
    "        return idx2char(idx.detach().cpu().numpy())\n",
    "    if isinstance(idx, np.ndarray):\n",
    "        return idx2char(idx.tolist())\n",
    "    if isinstance(idx, int):\n",
    "        return _idx2char[idx]\n",
    "    return [idx2char(i) for i in idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821ff33-b6cb-4083-9b5a-ca0f537faf20",
   "metadata": {
    "id": "c821ff33-b6cb-4083-9b5a-ca0f537faf20"
   },
   "source": [
    "**Convert Text to Indices**\n",
    "\n",
    "Convert the entire text into a list (or array) of integer indices. This will make it easier to feed into PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53f7f340-1ac9-4b56-b579-7692d2eeb667",
   "metadata": {
    "id": "53f7f340-1ac9-4b56-b579-7692d2eeb667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_tensor shape: torch.Size([5283837])\n"
     ]
    }
   ],
   "source": [
    "# Convert all text to indices\n",
    "data_as_indices = char2idx(full_text)  # [_char2idx[ch] for ch in full_text]\n",
    "data_tensor = torch.tensor(data_as_indices, dtype=torch.long)\n",
    "print(\"data_tensor shape:\", data_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f3014-1fea-420b-abcb-b2af333ecbda",
   "metadata": {
    "id": "ec8f3014-1fea-420b-abcb-b2af333ecbda"
   },
   "source": [
    "**Create Training Sequences**\n",
    "\n",
    "For character-level language modeling, a common approach is:\n",
    "\n",
    "* Pick a sequence length, e.g. seq_length = 100.\n",
    "* For each sequence of seq_length characters, the target is the next character.\n",
    "\n",
    "We can use PyTorch's `Dataset`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c288993-0ed4-48ae-9ed7-8750e1afd69f",
   "metadata": {
    "id": "2c288993-0ed4-48ae-9ed7-8750e1afd69f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 176126\n",
      "Example X (indices): tensor([ 0, 35, 33, 38, 31,  2, 32, 29, 38, 42])\n",
      "Example Y (index): tensor([35, 33, 38, 31,  2, 32, 29, 38, 42, 49])\n",
      "Example X (decoded): ['\\t', 'K', 'I', 'N', 'G', ' ', 'H', 'E', 'N', 'R']\n",
      "Example Y (decoded): ['K', 'I', 'N', 'G', ' ', 'H', 'E', 'N', 'R', 'Y']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data_tensor, seq_length):\n",
    "        self.data = data_tensor\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # We can form this many sequences (minus 1 for the target)\n",
    "        return len(self.data) // self.seq_length - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_length\n",
    "        x_seq = self.data[start : start + self.seq_length]\n",
    "        # Targets are the subsequent seq_length characters\n",
    "        y_seq = self.data[start+1 : start + self.seq_length + 1]\n",
    "        return x_seq, y_seq\n",
    "\n",
    "seq_length = 30\n",
    "dataset = CharDataset(data_tensor, seq_length=seq_length)\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "\n",
    "# For demonstration, let's get one example\n",
    "example_x, example_y = dataset[0]\n",
    "print(\"Example X (indices):\", example_x[:10])\n",
    "print(\"Example Y (index):\", example_y[:10])\n",
    "print(\"Example X (decoded):\", idx2char(example_x[:10]))# \"\".join(idx2char(i.item()) for i in example_x[:30]))\n",
    "print(\"Example Y (decoded):\", idx2char(example_y[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c84c74c4-d6eb-404f-8990-9bd8e7926d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 35, 33, 38, 31,  2, 32, 29, 38, 42, 49,  2, 46, 33, 33, 33,  1,  1,\n",
       "         1,  0, 28, 42, 25, 37, 25, 44, 33, 43,  2, 40])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2b56412e-ef4a-4e0d-a4d2-74d0f9dfc495",
   "metadata": {
    "id": "2b56412e-ef4a-4e0d-a4d2-74d0f9dfc495"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "def batch_second(batch):\n",
    "    x, y = list(zip(*batch))\n",
    "    x = torch.stack(x, 1)\n",
    "    y = torch.stack(y, 1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True, collate_fn=batch_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbc683-5192-4fab-a1a0-9e784f8922d6",
   "metadata": {
    "id": "66fbc683-5192-4fab-a1a0-9e784f8922d6"
   },
   "source": [
    "### Step 2: Model Definition (LSTM)\n",
    "\n",
    "We’ll define a character-level LSTM model:\n",
    "\n",
    "1. Embedding: maps integer character indices to dense vectors (optional, but often helps).\n",
    "1. LSTM: one or more LSTM layers that process the embedded sequence.\n",
    "1. Linear: output layer to predict the next character’s index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1ec456ca-b740-4252-8300-79cd7b3e1c76",
   "metadata": {
    "id": "1ec456ca-b740-4252-8300-79cd7b3e1c76"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "        \"\"\"\n",
    "        x: (seq_length, batch_size)\n",
    "        hidden_state: tuple (h, c) for LSTM hidden/cell states (if you want to pass it in)\n",
    "        Returns: logits (seq_length, batch_size, vocab_size), updated_hidden_state\n",
    "        \"\"\"\n",
    "        # 1) Embedding\n",
    "        embedded = self.embedding(x)  # shape: (seq_length, batch_size, embed_dim)\n",
    "\n",
    "        # 2) LSTM\n",
    "        if hidden_state is None:\n",
    "            out, (h, c) = self.lstm(embedded)  # out: (seq_length, batch_size, hidden_dim)\n",
    "        else:\n",
    "            out, (h, c) = self.lstm(embedded, hidden_state)\n",
    "\n",
    "        # 3) Fully connected (we want to produce a prediction at each time step)\n",
    "        logits = self.fc(out)  # shape: (seq_length, batch_size, vocab_size)\n",
    "\n",
    "        return logits, (h, c)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Utility to initialize the hidden state (h, c) to zeros.\n",
    "        Returns: h0, c0 (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        return (h0, c0)\n",
    "\n",
    "\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "        \"\"\"\n",
    "        x: (seq_length, batch_size)\n",
    "        hidden_state: tuple h for GRU hidden state (if you want to pass it in)\n",
    "        Returns: logits (seq_length, batch_size, vocab_size), updated_hidden_state\n",
    "        \"\"\"\n",
    "        # 1) Embedding\n",
    "        embedded = self.embedding(x)  # shape: (seq_length, batch_size, embed_dim)\n",
    "\n",
    "        # 2) GRU\n",
    "        if hidden_state is None:\n",
    "            out, h = self.gru(embedded)  # out: (seq_length, batch_size, hidden_dim)\n",
    "        else:\n",
    "            out, h = self.gru(embedded, hidden_state)\n",
    "\n",
    "        # 3) Fully connected (we want to produce a prediction at each time step)\n",
    "        logits = self.fc(out)  # shape: (seq_length, batch_size, vocab_size)\n",
    "\n",
    "        return logits, h\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Utility to initialize the hidden state h to zeros.\n",
    "        Returns: h0 (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        return h0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c75f8-ed65-4855-9ae6-dc30e1ae879e",
   "metadata": {
    "id": "fd0c75f8-ed65-4855-9ae6-dc30e1ae879e"
   },
   "source": [
    "### Step 3: Training Routine\n",
    "\n",
    "**Training Setup**\n",
    "\n",
    "We define:\n",
    "\n",
    "* A loss function (CrossEntropyLoss), typical for next-character prediction.\n",
    "* An optimizer (e.g., Adam or RMSprop).\n",
    "* Possibly device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "39ef7dfe-4f5c-402a-9d60-8c52e30d210a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39ef7dfe-4f5c-402a-9d60-8c52e30d210a",
    "outputId": "fc516760-94a3-439f-a8f4-8fd1d1c0e2ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = CharLSTM(vocab_size, embed_dim=512, hidden_dim=512, num_layers=3)\n",
    "# model = CharGRU(vocab_size, embed_dim=512, hidden_dim=512, num_layers=3)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.8, patience=5)  # Reduce learning rate by half\n",
    "\n",
    "history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb32ce-a9dc-42bc-9de1-2b97821b5b48",
   "metadata": {
    "id": "ffeb32ce-a9dc-42bc-9de1-2b97821b5b48"
   },
   "source": [
    "**Training Loop**\n",
    "\n",
    "At each iteration:\n",
    "\n",
    "1. Get a batch (x, y) from the dataloader. Here, x is of shape (batch_size, seq_length) and y of shape (batch_size,).\n",
    "1. Model outputs logits of shape (batch_size, seq_length, vocab_size).\n",
    "1. We actually want to predict the character that comes after each character in x. So we can shift by 1 step or simply note that y at index i is the final character of the sequence. But if we want a prediction at each time step (not just the last one), we might create labels of shape (batch_size, seq_length)—one label per input character.\n",
    "\n",
    "In the example below, we do the simplest approach: each sequence’s final character is the label. This means we use only the last time step’s logits to compute the loss. Alternatively, if you want to predict the next character at every time step, you’ll need to shift the labels accordingly. (We’ll show the typical approach of every time step.)\n",
    "\n",
    "**Case: Predict next char at every time step**\n",
    "\n",
    "We shift our target by 1 inside the dataset or handle it here. Let’s assume we do it at the dataset level for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7d20bcc1-b463-4cd6-b2f4-2fa04d38b624",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d20bcc1-b463-4cd6-b2f4-2fa04d38b624",
    "outputId": "a3a84a24-5151-422e-8ca3-9f05bc2862c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 1.6628, Correct prediction: 19.0%\n",
      "Epoch 10/1000, Loss: 1.2350, Correct prediction: 29.1%\n",
      "Epoch 20/1000, Loss: 1.1594, Correct prediction: 31.4%\n",
      "Epoch 30/1000, Loss: 1.1216, Correct prediction: 32.6%\n",
      "Epoch 40/1000, Loss: 1.1067, Correct prediction: 33.1%\n",
      "Epoch 50/1000, Loss: 1.1069, Correct prediction: 33.1%\n",
      "Epoch 60/1000, Loss: 1.0434, Correct prediction: 35.2%\n",
      "Epoch 70/1000, Loss: 1.0224, Correct prediction: 36.0%\n",
      "Epoch 80/1000, Loss: 1.0109, Correct prediction: 36.4%\n",
      "Epoch 90/1000, Loss: 1.0048, Correct prediction: 36.6%\n",
      "Epoch 100/1000, Loss: 1.0055, Correct prediction: 36.6%\n",
      "Epoch 110/1000, Loss: 0.9304, Correct prediction: 39.4%\n",
      "Epoch 120/1000, Loss: 0.9000, Correct prediction: 40.7%\n",
      "Epoch 130/1000, Loss: 0.8810, Correct prediction: 41.4%\n",
      "Epoch 140/1000, Loss: 0.8666, Correct prediction: 42.0%\n",
      "Epoch 150/1000, Loss: 0.8543, Correct prediction: 42.6%\n",
      "Epoch 160/1000, Loss: 0.8454, Correct prediction: 42.9%\n",
      "Epoch 170/1000, Loss: 0.8373, Correct prediction: 43.3%\n",
      "Epoch 180/1000, Loss: 0.8308, Correct prediction: 43.6%\n",
      "Epoch 190/1000, Loss: 0.8263, Correct prediction: 43.8%\n",
      "Epoch 200/1000, Loss: 0.8227, Correct prediction: 43.9%\n",
      "Epoch 210/1000, Loss: 0.8188, Correct prediction: 44.1%\n",
      "Epoch 220/1000, Loss: 0.8159, Correct prediction: 44.2%\n",
      "Epoch 230/1000, Loss: 0.7486, Correct prediction: 47.3%\n",
      "Epoch 240/1000, Loss: 0.7131, Correct prediction: 49.0%\n",
      "Epoch 250/1000, Loss: 0.6946, Correct prediction: 49.9%\n",
      "Epoch 260/1000, Loss: 0.6797, Correct prediction: 50.7%\n",
      "Epoch 270/1000, Loss: 0.6687, Correct prediction: 51.2%\n",
      "Epoch 280/1000, Loss: 0.6600, Correct prediction: 51.7%\n",
      "Epoch 290/1000, Loss: 0.6519, Correct prediction: 52.1%\n",
      "Epoch 300/1000, Loss: 0.6455, Correct prediction: 52.4%\n",
      "Epoch 310/1000, Loss: 0.6403, Correct prediction: 52.7%\n",
      "Epoch 320/1000, Loss: 0.6354, Correct prediction: 53.0%\n",
      "Epoch 330/1000, Loss: 0.6293, Correct prediction: 53.3%\n",
      "Epoch 340/1000, Loss: 0.6269, Correct prediction: 53.4%\n",
      "Epoch 350/1000, Loss: 0.6246, Correct prediction: 53.5%\n",
      "Epoch 360/1000, Loss: 0.6202, Correct prediction: 53.8%\n",
      "Epoch 370/1000, Loss: 0.6169, Correct prediction: 54.0%\n",
      "Epoch 380/1000, Loss: 0.6149, Correct prediction: 54.1%\n",
      "Epoch 390/1000, Loss: 0.6124, Correct prediction: 54.2%\n",
      "Epoch 400/1000, Loss: 0.6117, Correct prediction: 54.2%\n",
      "Epoch 410/1000, Loss: 0.6098, Correct prediction: 54.3%\n",
      "Epoch 420/1000, Loss: 0.6086, Correct prediction: 54.4%\n",
      "Epoch 430/1000, Loss: 0.6060, Correct prediction: 54.6%\n",
      "Epoch 440/1000, Loss: 0.6061, Correct prediction: 54.5%\n",
      "Epoch 450/1000, Loss: 0.5258, Correct prediction: 59.1%\n",
      "Epoch 460/1000, Loss: 0.5131, Correct prediction: 59.9%\n",
      "Epoch 470/1000, Loss: 0.5050, Correct prediction: 60.4%\n",
      "Epoch 480/1000, Loss: 0.5002, Correct prediction: 60.6%\n",
      "Epoch 490/1000, Loss: 0.4972, Correct prediction: 60.8%\n",
      "Epoch 500/1000, Loss: 0.4947, Correct prediction: 61.0%\n",
      "Epoch 510/1000, Loss: 0.4380, Correct prediction: 64.5%\n",
      "Epoch 520/1000, Loss: 0.4302, Correct prediction: 65.0%\n",
      "Epoch 530/1000, Loss: 0.4261, Correct prediction: 65.3%\n",
      "Epoch 540/1000, Loss: 0.4225, Correct prediction: 65.5%\n",
      "Epoch 550/1000, Loss: 0.4199, Correct prediction: 65.7%\n",
      "Epoch 560/1000, Loss: 0.4177, Correct prediction: 65.9%\n",
      "Epoch 570/1000, Loss: 0.4165, Correct prediction: 65.9%\n",
      "Epoch 580/1000, Loss: 0.3813, Correct prediction: 68.3%\n",
      "Epoch 590/1000, Loss: 0.3772, Correct prediction: 68.6%\n",
      "Epoch 600/1000, Loss: 0.3747, Correct prediction: 68.7%\n",
      "Epoch 610/1000, Loss: 0.3731, Correct prediction: 68.9%\n",
      "Epoch 620/1000, Loss: 0.3721, Correct prediction: 68.9%\n",
      "Epoch 630/1000, Loss: 0.3711, Correct prediction: 69.0%\n",
      "Epoch 640/1000, Loss: 0.3510, Correct prediction: 70.4%\n",
      "Epoch 650/1000, Loss: 0.3482, Correct prediction: 70.6%\n",
      "Epoch 660/1000, Loss: 0.3467, Correct prediction: 70.7%\n",
      "Epoch 670/1000, Loss: 0.3461, Correct prediction: 70.7%\n",
      "Epoch 680/1000, Loss: 0.3455, Correct prediction: 70.8%\n",
      "Epoch 690/1000, Loss: 0.3448, Correct prediction: 70.8%\n",
      "Epoch 700/1000, Loss: 0.3445, Correct prediction: 70.9%\n",
      "Epoch 710/1000, Loss: 0.3437, Correct prediction: 70.9%\n",
      "Epoch 720/1000, Loss: 0.3320, Correct prediction: 71.8%\n",
      "Epoch 730/1000, Loss: 0.3308, Correct prediction: 71.8%\n",
      "Epoch 740/1000, Loss: 0.3304, Correct prediction: 71.9%\n",
      "Epoch 750/1000, Loss: 0.3298, Correct prediction: 71.9%\n",
      "Epoch 760/1000, Loss: 0.3294, Correct prediction: 71.9%\n",
      "Epoch 770/1000, Loss: 0.3293, Correct prediction: 71.9%\n",
      "Epoch 780/1000, Loss: 0.3222, Correct prediction: 72.5%\n",
      "Epoch 790/1000, Loss: 0.3215, Correct prediction: 72.5%\n",
      "Epoch 800/1000, Loss: 0.3209, Correct prediction: 72.6%\n",
      "Epoch 810/1000, Loss: 0.3207, Correct prediction: 72.6%\n",
      "Epoch 820/1000, Loss: 0.3204, Correct prediction: 72.6%\n",
      "Epoch 830/1000, Loss: 0.3199, Correct prediction: 72.6%\n",
      "Epoch 840/1000, Loss: 0.3155, Correct prediction: 72.9%\n",
      "Epoch 850/1000, Loss: 0.3151, Correct prediction: 73.0%\n",
      "Epoch 860/1000, Loss: 0.3147, Correct prediction: 73.0%\n",
      "Epoch 870/1000, Loss: 0.3144, Correct prediction: 73.0%\n",
      "Epoch 880/1000, Loss: 0.3142, Correct prediction: 73.0%\n",
      "Epoch 890/1000, Loss: 0.3140, Correct prediction: 73.0%\n",
      "Epoch 900/1000, Loss: 0.3139, Correct prediction: 73.1%\n",
      "Epoch 910/1000, Loss: 0.3137, Correct prediction: 73.1%\n",
      "Epoch 920/1000, Loss: 0.3104, Correct prediction: 73.3%\n",
      "Epoch 930/1000, Loss: 0.3101, Correct prediction: 73.3%\n",
      "Epoch 940/1000, Loss: 0.3100, Correct prediction: 73.3%\n",
      "Epoch 950/1000, Loss: 0.3098, Correct prediction: 73.4%\n",
      "Epoch 960/1000, Loss: 0.3097, Correct prediction: 73.4%\n",
      "Epoch 970/1000, Loss: 0.3069, Correct prediction: 73.6%\n",
      "Epoch 980/1000, Loss: 0.3045, Correct prediction: 73.7%\n",
      "Epoch 990/1000, Loss: 0.3044, Correct prediction: 73.8%\n",
      "Epoch 1000/1000, Loss: 0.3025, Correct prediction: 73.9%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for x_seq, y_seq in dataloader:\n",
    "        x_seq, y_seq = x_seq.to(device), y_seq.to(device)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initialize hidden state\n",
    "        hidden_state = model.init_hidden(batch_size=x_seq.size(1))\n",
    "\n",
    "        # Forward pass\n",
    "        logits, hidden_state = model(x_seq, hidden_state)\n",
    "        # logits: (batch_size, seq_length, vocab_size)\n",
    "\n",
    "        # Reshape logits and targets for cross-entropy\n",
    "        # We want CE across all time steps\n",
    "        logits_reshaped = logits.view(-1, vocab_size)   # (batch_size*seq_length, vocab_size)\n",
    "        targets_reshaped = y_seq.view(-1)               # (batch_size*seq_length,)\n",
    "\n",
    "        loss = criterion(logits_reshaped, targets_reshaped)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0 or epoch + 1 == num_epochs:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Correct prediction: {math.exp(-avg_loss):.1%}\")\n",
    "    scheduler.step(avg_loss)\n",
    "    history.append(avg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d9b5b488-1e5a-472c-b208-9b9dcf9d6954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Save the model for future use\n",
    "MODEL_SAVE_PATH = os.path.join(os.path.abspath(\".\"), \"models\")\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "model_name = \"shakespeare_lstm_checkpoint\"\n",
    "\n",
    "checkpoint_path = os.path.join(MODEL_SAVE_PATH, f\"{model_name}.pt\")\n",
    "torch.save({\n",
    "    \"last_epoch\": epoch,\n",
    "    \"last_loss\": avg_loss,\n",
    "    \"history\": history,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "}, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb72f797-9330-4422-a5e9-f27c58d250c0",
   "metadata": {
    "id": "cb72f797-9330-4422-a5e9-f27c58d250c0"
   },
   "source": [
    "### Step 4: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "281a5f0f-53cf-4f65-97ba-db14e712a1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharLSTM(\n",
       "  (embedding): Embedding(80, 512)\n",
       "  (lstm): LSTM(512, 512, num_layers=3)\n",
       "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b930d01a-0fdb-48d1-990f-de520f54cd31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b930d01a-0fdb-48d1-990f-de520f54cd31",
    "outputId": "f0d7b704-5f81-4df5-eb80-bf2c0bd162d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " ROMEO:\n",
      "\tNISTRESS PAGE:)  IS P'ANTUS, ALEXANDER and DION]\n",
      "\n",
      "\t\t     How now, my lord!\n",
      "\n",
      "HAMLET\tThere's a double officer.\n",
      "\n",
      "ARVIRAGUS\tOr I would make his charity in Sparth by his gown.\n",
      "\n",
      "TOUCHSTONE\tI will confess the drum and fight:\n",
      "\tThe Duke of Exeter has her brain, an plain care at all; the truth is not in sun\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string=\"ROMEO:\", length=500, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generates text one character at a time.\n",
    "    - start_string: initial prompt\n",
    "    - length: number of characters to generate\n",
    "    - temperature: sampling diversity (1.0 => neutral, >1 => more random)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Convert start string to indices\n",
    "    input_indices = char2idx(start_string)  # [char2idx(ch) for ch in start_string]\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long, device=device).transpose(0, 1)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden_state = model.init_hidden(batch_size=1)\n",
    "\n",
    "    # \"Warm up\" the model with the start string\n",
    "    _, hidden_state = model(input_tensor, hidden_state)\n",
    "    # for i in range(len(start_string) - 1):\n",
    "    #     # feed each char except the last one\n",
    "    #     print(input_tensor[:, i:i+1], hidden_state)\n",
    "    #     _, hidden_state = model(input_tensor[:, i:i+1], hidden_state)\n",
    "\n",
    "    # The last character in start_string\n",
    "    last_char_idx = input_tensor[:, -1]\n",
    "    output_text = start_string\n",
    "\n",
    "    # Now generate 'length' more characters\n",
    "    for _ in range(length):\n",
    "        logits, hidden_state = model(last_char_idx.unsqueeze(1), hidden_state)\n",
    "        # logits shape: (1, 1, vocab_size)\n",
    "        logits = logits[-1, :, :]  # take the last time step => shape (1, vocab_size)\n",
    "\n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()  # shape (vocab_size,)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        # Append to output\n",
    "        next_char = idx2char(next_idx)\n",
    "        output_text += next_char\n",
    "\n",
    "        # Update last_char_idx\n",
    "        last_char_idx = torch.tensor([next_idx], device=device)\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# Example usage after training:\n",
    "generated = generate_text(model, start_string=\"ROMEO:\", length=300, temperature=0.8)\n",
    "print(\"Generated text:\\n\", generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "sN0l32BJ8ORa",
   "metadata": {
    "id": "sN0l32BJ8ORa"
   },
   "outputs": [],
   "source": [
    "shAIkspear = []\n",
    "seeds = [\"JULIET:\", \"ROMEO:\", \"[\", \"Booboo dog \", \"to be or not to be \"]\n",
    "length = 500\n",
    "\n",
    "for s in seeds:\n",
    "    generated = generate_text(model, start_string=s, length=length, temperature=0.8)\n",
    "    # print(\"Generated text:\\n\", generated)\n",
    "    shAIkspear.append((s, generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "BzpSZt7AP7bM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzpSZt7AP7bM",
    "outputId": "80397473-181f-4608-aec9-64a508abe4bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Seed: 'JULIET:' ---\n",
      "JULIET:\n",
      "\n",
      "\n",
      "VARUS\t(PLATISTA:\n",
      "\n",
      "\n",
      "DUKE OF SUFFOLK\t(SUFFOLK:)\n",
      "\n",
      "EARL OF WARWICK\t(WARWICK:)\n",
      "\n",
      "EARL OF SALISBURY:)\n",
      "\n",
      "LORD CLIFFORD\t(CLIFFORD:)\n",
      "\n",
      "ROBERT\n",
      "MALV.\t|\n",
      "\tan another portable cost, a souther with to Pandarus of France; Caesar is\n",
      "\tA soldier to remove, whose self-same blossom speaks matters of blood,\n",
      "\tInterest I remember an abyst as to go reverend fail. Here comes the manners all unbetter than the king.\n",
      "\n",
      "\t[Re-enter PRINCE HENRY]\n",
      "\n",
      "PRINCE HENRY\tThou didst well; or else your lordship is fortunate: I join with us.\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: 'ROMEO:' ---\n",
      "ROMEO:)\n",
      "\n",
      "LORENZO\tHang him, meaner to my lord!\n",
      "\n",
      "HASTINGS\tThere, whom I am, I come; denial: if she be by fiveness and the rough too\n",
      "\tAlence too, live when I a subject for the very thoughts of ourselves to stand, I'll make him eat of thee,\n",
      "\tThat would unstate of use within her: therefore shall he see the emperor for her: I must draw by me.\n",
      "\n",
      "MARIA\tNay, good Sir Hugh, my lord, entertain him.\n",
      "\n",
      "DUCHESS OF YORK\tI love the king engenders that.\n",
      "\n",
      "DON PEDRO\tSee, see; here comes my mind too.\n",
      "\n",
      "CORIOLANUS\tIn love-si\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: '[' ---\n",
      "[As the Page within]\n",
      "\n",
      "\t[Enter LAFE, de guard]\n",
      "\n",
      "\t[Enter SILVIA and PISTOL]\n",
      "\n",
      "\tHow goes the basket? the better for you your promised chamberlain; but I can tell thee where I take the time of men; the note is on him still misplaced. Well, I'll be sworn to wink to him that you make 'gainst thee for a mouth.\n",
      "\n",
      "GLOUCESTER\tThe worser weave our hands.\n",
      "\n",
      "SICINIUS\tWhen I come unto a word:\n",
      "\tLook, here comes Oberon.\n",
      "\n",
      "Fairy Lord\tFair one, one word.\n",
      "\n",
      "FLAVIUS\t'Tis all elder; I would therefore should a sound\n",
      "\tOf yo\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: 'Booboo dog ' ---\n",
      "Booboo dog and let in Germany send him, he looks\n",
      "\twith them that with his boast. But in your cap, sir, a court: see't she comes by the way I have to live.\n",
      "\n",
      "MENAS\t[Aside]  I pray thee lay him up;\n",
      "\n",
      "\t[Singing the doctor]\n",
      "\n",
      "KING HENRY VI\tHere.\n",
      "\n",
      "CASSIUS\tWho of them?\n",
      "\n",
      "DIANA\tAy, my good lord; and truly, when I saw the character he is very sporting with her virtue; all my body's arms.\n",
      "\n",
      "DOLL TESTMORELAND\tThere is something in the people, because little. What are you? what wonder will you do? since when I spoke our d\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: 'to be or not to be ' ---\n",
      "to be or not to be said and sure\n",
      "\tAnd on your royal presence that dares do the dull gold and function of your jesters? I'll be with you so because thou hast here before to-day; it is as well angry with the river of\n",
      "\tmy mistress' sorrow, it is not moved wherein the base wound of thine own dependence; for my vows, more worth a simple object. What the gallows of Pompey I love\n",
      "\tThat I should praise myself. Wands thine enemy, and all our young mistress will content me: she hath bestowed like syllables most; and out the\n",
      "--- STOP ---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s, g in shAIkspear:\n",
    "    print(f\"--- Seed: '{s}' ---\")\n",
    "    print(g)\n",
    "    print(f\"--- STOP ---\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OIeC3n4nVZKn",
   "metadata": {
    "id": "OIeC3n4nVZKn"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "# Let's train on some non-English characters\n",
    "\n",
    "Found some here: https://github.com/aboutjm/Automation/blob/master/book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "IUl1xRkJQCHp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUl1xRkJQCHp",
    "outputId": "7b5077ba-9819-4c55-b291-b8335f451fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/aboutjm/Automation/master/book/%5B%E4%B8%89%E4%BD%931-3%2B%E4%B8%89%E4%BD%93X%E4%BF%AE%E8%AE%A2%E5%A2%9E%E8%A1%A5%5DTXT%E7%B2%BE%E6%A0%A1%E7%89%88.%E5%88%98%E6%85%88%E6%AC%A3/%E4%B8%89%E4%BD%931%E7%96%AF%E7%8B%82%E5%B9%B4%E4%BB%A3.txt\n",
      "Downloading https://raw.githubusercontent.com/aboutjm/Automation/master/book/%5B%E4%B8%89%E4%BD%931-3%2B%E4%B8%89%E4%BD%93X%E4%BF%AE%E8%AE%A2%E5%A2%9E%E8%A1%A5%5DTXT%E7%B2%BE%E6%A0%A1%E7%89%88.%E5%88%98%E6%85%88%E6%AC%A3/%E4%B8%89%E4%BD%932%E9%BB%91%E6%9A%97%E6%A3%AE%E6%9E%97.txt\n",
      "Downloading https://raw.githubusercontent.com/aboutjm/Automation/master/book/%5B%E4%B8%89%E4%BD%931-3%2B%E4%B8%89%E4%BD%93X%E4%BF%AE%E8%AE%A2%E5%A2%9E%E8%A1%A5%5DTXT%E7%B2%BE%E6%A0%A1%E7%89%88.%E5%88%98%E6%85%88%E6%AC%A3/%E4%B8%89%E4%BD%933%E6%AD%BB%E7%A5%9E%E6%B0%B8%E7%94%9F.txt\n",
      "Downloading https://raw.githubusercontent.com/aboutjm/Automation/master/book/%5B%E4%B8%89%E4%BD%931-3%2B%E4%B8%89%E4%BD%93X%E4%BF%AE%E8%AE%A2%E5%A2%9E%E8%A1%A5%5DTXT%E7%B2%BE%E6%A0%A1%E7%89%88.%E5%88%98%E6%85%88%E6%AC%A3/%E4%B8%89%E4%BD%93X%E4%BF%AE%E8%AE%A2%E5%A2%9E%E8%A1%A5%E7%89%88.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "DATA_PATH = os.path.join(os.path.abspath(\".\"), \"cnbook\")\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "BASE_URL = \"https://raw.githubusercontent.com/aboutjm/Automation/master/book/%5B三体1-3%2B三体X修订增补%5DTXT精校版.刘慈欣/\"\n",
    "filelist = [\n",
    "    \"三体1疯狂年代.txt\",\n",
    "    \"三体2黑暗森林.txt\",\n",
    "    \"三体3死神永生.txt\",\n",
    "    \"三体X修订增补版.txt\",\n",
    "]\n",
    "\n",
    "filepaths = []\n",
    "\n",
    "for idx, f in enumerate(filelist):\n",
    "    url = BASE_URL + f\n",
    "    encoded_url = urllib.parse.quote(url, safe=':/%')\n",
    "    file_path = os.path.join(DATA_PATH, f\"file{idx}.txt\")\n",
    "    print(f\"Downloading {encoded_url}\")\n",
    "    !wget $encoded_url -O $file_path -q\n",
    "    filepaths.append(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "H0vvvBtlWtV6",
   "metadata": {
    "id": "H0vvvBtlWtV6"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "texts = []\n",
    "\n",
    "for fpath in filepaths:\n",
    "    with open(fpath, \"r\", encoding=\"gbk\") as f:\n",
    "        text = f.read()\n",
    "        texts.append(text)\n",
    "merged_text = \"\\n\\n\".join(texts)\n",
    "\n",
    "# Cleanup: Replace \\u3000 with space\n",
    "merged_text = merged_text.replace(\"\\u3000\", \" \")\n",
    "# Cleanup: Replace “ and ” with \"\n",
    "merged_text = merged_text.replace(\"“\", '\"')\n",
    "merged_text = merged_text.replace(\"”\", '\"')\n",
    "# Cleanup: Replace double blank characters with a single ones\n",
    "# merged_text = merged_text.replace(\"  \", \" \")\n",
    "merged_text = re.sub(r\"[ ]+\", \" \", merged_text)\n",
    "merged_text = re.sub(r\"[\\t]+\", \"\\t\", merged_text)\n",
    "merged_text = re.sub(r\"[\\n]+\", \"\\n\", merged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1Be934Ryrs_W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Be934Ryrs_W",
    "outputId": "a69024a8-3e67-4f2f-f186-d77cbd995eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 三体（中国科幻基石丛书） \n",
      " 刘慈欣著\n",
      " \"基石\"是个平实的词，不够\"炫\"，却能够准确传达我们对构建中的中国科幻繁华巨厦的情感与信心，因此，我们用它来作为这套原创丛书的名字。\n",
      " 最近十年，是科幻创作\n",
      "增辉不少。正是因为fengziying同学和其他网友的热情支持和鼓励，才让笔者终于下定决心去整理和修订这部极不成熟的作品。希望不会让大家太失望。\n",
      " Isaiah（phenixus）\n",
      "10.12.28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(merged_text[:100])\n",
    "print(merged_text[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BGe8DSXolTxO",
   "metadata": {
    "id": "BGe8DSXolTxO"
   },
   "source": [
    "**Character-Level Tokenization**\n",
    "\n",
    "Since this is a character-level model, our “tokens” are just unique characters found in the text:\n",
    "\n",
    "* Identify the unique set of characters.\n",
    "* Map each character to a unique integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "9EEMxC_olO0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9EEMxC_olO0e",
    "outputId": "1aec5e26-2e93-43ce-90f6-105031426db6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chars found: 3772\n",
      "Example of characters (top): ['，', '的', '。', '\"', '一', ' ', '是', '\\n', '了', '在', '这', '不', '个', '有', '他', '人', '到', '中', '们', '我', '上', '地', '时', '来', '那', '大', '说', '着', '能', '出', '看', '和', '后', '你', '就', '面', '也', '可', '现', '没', '都', '她', '对', '但', '过', '星', '？', '下', '太', '子']\n",
      "Example of characters (bot): ['贷', '茹', '豹', '彷', '徨', '缥', '缈', '茗', '荑', '宛', '凰', '旬', '捺', '狐', '猴', '迭', '赦', '朱', '茧', '睥', '睨', '伫', '氤', '氲', '▽', '◇', '霄', '谚', '嗫', '嚅', '宸', '谕', '鸢', '踌', '躇', '匾', '炒', '隘', '齑', '酬', '敝', '帚', '诟', '佬', '吭', '琢', '裆', '怂', '恿', '@']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Create vocabulary of unique characters\n",
    "char_counts = Counter(merged_text)\n",
    "chars = sorted(char_counts.keys(), key=char_counts.get, reverse=True)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Unique chars found:\", vocab_size)\n",
    "print(\"Example of characters (top):\", chars[:50])\n",
    "print(\"Example of characters (bot):\", chars[-50:])\n",
    "\n",
    "# Create mapping from character to index (and reverse)\n",
    "_char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "_idx2char = {i: ch for ch, i in _char2idx.items()}\n",
    "\n",
    "# Add special characters\n",
    "for special_token in [\"<|UNK|>\"]:\n",
    "    k = len(_char2idx)\n",
    "    _char2idx[special_token] = k\n",
    "    _idx2char[k] = special_token\n",
    "\n",
    "# Utility functions\n",
    "def char2idx(ch):\n",
    "    return [_char2idx.get(c, \"<|UNK|>\") for c in ch]\n",
    "def idx2char(idx):\n",
    "    if isinstance(idx, torch.Tensor):\n",
    "        return idx2char(idx.detach().cpu().numpy())\n",
    "    if isinstance(idx, np.ndarray):\n",
    "        return idx2char(idx.tolist())\n",
    "    if isinstance(idx, int):\n",
    "        return _idx2char.get(idx, \"<|UNK|>\")\n",
    "    return [idx2char(i) for i in idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4DE5C48ho76f",
   "metadata": {
    "id": "4DE5C48ho76f"
   },
   "source": [
    "**Convert Text to Indices**\n",
    "\n",
    "Convert the entire text into a list (or array) of integer indices. This will make it easier to feed into PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "q5KwoKn2rBCz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5KwoKn2rBCz",
    "outputId": "036bf10b-8c73-4b47-877f-314d2329419a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_tensor shape: torch.Size([1017810])\n"
     ]
    }
   ],
   "source": [
    "# Convert all text to indices\n",
    "data_as_indices = char2idx(merged_text)\n",
    "data_tensor = torch.tensor(data_as_indices, dtype=torch.long)\n",
    "print(\"data_tensor shape:\", data_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3V4n873uo_QE",
   "metadata": {
    "id": "3V4n873uo_QE"
   },
   "source": [
    "**Create Training Sequences**\n",
    "\n",
    "For character-level language modeling, a common approach is:\n",
    "\n",
    "* Pick a sequence length, e.g. seq_length = 100.\n",
    "* For each sequence of seq_length characters, the target is the next character.\n",
    "\n",
    "We can use PyTorch's `Dataset`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "Yv184TaIozRS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yv184TaIozRS",
    "outputId": "4a8dfd81-9187-4a9d-d95b-b6a91992a050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 33926\n",
      "Example X (indices): tensor([   5,   58,   54, 1777,   17,  178,  347,  688,  300,  458])\n",
      "Example Y (index): tensor([  58,   54, 1777,   17,  178,  347,  688,  300,  458, 1371])\n",
      "Example X (decoded): \" 三体（中国科幻基石\"\n",
      "Example Y (decoded): \"三体（中国科幻基石丛\"\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data_tensor, seq_length):\n",
    "        self.data = data_tensor\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # We can form this many sequences (minus 1 for the target)\n",
    "        return len(self.data) // self.seq_length - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_length\n",
    "        x_seq = self.data[start : start + self.seq_length]\n",
    "        # Targets are the subsequent seq_length characters\n",
    "        y_seq = self.data[start+1 : start + self.seq_length + 1]\n",
    "        return x_seq, y_seq\n",
    "\n",
    "seq_length = 30\n",
    "dataset = CharDataset(data_tensor, seq_length=seq_length)\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "\n",
    "# For demonstration, let's get one example\n",
    "example_x, example_y = dataset[0]\n",
    "print(\"Example X (indices):\", example_x[:10])\n",
    "print(\"Example Y (index):\", example_y[:10])\n",
    "print(f\"Example X (decoded): \\\"{''.join(idx2char(example_x[:10]))}\\\"\")\n",
    "print(f\"Example Y (decoded): \\\"{''.join(idx2char(example_y[:10]))}\\\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "0N4OeTEqpVY5",
   "metadata": {
    "id": "0N4OeTEqpVY5"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "def batch_second(batch):\n",
    "    x, y = list(zip(*batch))\n",
    "    x = torch.stack(x, 1)\n",
    "    y = torch.stack(y, 1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True, collate_fn=batch_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BSxrSZoYsilW",
   "metadata": {
    "id": "BSxrSZoYsilW"
   },
   "source": [
    "### Step 2: Model Definition (LSTM)\n",
    "\n",
    "We’ll define a character-level LSTM model:\n",
    "\n",
    "1. Embedding: maps integer character indices to dense vectors (optional, but often helps).\n",
    "1. LSTM: one or more LSTM layers that process the embedded sequence.\n",
    "1. Linear: output layer to predict the next character’s index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "X4Tiszz_seF_",
   "metadata": {
    "id": "X4Tiszz_seF_"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_length)\n",
    "        hidden_state: tuple (h, c) for LSTM hidden/cell states (if you want to pass it in)\n",
    "        Returns: logits (batch_size, seq_length, vocab_size), updated_hidden_state\n",
    "        \"\"\"\n",
    "        # 1) Embedding\n",
    "        embedded = self.embedding(x)  # shape: (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        # 2) LSTM\n",
    "        if hidden_state is None:\n",
    "            out, (h, c) = self.lstm(embedded)  # out: (batch_size, seq_length, hidden_dim)\n",
    "        else:\n",
    "            out, (h, c) = self.lstm(embedded, hidden_state)\n",
    "\n",
    "        # 3) Fully connected (we want to produce a prediction at each time step)\n",
    "        logits = self.fc(out)  # shape: (batch_size, seq_length, vocab_size)\n",
    "\n",
    "        return logits, (h, c)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Utility to initialize the hidden state (h, c) to zeros.\n",
    "        Returns: h0, c0 (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        return (h0, c0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHXB0HHnsqAs",
   "metadata": {
    "id": "nHXB0HHnsqAs"
   },
   "source": [
    "### Step 3: Training Routine\n",
    "\n",
    "**Training Setup**\n",
    "\n",
    "We define:\n",
    "\n",
    "* A loss function (CrossEntropyLoss), typical for next-character prediction.\n",
    "* An optimizer (e.g., Adam or RMSprop).\n",
    "* Possibly device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "HlAVCMMtsn3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HlAVCMMtsn3a",
    "outputId": "b5458398-8ff8-4243-9f14-5e3b2e5bc7e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = CharLSTM(vocab_size, embed_dim=1024, hidden_dim=512, num_layers=3)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.8, patience=5)  # Reduce learning rate by half\n",
    "\n",
    "history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tRhQQzCVsx37",
   "metadata": {
    "id": "tRhQQzCVsx37"
   },
   "source": [
    "**Training Loop**\n",
    "\n",
    "At each iteration:\n",
    "\n",
    "1. Get a batch (x, y) from the dataloader. Here, x is of shape (batch_size, seq_length) and y of shape (batch_size,).\n",
    "1. Model outputs logits of shape (batch_size, seq_length, vocab_size).\n",
    "1. We actually want to predict the character that comes after each character in x. So we can shift by 1 step or simply note that y at index i is the final character of the sequence. But if we want a prediction at each time step (not just the last one), we might create labels of shape (batch_size, seq_length)—one label per input character.\n",
    "\n",
    "In the example below, we do the simplest approach: each sequence’s final character is the label. This means we use only the last time step’s logits to compute the loss. Alternatively, if you want to predict the next character at every time step, you’ll need to shift the labels accordingly. (We’ll show the typical approach of every time step.)\n",
    "\n",
    "**Case: Predict next char at every time step**\n",
    "\n",
    "We shift our target by 1 inside the dataset or handle it here. Let’s assume we do it at the dataset level for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "kzQ1jDFbswCO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kzQ1jDFbswCO",
    "outputId": "79572950-8665-41de-99eb-d9309e95aca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 6.2259, Correct prediction: 0.2%\n",
      "Epoch 10/1000, Loss: 3.5211, Correct prediction: 3.0%\n",
      "Epoch 20/1000, Loss: 2.7802, Correct prediction: 6.2%\n",
      "Epoch 30/1000, Loss: 2.2512, Correct prediction: 10.5%\n",
      "Epoch 40/1000, Loss: 1.8335, Correct prediction: 16.0%\n",
      "Epoch 50/1000, Loss: 1.4968, Correct prediction: 22.4%\n",
      "Epoch 60/1000, Loss: 1.2244, Correct prediction: 29.4%\n",
      "Epoch 70/1000, Loss: 1.0095, Correct prediction: 36.4%\n",
      "Epoch 80/1000, Loss: 0.8455, Correct prediction: 42.9%\n",
      "Epoch 90/1000, Loss: 0.7192, Correct prediction: 48.7%\n",
      "Epoch 100/1000, Loss: 0.6302, Correct prediction: 53.3%\n",
      "Epoch 110/1000, Loss: 0.5559, Correct prediction: 57.4%\n",
      "Epoch 120/1000, Loss: 0.5016, Correct prediction: 60.6%\n",
      "Epoch 130/1000, Loss: 0.4831, Correct prediction: 61.7%\n",
      "Epoch 140/1000, Loss: 0.4495, Correct prediction: 63.8%\n",
      "Epoch 150/1000, Loss: 0.2593, Correct prediction: 77.2%\n",
      "Epoch 160/1000, Loss: 0.2150, Correct prediction: 80.7%\n",
      "Epoch 170/1000, Loss: 0.2058, Correct prediction: 81.4%\n",
      "Epoch 180/1000, Loss: 0.2776, Correct prediction: 75.8%\n",
      "Epoch 190/1000, Loss: 0.1895, Correct prediction: 82.7%\n",
      "Epoch 200/1000, Loss: 0.1772, Correct prediction: 83.8%\n",
      "Epoch 210/1000, Loss: 0.1719, Correct prediction: 84.2%\n",
      "Epoch 220/1000, Loss: 0.1664, Correct prediction: 84.7%\n",
      "Epoch 230/1000, Loss: 0.1653, Correct prediction: 84.8%\n",
      "Epoch 240/1000, Loss: 0.1648, Correct prediction: 84.8%\n",
      "Epoch 250/1000, Loss: 0.1643, Correct prediction: 84.9%\n",
      "Epoch 260/1000, Loss: 0.1605, Correct prediction: 85.2%\n",
      "Epoch 270/1000, Loss: 0.1600, Correct prediction: 85.2%\n",
      "Epoch 280/1000, Loss: 0.1595, Correct prediction: 85.3%\n",
      "Epoch 290/1000, Loss: 0.1594, Correct prediction: 85.3%\n",
      "Epoch 300/1000, Loss: 0.1589, Correct prediction: 85.3%\n",
      "Epoch 310/1000, Loss: 0.1587, Correct prediction: 85.3%\n",
      "Epoch 320/1000, Loss: 0.1559, Correct prediction: 85.6%\n",
      "Epoch 330/1000, Loss: 0.1542, Correct prediction: 85.7%\n",
      "Epoch 340/1000, Loss: 0.1527, Correct prediction: 85.8%\n",
      "Epoch 350/1000, Loss: 0.1526, Correct prediction: 85.8%\n",
      "Epoch 360/1000, Loss: 0.1525, Correct prediction: 85.9%\n",
      "Epoch 370/1000, Loss: 0.1524, Correct prediction: 85.9%\n",
      "Epoch 380/1000, Loss: 0.1523, Correct prediction: 85.9%\n",
      "Epoch 390/1000, Loss: 0.1510, Correct prediction: 86.0%\n",
      "Epoch 400/1000, Loss: 0.1501, Correct prediction: 86.1%\n",
      "Epoch 410/1000, Loss: 0.1493, Correct prediction: 86.1%\n",
      "Epoch 420/1000, Loss: 0.1493, Correct prediction: 86.1%\n",
      "Epoch 430/1000, Loss: 0.1486, Correct prediction: 86.2%\n",
      "Epoch 440/1000, Loss: 0.1480, Correct prediction: 86.2%\n",
      "Epoch 450/1000, Loss: 0.1476, Correct prediction: 86.3%\n",
      "Epoch 460/1000, Loss: 0.1473, Correct prediction: 86.3%\n",
      "Epoch 470/1000, Loss: 0.1470, Correct prediction: 86.3%\n",
      "Epoch 480/1000, Loss: 0.1467, Correct prediction: 86.4%\n",
      "Epoch 490/1000, Loss: 0.1465, Correct prediction: 86.4%\n",
      "Epoch 500/1000, Loss: 0.1463, Correct prediction: 86.4%\n",
      "Epoch 510/1000, Loss: 0.1462, Correct prediction: 86.4%\n",
      "Epoch 520/1000, Loss: 0.1461, Correct prediction: 86.4%\n",
      "Epoch 530/1000, Loss: 0.1460, Correct prediction: 86.4%\n",
      "Epoch 540/1000, Loss: 0.1460, Correct prediction: 86.4%\n",
      "Epoch 550/1000, Loss: 0.1459, Correct prediction: 86.4%\n",
      "Epoch 560/1000, Loss: 0.1459, Correct prediction: 86.4%\n",
      "Epoch 570/1000, Loss: 0.1459, Correct prediction: 86.4%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[266], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m targets_reshaped \u001b[38;5;241m=\u001b[39m y_seq\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)               \u001b[38;5;66;03m# (batch_size*seq_length,)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits_reshaped, targets_reshaped)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/micromamba/envs/llm/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/llm/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/llm/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MODEL_SAVE_PATH = os.path.join(os.path.abspath(\".\"), \"models\")\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "num_epochs = 1000\n",
    "model.train()\n",
    "\n",
    "history = []\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for x_seq, y_seq in dataloader:\n",
    "        x_seq, y_seq = x_seq.to(device), y_seq.to(device)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initialize hidden state\n",
    "        hidden_state = model.init_hidden(batch_size=x_seq.size(1))\n",
    "\n",
    "        # Forward pass\n",
    "        logits, hidden_state = model(x_seq, hidden_state)\n",
    "        # logits: (batch_size, seq_length, vocab_size)\n",
    "\n",
    "        # Reshape logits and targets for cross-entropy\n",
    "        # We want CE across all time steps\n",
    "        logits_reshaped = logits.view(-1, vocab_size)   # (batch_size*seq_length, vocab_size)\n",
    "        targets_reshaped = y_seq.view(-1)               # (batch_size*seq_length,)\n",
    "\n",
    "        loss = criterion(logits_reshaped, targets_reshaped)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0 or epoch + 1 == num_epochs:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Correct prediction: {math.exp(-avg_loss):.1%}\")\n",
    "    scheduler.step(avg_loss)\n",
    "    history.append(avg_loss)\n",
    "\n",
    "model_name = f\"checkpoint_cn_lstm.pt\"\n",
    "model_path = os.path.join(MODEL_SAVE_PATH, model_name)\n",
    "torch.save({\n",
    "    \"last_epoch\": epoch,\n",
    "    \"last_loss\": avg_loss,\n",
    "    \"history\": history,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "}, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "d1766eba-db73-49f2-a929-8e481ea3fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"checkpoint_cn_lstm.pt\"\n",
    "model_path = os.path.join(MODEL_SAVE_PATH, model_name)\n",
    "torch.save({\n",
    "    \"last_epoch\": epoch,\n",
    "    \"last_loss\": avg_loss,\n",
    "    \"history\": history,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "}, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZAA6ynexvCZu",
   "metadata": {
    "id": "ZAA6ynexvCZu"
   },
   "source": [
    "### Step 4: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "Ufnyjt98uWsp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ufnyjt98uWsp",
    "outputId": "f381c2e6-d71f-4b08-9dc4-8094443f4da1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "  AA是男孩，眼睛在走下去，就把他怀中火火苍老她的双手套起去，穿过大脑蒸发之条，都没关系。第二个台阶的人中他在童年里度过一生，但她所在的小桌上是很温暖的，她不可能随即给孩子什么都没有。往常这样的人很多，都是美国中地球危机之后，他慢慢把身份刺好苦致打击。\n",
      " \"你这儿看好。\"\n",
      " 程心从理采划为敌人的门，蜂拥地面对着针一步走向大厅一样。实际上没有使她走完了中心，致极功能更新公里的所有经济，却不堪给他杀了，用那种品冰也会压下来，连脑灵发的这幅图画上已经占据了大低谷的事了，地面的火焰和一个有些报告的里呵槽就完全看不到尽头，她软软地回答她序，以免被遗齐，因为他心里还是沉重地进入圈，在路巴浮着的他，以他以为这\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string=\"ROMEO:\", length=500, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generates text one character at a time.\n",
    "    - start_string: initial prompt\n",
    "    - length: number of characters to generate\n",
    "    - temperature: sampling diversity (1.0 => neutral, >1 => more random)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Convert start string to indices\n",
    "    input_indices = char2idx(start_string)  # [char2idx(ch) for ch in start_string]\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long, device=device).transpose(0, 1)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden_state = model.init_hidden(batch_size=1)\n",
    "\n",
    "    # \"Warm up\" the model with the start string\n",
    "    _, hidden_state = model(input_tensor, hidden_state)\n",
    "    # for i in range(len(start_string) - 1):\n",
    "    #     # feed each char except the last one\n",
    "    #     print(input_tensor[:, i:i+1], hidden_state)\n",
    "    #     _, hidden_state = model(input_tensor[:, i:i+1], hidden_state)\n",
    "\n",
    "    # The last character in start_string\n",
    "    last_char_idx = input_tensor[:, -1]\n",
    "    output_text = start_string\n",
    "\n",
    "    # Now generate 'length' more characters\n",
    "    for _ in range(length):\n",
    "        logits, hidden_state = model(last_char_idx.unsqueeze(1), hidden_state)\n",
    "        # logits shape: (1, 1, vocab_size)\n",
    "        logits = logits[-1, :, :]  # take the last time step => shape (1, vocab_size)\n",
    "\n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()  # shape (vocab_size,)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        # Append to output\n",
    "        next_char = idx2char(next_idx)\n",
    "        output_text += next_char\n",
    "\n",
    "        # Update last_char_idx\n",
    "        last_char_idx = torch.tensor([next_idx], device=device)\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# Example usage after training:\n",
    "generated = generate_text(model, start_string=\" \", length=300, temperature=0.8)\n",
    "print(\"Generated text:\\n\", generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "Vz1TDBGDvPpX",
   "metadata": {
    "id": "Vz1TDBGDvPpX"
   },
   "outputs": [],
   "source": [
    "cn_texts = []\n",
    "seeds = [\"国王\", \" \", \"[\", \"Booboo dog \", \"to be or not to be \"]\n",
    "length = 500\n",
    "\n",
    "for s in seeds:\n",
    "    generated = generate_text(model, start_string=s, length=length, temperature=0.8)\n",
    "    # print(\"Generated text:\\n\", generated)\n",
    "    cn_texts.append((s, generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ce924a9e-d369-4951-920a-2b87df88166b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Seed: '国王' ---\n",
      "国王，十维宇宙中还有走处的每一个点，只能看到一座颗粒，我们一起跑到墓岛上，每时每刻都在发生的稳定和素会一百万个元议，你看到的有足球者在漫长的记忆里其前就被提前出这种都不存在。但是他们想象的那样永远不存在。自从经体用了降维展开，宇宙是个之蓝星，最高的条件都一样。这是天使王国就来到了这么多造曲线的底片边缘，能够完成这个事情可能毁灭性的灾难，至少我觉得那个存在的社会上来完成。主宰和你了一件更重要的事儿。\"\n",
      " \"还是那句话：这事儿我知道的是，说明你那个老家家屯时火上，游民时间的情况很难显示，他们甚至没打仗了。\"\n",
      " 在回沃伯顿的西田说。\n",
      " \"汪教授，我这次不知道这不等地说明他是不懂了口态空间的空间？那好，\"叶文洁说。窗外，太平洋\"号是开始时告诉你了，可惜画师看了他一生，即使在原子投入过了多少年来，不知是不是真的，自己的父母不是最好的结果。\"\n",
      " \"我首先要说明的是，你说的那些都是自动贝洋，在十年内不同地前抽上车刚才华，受到侵略者的那种武器，以防造了百年前的那些，你一直在为指甲弹出来的人，有点种头骨，随便翻一杯全手，我不指望我们的最高统帅，酒，随便走，而不是在我上家。为了你的影子了，最后都这么打算！\"大\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: ' ' ---\n",
      " AA是男孩，汪淼心里一看，\"云天明平静了下来，在这个时候，他还真考虑宇宙这样古了，但那一天，在一个月的宇宙中，我不断觉得这么想，那他现在也许会就该？十分钟过去了，谢谢你的感觉一时却又明白了这种误想，那这儿，有时还更加神秘的东西，博格和大气上的漫长美丽有厚觉的手。在两个人的问题稍宽灵法中的深意，刚来的几个村就说明，那里面可千人！\"他指着图像时，冲挥了本神方式神秘多神，刚才打开酒块炉外，还有人见到过的路上了里面。虽然这些一脸迷纸，但图像被包括两排\"号上的人群说：\"请上老家伙们和信仰家伙就看到了过去的东西，让你们那时成千的定位。\"大史说。\n",
      " \"不，陛下，这是一个边长征趣，还有六秒钟都没有用，要警告几乎多好，那些人显然是出坐的村杨，歌者也觉得自己竟破头再拿餐。\"然后狠了粗方的微点点，手中拿着一把大纸边的文件、打抖、震动香皂，对那人说话就是汪淼，不需要问这位与会者，见到大家脸上的每一个部分才是史天面地内的名称，这时医生才认识了一个星期，也许连这事没有什么不大，直接找人不断地给你们登机。\"\n",
      " \"叶老师，您这么想不上以，早来喝山人，有可不是钱的，来了解决这个要求的情报：欧洲和你份军都在专格取法上表据\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: '[' ---\n",
      "[公里之森工作是以及连形舰内的全部描述，那里就价的比人有一个漏斗形都没有图形，还带着这样的平淡了一下，整个宇宙都被越过了。该表面的物理权是很难想象的，但有一点能够在地下的大部分时间都处于一种很巨大的电磁炮炸碎碎斑状态，变化过，也就是二维化的太空城正在被封装着小行星带的东西。三体行星从另一侧看，当它只有木星，这个球形大概在公元世纪的飞船当然可以，这个世界用无神湮挡的颗粒们像古着一个个真文，\"自然选择\"号就不能改变光速，这是星际尘埃，这种技术能合在民用考察队，独自做目体差点引力波宇宙飞船前面，有一块陨石拆肉，都经过远弱继入天线之外。她不知不觉后。在以后的四个天时纪中，当时的脚片都是被光粒击内的空间，几乎要变成一个小小的黑色两点。\n",
      " 但宽阔的油谷只在剧烈的轰鸣声中显得很激动，竟依次看到了水滴的飞行，人们都能看清四维碎块。激动以震惊，常伟思还是一片那些覆盖的宇宙，但十年时间，能够在宇宙中只有千颗，减速效果很接近，只用了一个漫长的落叶角，那是几千万吨透明的浪潮和流度而二十亿之一的……\"\n",
      " \"那文物呢？\"\n",
      " \"是啊，我们去冥王星。\"\n",
      " \"不，还是给我喝酒的儿子，\"老弟，这个儿子就要了，那个世界，中魔\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: 'Booboo dog ' ---\n",
      "Booboo dog :35。不小，谁会让你们等着那份报明的反言件了。新纪元已经快意识到这样一个事实：星际间建立了18米直径至百度写法，都一直没有倒是件很好的，毕业组成了一件幅数网列，由十万年出现的空间为三体世界的收获。[先就为所假成两个世纪，至少可以勉强明确再转下一个新的小纸子里，我们也可能把绝对可能爆发。\"\n",
      " \"可我知道，\"云天明轻轻地问，使得三体入侵他的直觉应该可以过去，除人类社会的十分钟过去了，第三个台世才迁掉已经十分轻便上，人们发现它是个好时窗口，发送你们要回来。\"\n",
      " \"真是一个傻丫头，那就不真想见了！\"2046好，是问从她上出去中的罗辑。\n",
      " 伊文斯说：\"是的，我还是把要听到这里的那些宝石，真的就是一粒枪，有大幅星物卷起来像什么看电波那个，是三体2的东3根吧。\"\n",
      " 汪淼走到默斯默地说。\n",
      " \"如果它真是一样……是真间。\"汪淼摇摇头说。\n",
      " \"主，请你去做，你们没有超级计算机，只能诱发它们为2003年 ，约真空，黑域是每根子河，从来没有出现过这个方向，无穷无尽……怎么落一般地开始上，不可能再仔细看看，如同地球上的人相反如果再闪烁了它就不小，将来每一个小模会了，放松倍呢？你去反留区一旦流动，转动就死了，你\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: 'to be or not to be ' ---\n",
      "to be or not to be betody.m，忽然想到了当我心门的？\"\n",
      " \"你只有两个可能吗？\"\n",
      " \"是的。\"\n",
      " \"问什么？\n",
      " 为什么？\n",
      " 歌者讲完美丽的组革兵，把一些头脑颤抖了，\"她说，把那些故事中的西杨单摆着，那是一枚枚它胶卷的头发，向湖中扩散开来，没有收集的可能性什么？\n",
      " 随着三体文明的种理事产出能量振动的间隔，每个倒计时，他也是可以给人一种薄水，倒是从真实发出的胶卷就怀孕了，那个来自整个世界在智子为你把波动吸振过来完全更具体的样子，就像个子三体世界有着的社会混乱，恐怖天文单位外有了什么地方？\"\n",
      " \"一切都是为了之前的低熵文明。\"伽尔宁说，这个信息被包裹完好无恶的人机段突破，开始了激败世界发出的电磁波立刻进入尘埃后围回到暗业中，随即停止了尾翼，纸面在纸边片上，又落下了一条增加集体，他只看着她，那是一双眼睛，从中误入了第二颗恒星的存在。\n",
      " 他们把重叠在第二维地海王星文明想法呢？那个天堂中也一样，幽灵倒计时已完全变成了月光，人们发现它是个魔鬼。然而实实过一地，可以主宰做出这种感觉。\n",
      " 伊文斯：\"是的，早已不！这么想，这当然不是你把整个银河系】\n",
      " 一个叫黑洞间的声音，这是莫家智慧时代发生的一代画师，寻找的生命就是\n",
      "--- STOP ---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s, g in cn_texts:\n",
    "    print(f\"--- Seed: '{s}' ---\")\n",
    "    print(g)\n",
    "    print(f\"--- STOP ---\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
