{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73823db2-cdf3-4172-afd2-6ee5817d2ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hack to get repo path on remotes\n",
    "import os\n",
    "REPOPATH = os.path.abspath('.')\n",
    "if \"MLX_USER\" in os.environ:\n",
    "    REPOPATH = os.path.join(\"/mlx/users\", os.environ[\"MLX_USER\"], \"repo/LLM-Tutorials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c286d0-48dc-4707-bb04-10cc87727c3e",
   "metadata": {
    "id": "01c286d0-48dc-4707-bb04-10cc87727c3e"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e996f26d-f6cf-444c-bac2-511c70ef9425",
   "metadata": {
    "id": "e996f26d-f6cf-444c-bac2-511c70ef9425"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed3798-5270-4bd3-b0d4-c7bfd6b42a8f",
   "metadata": {
    "id": "00ed3798-5270-4bd3-b0d4-c7bfd6b42a8f"
   },
   "source": [
    "# Session 2 – NLP Models and Training Basics (RNN)\n",
    "  \n",
    "In this notebook, we will dive into fundamental sequence models such as **RNNs** and **LSTMs**. We’ll also cover basic neural embeddings, training objectives, and see how to implement and train a **simple text generator** using an LSTM.\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction and Overview](#introduction)\n",
    "2. [Recurrent Neural Networks (RNNs)](#rnns)\n",
    "   - [The RNN Cell](#rnn-cell)\n",
    "   - [Vanishing and Exploding Gradients](#vanishing)\n",
    "3. [Long Short-Term Memory (LSTM)](#lstm)\n",
    "   - [Key Intuition Behind LSTM Gates](#lstm-gates)\n",
    "4. [Embeddings](#embeddings)\n",
    "5. [Basic Training Objectives in Language Modeling](#training-objectives)\n",
    "   - [Next Token Prediction](#next-token-pred)\n",
    "   - [Perplexity](#perplexity)\n",
    "6. [Implementing a Simple LSTM Text Generator in PyTorch](#implementation)\n",
    "   - [Data Preparation](#data-prep)\n",
    "   - [Model Definition](#model-def)\n",
    "   - [Training Loop](#training-loop)\n",
    "   - [Generating Text](#generate-text)\n",
    "\n",
    "Each section will be followed by one or more **Exercises** to help you practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b50b1-32ad-4ce9-a35f-4f4e2d8bb4f0",
   "metadata": {
    "id": "bc7b50b1-32ad-4ce9-a35f-4f4e2d8bb4f0"
   },
   "source": [
    "# <a id=\"overview\"></a>1. Overview and Setup\n",
    "\n",
    "This tutorial assumes you have:\n",
    "\n",
    "- **Basic Python** knowledge.\n",
    "- A local or cloud environment (e.g., Jupyter, Colab) with **PyTorch** installed.\n",
    "  - If needed, install PyTorch via `pip install torch` or follow instructions at [pytorch.org](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "No prior reading of other sessions is required; we’ll present all the essentials here.\n",
    "\n",
    "### Quick Setup Check\n",
    "```python\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "```\n",
    "\n",
    "Ensure you see a version number (e.g., `2.0.0` or similar) printed. If you get an error, please install or update PyTorch before continuing.\n",
    "\n",
    "- We’ll focus on **RNNs** and **LSTMs**.  \n",
    "- We’ll learn **why** they are powerful for sequential data.  \n",
    "- We’ll cover **basic training objectives** (like next-token prediction) for language modeling.  \n",
    "- Finally, we’ll implement a small **LSTM-based text generator**.\n",
    "\n",
    "**By the end of this session**, you should be able to:\n",
    "1. Understand how an RNN cell and LSTM cell process sequential data.  \n",
    "2. Implement an **LSTM** in a deep learning framework (here, PyTorch).  \n",
    "3. Train and evaluate a **text-generation** model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5d4f43-f69f-4efc-9c2f-6eafe5fca19b",
   "metadata": {
    "id": "1d5d4f43-f69f-4efc-9c2f-6eafe5fca19b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a964978-2cdb-4568-9f2b-97b81c07b33a",
   "metadata": {
    "id": "0a964978-2cdb-4568-9f2b-97b81c07b33a"
   },
   "source": [
    "# 2. Recurrent Neural Networks (RNNs)<a id=\"rnns\"></a>\n",
    "\n",
    "Recurrent Neural Networks are designed to handle **sequential data** by maintaining a hidden state that captures information about previous time steps.\n",
    "\n",
    "## Key Idea\n",
    "At each time step $t$:\n",
    "1. The RNN takes an input $x_t$ and the hidden state from the previous time step $h_{t-1}$.\n",
    "2. It produces a new hidden state $h_t$.\n",
    "\n",
    "Mathematically, a very **basic** RNN can be written as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h) \\\\\n",
    "y_t &= W_{hy} h_t + b_y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $h_t$ is the updated hidden state.\n",
    "- $y_t$ is the output at time step $t$ (used for tasks like classification or next-token prediction).\n",
    "- $W_{hh}, W_{xh}, W_{hy}$ are learned weight matrices.\n",
    "\n",
    "**Rearrangement of Terms**\n",
    "\n",
    "Notice that the term $W_{hh} h_{t-1} + W_{xh} x_t$ uses two matrix multiplications and an addition.\n",
    "Unless compiled, these two multiplications will be performed sequentially.\n",
    "We can gain a slight improvement if we concatenate $h$ and $x$, and use a single matrix multiplication by a larger weight matrix:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\tanh(W_h H_t + b_h) \\\\\n",
    "y_t &= W_{hy} h_t + b_y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $H_t = [h_{t-1}||x_t]$ is the concatenation of $h$ and $x$\n",
    "- $W_{h} = [W_{hh}||W_{xh}]$ is the cconcatenaation of $W_{hh}, W_{xh}$\n",
    "\n",
    "\n",
    "<img src=\"img_src/RNNs.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf63a95-efb9-43c0-ad96-b8731e927366",
   "metadata": {
    "id": "6bf63a95-efb9-43c0-ad96-b8731e927366"
   },
   "source": [
    "\n",
    "## <a id=\"rnn-cell\"></a>The RNN Cell\n",
    "\n",
    "The **RNN cell** is the fundamental computational unit. At time step $t$:\n",
    "1. **Input**: current token (often embedded) + previous hidden state.\n",
    "2. **Output**: updated hidden state + optional output vector.\n",
    "\n",
    "If you unroll this cell over time for $T$ steps, you get a **computation graph** that looks like a chain, where each link is an RNN cell.\n",
    "\n",
    "<img src=\"img_src/RNN-folded.svg\"/>\n",
    "<img src=\"img_src/RNN-unfolded.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281f15f-52e2-4038-9fa1-116d35e884fa",
   "metadata": {
    "id": "b281f15f-52e2-4038-9fa1-116d35e884fa"
   },
   "source": [
    "### Exercise: Implement a Toy RNN Cell\n",
    "**Goal**:  \n",
    "1. Write a Python function that computes a single time-step of an RNN.  \n",
    "1. Use NumPy or PyTorch (in NumPy style) to do the matrix multiplication and a `tanh` activation.  \n",
    "1. Test it on a small input (e.g., input dimension of 5, hidden dimension of 3).\n",
    "\n",
    "*(Keep it simple—focus on the concept, not a full RNN unrolled over time.)*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0561a7ed-91c1-4cca-81f9-bf7e38c6a8da",
   "metadata": {
    "id": "0561a7ed-91c1-4cca-81f9-bf7e38c6a8da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def rnn_step(x_t, h_prev, Wxh, Whh, bh):\n",
    "    \"\"\"Simple RNN step\n",
    "\n",
    "    Args:\n",
    "        x_t: shape (batch_size, input_dim)\n",
    "        h_prev: shape (batch_size, hidden_dim)\n",
    "        Wxh: shape (input_dim, hidden_dim)\n",
    "        Whh: shape (hidden_dim, hidden_dim)\n",
    "        bh: shape (hidden_dim,)\n",
    "    Returns:\n",
    "        h_t: shape (batch_size, hidden_dim)\n",
    "    \"\"\"\n",
    "    weighted_h = h_prev @ Whh  # shape: (batch_size, hidden_dim)\n",
    "    weighted_x = x_t @ Wxh  # shape: (batch_size, hidden_dim)\n",
    "    linear_hx = weighted_h + weighted_x + bh\n",
    "    nonlinear_hx = torch.tanh(linear_hx)\n",
    "    return nonlinear_hx\n",
    "\n",
    "# Tests -- we only consider shapes here\n",
    "N = (1, 2, 5)  # Batch sizes\n",
    "hidden_dims = (1, 2, 5)  # Hidden sizes\n",
    "input_dims = (1, 2, 5)  # Input sizes\n",
    "\n",
    "failed_cases = []\n",
    "for batch_size, hdim, xdim in itertools.product(N, hidden_dims, input_dims):\n",
    "    x = torch.ones(batch_size, xdim)\n",
    "    h = torch.zeros(batch_size, hdim)\n",
    "    Wxh = torch.ones(xdim, hdim)\n",
    "    Whh = torch.ones(hdim, hdim)\n",
    "    bh = torch.zeros(hdim)\n",
    "    expect_shape = (batch_size, hdim)\n",
    "    with torch.no_grad():\n",
    "        h_next = rnn_step(x, h, Wxh, Whh, bh)\n",
    "    if h_next.shape != expect_shape:\n",
    "        print('x', end='')\n",
    "        failed_cases.append((h_next.shape, expect_shape))\n",
    "    else:\n",
    "        print('.', end='')\n",
    "print()\n",
    "for got, expected in failed_cases:\n",
    "    print(f\"{expected} vs. {got}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb613ff0-dde2-4208-a71a-63bccf0a62c8",
   "metadata": {
    "id": "cb613ff0-dde2-4208-a71a-63bccf0a62c8"
   },
   "outputs": [],
   "source": [
    "# PyTorch implementation\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_dim, state_dim):\n",
    "        super().__init__(data_dim, state_dim)\n",
    "\n",
    "        # Define parameters to train\n",
    "        self.input_linear = nn.Linear(  # Takes [x||h_prev] and produces h_next\n",
    "            in_features=self.data_dim + self.state_dim,\n",
    "            out_features=self.state_dim,\n",
    "            bias=True)\n",
    "        self.output_linear = nn.Linear(  # Takes h_next and produces y\n",
    "            in_features=self.state_dim,\n",
    "            out_features=self.data_dim,\n",
    "            bias=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        # Concatenate x and hidden_state\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.shape[0], self.state_dim, device=x.device, dtype=x.dtype)\n",
    "        xh = torch.hstack([x, h])\n",
    "\n",
    "        # Compute new hidden state\n",
    "        xh = self.input_linear(xh)\n",
    "        h_next = self.tanh(xh)\n",
    "\n",
    "        # Compute output\n",
    "        y = self.output_linear(h_next)\n",
    "        return y, h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113605e-5e6d-4762-909e-a0a218010c32",
   "metadata": {
    "id": "a113605e-5e6d-4762-909e-a0a218010c32"
   },
   "source": [
    "## <a id=\"vanishing\"></a>Vanishing and Exploding Gradients\n",
    "\n",
    "**Problem**: Simple RNNs often struggle with **long-term dependencies** due to **vanishing** or **exploding gradients**. That means:\n",
    "- When sequences are long, the gradient that flows backward through time either becomes extremely small (**vanishes**) or extremely large (**explodes**).\n",
    "- This makes training unstable or ineffective for capturing long-range context.\n",
    "\n",
    "**Solution**: Specialized RNN variants like **LSTM** or **GRU** mitigate these issues by incorporating gating mechanisms.\n",
    "\n",
    "### Research Note: let's invent a GRU (Gated Recurrent Unit)\n",
    "\n",
    "**RNN** : $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$\n",
    "\n",
    "* **Problem:** To compute the gradient of $h_1$ (or any early token), we need to multiply the gradients by small values in $W_h$, thus **vanishing** it.\n",
    "* **Solution:** Intelligently choose the previous memory: $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$ or $h_t = h_{t-1}$\n",
    "\n",
    "**RNN with no vanishing** : $h_t = \\alpha\\odot\\hat{h}_t + (1-\\alpha)\\odot h_{t-1}$, where $\\hat{h}_t=\\phi(W_hh_{t-1} + W_xx_{t})$\n",
    "\n",
    "* **Problem:** To compute the gradient of $h_1$ (or any early token), we need to multiply the gradients by large values in $W_h$, thus **exploding** it.\n",
    "* **Solution:** Intelligently choose to set the previous memory to zero before multiplying it by the weights: $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$ or $h_t = \\phi(W_xx_{t})$\n",
    "\n",
    "**RNN with no explosion** : $h_t = \\phi(W_h(\\beta \\odot h_{t-1}) + W_xx_{t})$\n",
    "\n",
    "* **Problem:** How do we decide on the values of $\\alpha$ and $\\beta$?\n",
    "* **Solution:** Don't! Let the data decide (learning)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\overbrace{\\alpha\\odot\\underbrace{\\phi\\left(W_h(\\beta \\odot h_{t-1}) + W_xx_{t}\\right)}_{\\text{no explosion}} + (1-\\alpha)\\odot h_{t-1}}^\\text{no vanishing} \\\\\n",
    "\\text{where}\\\\\n",
    "\\alpha &= \\sigma\\left(Ah_{t-1} + Bx_t\\right) &&\\text{Memory Update Gate}\\\\\n",
    "\\beta &= \\sigma\\left(Ch_{t-1} + Dx_t\\right) &&\\text{Memory Reset Gate}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Congratulations**, you have just invented a **Gated Recurrent Unit** (GRU)!\n",
    "\n",
    "*An earlier version of a gated recurrent network is [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory), which follows very similar logic for preserving the long-term context infromation.*\n",
    "\n",
    "| Network | Complexity | Long-Term Relationship | Gradient Issues |\n",
    "|---------|--------------|------|----|\n",
    "| RNN (tanh) | (++) | None | (-) |\n",
    "| GRU | (+) | (+)<br/>Single state | (++) |\n",
    "| LSTM | (--) | (++)<br/>Separate state for long and short terms | (++) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b445c-237d-4f50-a24c-b8cc229f8e8d",
   "metadata": {
    "id": "ba8b445c-237d-4f50-a24c-b8cc229f8e8d"
   },
   "source": [
    "# <a id=\"lstm\"></a>3. Long Short-Term Memory (LSTM)\n",
    "\n",
    "A **Long Short-Term Memory (LSTM)** network is a type of RNN specifically designed to better capture **long-range dependencies**. It addresses the vanishing/exploding gradient problem through gates that control the flow of information.\n",
    "\n",
    "### <a id=\"lstm-gates\"></a>Key Intuition Behind LSTM Gates\n",
    "\n",
    "Typical LSTM equations:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "f_t &= \\sigma(W_f [h_{t-1}, x_t] + b_f)\n",
    "&\\quad(\\text{Forget Gate}) \\\\\n",
    "i_t &= \\sigma(W_i [h_{t-1}, x_t] + b_i)\n",
    "&\\quad(\\text{Input Gate}) \\\\\n",
    "\\tilde{C_t} &= \\tanh(W_C [h_{t-1}, x_t] + b_C)\n",
    "&\\quad(\\text{Candidate Values}) \\\\\n",
    "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C_t}\n",
    "&\\quad(\\text{Cell State Update}) \\\\\n",
    "o_t &= \\sigma(W_o [h_{t-1}, x_t] + b_o)\n",
    "&\\quad(\\text{Output Gate}) \\\\\n",
    "h_t &= o_t \\odot \\tanh(C_t)\n",
    "&\\quad(\\text{New Hidden State})\n",
    "\\end{aligned}$$\n",
    "\n",
    "- **Forget Gate** ($f_t$): decides how much old state to keep.\n",
    "- **Input Gate** ($i_t$): decides how much new information to add.\n",
    "- **Candidate** ($\\tilde{C_t}$): proposed update to the cell state.\n",
    "- **Output Gate** ($o_t$): decides how much cell state to output as hidden state.\n",
    "\n",
    "This gating mechanism helps **preserve gradients** across many time steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f51e6-2a10-4cec-aff8-e581bc4b072f",
   "metadata": {
    "id": "296f51e6-2a10-4cec-aff8-e581bc4b072f"
   },
   "source": [
    "### Exercise: Compare RNN and GRU Outputs (or LSTM if you prefer)\n",
    "1. Create a synthetic sequence of length 20.  \n",
    "2. Feed it into a small **Vanilla RNN** and a small **GRU** (in PyTorch).  \n",
    "3. Compare the final hidden states after feeding all time steps. Are they similar? If you vary the length from 20 to 50 to 100, how do the hidden states change?\n",
    "\n",
    "*Hint*: This is a conceptual experiment. You can use random inputs, then measure how the hidden states drift over longer sequences.\n",
    "\n",
    "*Hint*: **If you really want**, you can use the utilities in the `utils.py` file to generate simple synthetic sequences (`generate_synthetic_sequences`).\n",
    "\n",
    "*Hint*: You don't have to train the network, but if you want to you can use `utils.py` (`train_recurrent`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8212c123-1c29-414d-b919-0625d7673f88",
   "metadata": {
    "id": "8212c123-1c29-414d-b919-0625d7673f88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden output shapes: h_rnn.shape=torch.Size([1, 3, 2]), h_gru.shape=torch.Size([1, 3, 2]), h_lstm.shape=torch.Size([1, 3, 2]), c_lstm.shape=torch.Size([1, 3, 2])\n",
      "Hidden state norms: 1.61e+00, 1.25e+00, 2.20e+00\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "batch_size = 3\n",
    "sequence_length = 1000\n",
    "xdim = 3\n",
    "hdim = 2\n",
    "\n",
    "x = torch.ones(sequence_length, batch_size, xdim)  # Input sequence\n",
    "h = torch.zeros(1, batch_size, hdim)  # Initial hidden state / initial memory\n",
    "\n",
    "rnn_model = nn.RNN(input_size=xdim, hidden_size=hdim, num_layers=1, batch_first=False, bidirectional=False)\n",
    "gru_model = nn.GRU(input_size=xdim, hidden_size=hdim, num_layers=1, batch_first=False, bidirectional=False)\n",
    "lstm_model = nn.LSTM(input_size=xdim, hidden_size=hdim, batch_first=False)\n",
    "\n",
    "rnn_model.zero_grad()\n",
    "gru_model.zero_grad()\n",
    "lstm_model.zero_grad()\n",
    "\n",
    "y_rnn, h_rnn = rnn_model(x)\n",
    "y_gru, h_gru = gru_model(x)\n",
    "y_lstm, (h_lstm, c_lstm) = lstm_model(x)\n",
    "\n",
    "print(f\"Hidden output shapes: {h_rnn.shape=}, {h_gru.shape=}, {h_lstm.shape=}, {c_lstm.shape=}\")\n",
    "\n",
    "# Very basic error -- just minimizing the norm of the memory\n",
    "rnn_error = h_rnn.norm()\n",
    "gru_error = h_gru.norm()\n",
    "lstm_error = h_lstm.norm() + c_lstm.norm()\n",
    "\n",
    "rnn_error.backward()\n",
    "gru_error.backward()\n",
    "lstm_error.backward()\n",
    "\n",
    "print(f\"Hidden state norms: {rnn_error:.2e}, {gru_error:.2e}, {lstm_error:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab50badc-ac31-43ff-a469-da174722fe5c",
   "metadata": {
    "id": "ab50badc-ac31-43ff-a469-da174722fe5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1370, 0.3210],\n",
       "         [0.1878, 0.4400]]),\n",
       " tensor([[-3.1429e-03,  8.0500e-03],\n",
       "         [-6.9294e-03,  1.7748e-02],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [-1.4641e-08,  3.7500e-08],\n",
       "         [ 1.1975e-02, -3.0671e-02],\n",
       "         [-1.9021e-01,  4.8720e-01]]),\n",
       " tensor([[0.1712, 0.1392],\n",
       "         [0.0473, 0.0385],\n",
       "         [0.1563, 0.1271],\n",
       "         [0.0478, 0.0389],\n",
       "         [0.1135, 0.0923],\n",
       "         [0.2680, 0.2180],\n",
       "         [0.0829, 0.0674],\n",
       "         [0.0396, 0.0322]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.weight_hh_l0.grad, gru_model.weight_hh_l0.grad, lstm_model.weight_hh_l0.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b2473-c806-405e-b0c3-647151ebdcba",
   "metadata": {
    "id": "bd0b2473-c806-405e-b0c3-647151ebdcba"
   },
   "source": [
    "# <a id=\"embeddings\"></a>4. Embeddings\n",
    "\n",
    "**TODO: Embedding and Latent Space Explanation**\n",
    "\n",
    "When dealing with text, each word or token is usually mapped to an **embedding** vector rather than a large one-hot vector.\n",
    "\n",
    "- **Embedding Layer**: A learnable matrix that maps token indices to dense vectors of fixed dimension $d$.\n",
    "- This helps the model learn **semantic relationships** between words.\n",
    "\n",
    "For example:\n",
    "- Word “hello” → index 5 → embedding vector $\\mathbf{e} \\in \\mathbb{R}^d$.\n",
    "\n",
    "Most frameworks (like PyTorch) provide a built-in layer, `nn.Embedding(vocab_size, embed_dim)`, that handles this.\n",
    "\n",
    "\n",
    "### Exercise 3: Custom Embedding Lookup\n",
    "1. Create a small vocabulary of 5 tokens.  \n",
    "2. Initialize a random embedding matrix of shape $(5, d)$.  \n",
    "3. Write a function that takes a token index and returns the corresponding embedding row.  \n",
    "4. Compare with `nn.Embedding` in PyTorch for the same matrix initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060618e9-54bd-4212-8fd4-e1bf674ca2f4",
   "metadata": {
    "id": "060618e9-54bd-4212-8fd4-e1bf674ca2f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom lookup vector: tensor([ 2.2082, -0.6380,  0.4617])\n",
      "nn.Embedding lookup vector: tensor([ 2.2082, -0.6380,  0.4617])\n",
      "Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "embed_dim = 3\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Step 1: create a random embedding matrix\n",
    "embedding_matrix = torch.randn(vocab_size, embed_dim)\n",
    "\n",
    "def custom_embed_lookup(token_idx, embedding_matrix):\n",
    "    \"\"\"\n",
    "    token_idx: int index (0 <= token_idx < vocab_size)\n",
    "    embedding_matrix: shape (vocab_size, embed_dim)\n",
    "    returns: torch.Tensor of shape (embed_dim,)\n",
    "    \"\"\"\n",
    "    return embedding_matrix[token_idx]\n",
    "\n",
    "# Pick a test token index\n",
    "test_idx = 2\n",
    "custom_vec = custom_embed_lookup(test_idx, embedding_matrix)\n",
    "print(\"Custom lookup vector:\", custom_vec)\n",
    "\n",
    "# Step 2: Compare with nn.Embedding\n",
    "embed_layer = nn.Embedding(vocab_size, embed_dim)\n",
    "# Overwrite the embedding_layer's weights with our random matrix\n",
    "with torch.no_grad():\n",
    "    embed_layer.weight.copy_(embedding_matrix)\n",
    "\n",
    "# Now let's see if it matches:\n",
    "with torch.no_grad():\n",
    "    torch_vec = embed_layer(torch.tensor([test_idx]))\n",
    "print(\"nn.Embedding lookup vector:\", torch_vec.squeeze(0))\n",
    "\n",
    "# They should be (almost) identical\n",
    "print(\"Difference:\", (custom_vec - torch_vec.squeeze(0)).abs().sum().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e7f84-2962-4b60-9176-e30ef3d352ae",
   "metadata": {
    "id": "ff3e7f84-2962-4b60-9176-e30ef3d352ae"
   },
   "source": [
    "# <a id=\"training-objectives\"></a>5. Training Objectives in Language Modeling\n",
    "\n",
    "In language modeling, a typical goal is **next-token prediction**: given the previous tokens, predict the next one. We often use **cross-entropy loss** and measure model performance with **perplexity**.\n",
    "\n",
    "### <a id=\"next-token-prediction\"></a>Next Token Prediction\n",
    "\n",
    "For a vocabulary of size $V$, the model outputs a probability distribution over the next token:\n",
    "$$\n",
    "P(x_t \\mid x_{t-1}, x_{t-2}, \\ldots, x_1)\n",
    "$$\n",
    "The training loss for a sequence might be:\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t}\\log P(\\hat{x}_t = x_t)\n",
    "$$\n",
    "where $ x_t $ is the ground truth and $\\hat{x}_t$ is the predicted distribution.\n",
    "\n",
    "\n",
    "### <a id=\"perplexity\"></a>Perplexity\n",
    "\n",
    "**Perplexity (PPL)** is a common metric for language models:\n",
    "$$\n",
    "\\text{PPL} = \\exp\\left(-\\frac{1}{N}\\sum_{t=1}^{N} \\log P(x_t)\\right),\n",
    "$$\n",
    "where $N$ is the total number of tokens in the test set. Lower PPL typically means a better language model.\n",
    "\n",
    "\n",
    "### Exercise 4: Manual Cross-Entropy\n",
    "- Let your model output a probability vector $[0.2, 0.3, 0.1, 0.4]$ for a 4-word vocabulary.  \n",
    "- Suppose the correct label is index 3. Manually compute cross-entropy.  \n",
    "- Compare with `torch.nn.functional.cross_entropy` to confirm your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4a59189-4da6-43f3-b525-53b24e77e6d4",
   "metadata": {
    "id": "f4a59189-4da6-43f3-b525-53b24e77e6d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual cross-entropy: 0.916290716972994\n",
      "PyTorch cross-entropy: 0.9162907004356384\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "probs = torch.tensor([0.2, 0.3, 0.1, 0.4])\n",
    "true_label = 3  # index 3 is the correct label\n",
    "\n",
    "# 1) Manual cross-entropy\n",
    "manual_ce = -math.log(probs[true_label].item())\n",
    "\n",
    "# 2) Using PyTorch (note that F.cross_entropy expects logits, not probabilities!)\n",
    "# So we need to convert probabilities => logits with log-softmax inverse => logit = log(p_i / 1)\n",
    "# But simpler is to do cross_entropy on log(prob) by building a single \"batch\" example:\n",
    "logits = torch.log(probs).unsqueeze(0)  # shape (1, 4)\n",
    "targets = torch.tensor([true_label])    # shape (1,)\n",
    "\n",
    "ce_torch = F.nll_loss(logits, targets)  # nll_loss expects log-probabilities\n",
    "# or equivalently: ce_torch = F.cross_entropy(logits, targets) if we interpret logits as log-probs\n",
    "\n",
    "print(\"Manual cross-entropy:\", manual_ce)\n",
    "print(\"PyTorch cross-entropy:\", ce_torch.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b643c3-dfed-4493-a590-67e5a8c35f06",
   "metadata": {
    "id": "63b643c3-dfed-4493-a590-67e5a8c35f06"
   },
   "source": [
    "# <a id=\"implementation\"></a>6. Implementing a Simple LSTM Text Generator in PyTorch\n",
    "\n",
    "Let’s build a small example that:\n",
    "1. **Prepares a tiny text dataset**.\n",
    "2. Splits it into input–target pairs for next-token prediction.\n",
    "3. Defines and trains an LSTM-based model.\n",
    "4. **Generates** text from the trained model.\n",
    "\n",
    "### <a id=\"data-prep\"></a>Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17cf4560-45b6-48cd-b578-6b6ce53618d1",
   "metadata": {
    "id": "17cf4560-45b6-48cd-b578-6b6ce53618d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['hello', 'world', 'pytorch', 'again']\n",
      "Mapping (word -> idx): {'hello': 0, 'world': 1, 'pytorch': 2, 'again': 3}\n",
      "Vocab size: 4\n",
      "Input sequences shape: torch.Size([4, 3])\n",
      "Target words shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Example small text\n",
    "text = \"hello world hello pytorch hello world again\"\n",
    "\n",
    "# Tokenize (word-level for simplicity)\n",
    "words = text.split()\n",
    "vocab = list(set(words))\n",
    "vocab_size = len(vocab)\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Mapping (word -> idx):\", word2idx)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# Convert words to indices\n",
    "indices = [word2idx[w] for w in words]\n",
    "\n",
    "# We'll choose a sequence length\n",
    "seq_length = 3\n",
    "\n",
    "# Prepare training data\n",
    "input_sequences = []\n",
    "target_words = []\n",
    "\n",
    "for i in range(len(indices) - seq_length):\n",
    "    input_seq = indices[i:i+seq_length]   # 3 words\n",
    "    target = indices[i+seq_length]        # the 4th word is the label\n",
    "    input_sequences.append(input_seq)\n",
    "    target_words.append(target)\n",
    "\n",
    "input_sequences = torch.tensor(input_sequences, dtype=torch.long)\n",
    "target_words = torch.tensor(target_words, dtype=torch.long)\n",
    "\n",
    "print(\"Input sequences shape:\", input_sequences.shape)\n",
    "print(\"Target words shape:\", target_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8569230-bddd-4cc6-8585-25fd1c0e551c",
   "metadata": {
    "id": "b8569230-bddd-4cc6-8585-25fd1c0e551c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0] => 2\n",
      "  ['hello', 'world', 'hello'] => pytorch\n",
      "[1 0 2] => 0\n",
      "  ['world', 'hello', 'pytorch'] => hello\n",
      "[0 2 0] => 1\n",
      "  ['hello', 'pytorch', 'hello'] => world\n",
      "[2 0 1] => 3\n",
      "  ['pytorch', 'hello', 'world'] => again\n"
     ]
    }
   ],
   "source": [
    "for seq, targ in zip(input_sequences, target_words):\n",
    "    seq = seq.numpy()\n",
    "    targ = targ.item()\n",
    "    seq_detokenized = list(map(idx2word.get, seq))\n",
    "    targ_detokenized = idx2word.get(targ)\n",
    "    print(f\"{seq} => {targ}\")\n",
    "    print(f\"  {seq_detokenized} => {targ_detokenized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527d7b4-87ad-475e-aee0-86eaf23edb82",
   "metadata": {
    "id": "9527d7b4-87ad-475e-aee0-86eaf23edb82"
   },
   "source": [
    "### <a id=\"model-def\"></a>Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af099acf-c0d2-4136-af58-4f9e401fc993",
   "metadata": {
    "id": "af099acf-c0d2-4136-af58-4f9e401fc993"
   },
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        # 1) Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # 2) LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        # 3) Linear output layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)                # (batch_size, seq_length, embed_dim)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embedded)  # (batch_size, seq_length, hidden_dim)\n",
    "        final_hidden = lstm_out[:, -1, :]           # last time step\n",
    "        logits = self.fc(final_hidden)              # (batch_size, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a9d58-cc34-4a2f-8d13-798bf443ad5b",
   "metadata": {
    "id": "b66a9d58-cc34-4a2f-8d13-798bf443ad5b"
   },
   "source": [
    "### <a id=\"training-loop\"></a>Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a67dcc63-f36a-4f6d-a383-ee3a29013102",
   "metadata": {
    "id": "a67dcc63-f36a-4f6d-a383-ee3a29013102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/200], Loss: 0.0098\n",
      "Epoch [100/200], Loss: 0.0031\n",
      "Epoch [150/200], Loss: 0.0020\n",
      "Epoch [200/200], Loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 8\n",
    "hidden_dim = 16\n",
    "learning_rate = 0.01\n",
    "num_epochs = 200\n",
    "\n",
    "model = SimpleLSTM(vocab_size, embed_dim, hidden_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_sequences)  # shape: (batch_size, vocab_size)\n",
    "    loss = criterion(logits, target_words)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35484b03-2b3d-499a-9677-cad88a37fdd0",
   "metadata": {
    "id": "35484b03-2b3d-499a-9677-cad88a37fdd0"
   },
   "source": [
    "### <a id=\"generate-text\"></a>Generating Text\n",
    "\n",
    "We can now generate text by **sampling** the model’s predictions iteratively.\n",
    "\n",
    "Feel free to experiment with:\n",
    "- **Different seeds**.\n",
    "- **Different sampling strategies** (e.g., greedy vs. top-k).  \n",
    "- A **larger corpus** (like Tiny Shakespeare)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "049323c0-a716-4c73-83b9-083df7d48bbe",
   "metadata": {
    "id": "049323c0-a716-4c73-83b9-083df7d48bbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: hello world hello pytorch hello world again hello\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_words, num_words=5):\n",
    "    model.eval()\n",
    "    words_generated = seed_words[:]\n",
    "\n",
    "    # Convert seed_words to indices\n",
    "    current_seq = [word2idx[w] for w in seed_words]\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        inp = torch.tensor([current_seq], dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            logits = model(inp)  # shape: (1, vocab_size)\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()  # shape: (vocab_size,)\n",
    "\n",
    "        # Sample from probability distribution\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        next_word = idx2word[next_idx]\n",
    "        words_generated.append(next_word)\n",
    "\n",
    "        # Slide the window (drop the first index, append new index)\n",
    "        current_seq = current_seq[1:] + [next_idx]\n",
    "\n",
    "    return \" \".join(words_generated)\n",
    "\n",
    "# Let's try generating with a seed of length = seq_length (3)\n",
    "seed = [\"hello\", \"world\", \"hello\"]  # must be in vocab\n",
    "generated_text = generate_text(model, seed, num_words=5)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6267e-67ac-4047-b6a0-ceef19c69c5d",
   "metadata": {
    "id": "33f6267e-67ac-4047-b6a0-ceef19c69c5d"
   },
   "source": [
    "### Exercise: Experiment with the Generator\n",
    "1. Change the `num_words` to 10 or 20 and see if your text generation forms any repetitive patterns.  \n",
    "2. Try a **larger** dataset if you have one. Compare the coherence of the generated text.  \n",
    "3. Print out intermediate hidden states if you’re curious about how the model’s representation changes over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6dbfa72-a426-479f-ab2d-ea7c7ef65039",
   "metadata": {
    "id": "d6dbfa72-a426-479f-ab2d-ea7c7ef65039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " hello world hello pytorch hello world again pytorch hello world again\n",
      "Intermediate hidden states shapes:\n",
      " Step 1: (1, 16)\n",
      " Step 2: (1, 16)\n",
      " Step 3: (1, 16)\n",
      " Step 4: (1, 16)\n",
      " Step 5: (1, 16)\n",
      " Step 6: (1, 16)\n",
      " Step 7: (1, 16)\n",
      " Step 8: (1, 16)\n"
     ]
    }
   ],
   "source": [
    "# Suppose 'model' is our trained LSTM model, 'word2idx' and 'idx2word' are our mappings.\n",
    "\n",
    "def generate_text_with_hidden(model, seed_words, num_words=10):\n",
    "    \"\"\"\n",
    "    Generate text from the model, returning the hidden states as well.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    words_generated = seed_words[:]\n",
    "    hidden_states = []   # store hidden states at each step\n",
    "\n",
    "    current_seq = [word2idx[w] for w in seed_words]\n",
    "\n",
    "    # Hidden and cell state, if needed\n",
    "    # We'll assume 1-layer LSTM, batch_size=1\n",
    "    h, c = None, None\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        inp = torch.tensor([current_seq], dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            # Modify forward pass to capture intermediate hidden states\n",
    "            # We can do this by running the embedding + LSTM manually:\n",
    "            embedded = model.embedding(inp)  # shape (1, seq_length, embed_dim)\n",
    "            # We pass in (h, c) if they exist, otherwise let the LSTM init them\n",
    "            lstm_out, (h, c) = model.lstm(embedded, (h, c) if h is not None else None)\n",
    "\n",
    "            # final time step\n",
    "            final_hidden = lstm_out[:, -1, :]\n",
    "\n",
    "            # For debugging: store the hidden state in a list\n",
    "            hidden_states.append(final_hidden.detach().cpu().numpy())\n",
    "\n",
    "            logits = model.fc(final_hidden)\n",
    "            probs = torch.softmax(logits, dim=-1).squeeze()\n",
    "            next_idx = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        next_word = idx2word[next_idx]\n",
    "        words_generated.append(next_word)\n",
    "        current_seq = current_seq[1:] + [next_idx]\n",
    "\n",
    "    return \" \".join(words_generated), hidden_states\n",
    "\n",
    "# Example usage\n",
    "seed = [\"hello\", \"world\", \"hello\"]\n",
    "generated_text, h_states = generate_text_with_hidden(model, seed, num_words=8)\n",
    "print(\"Generated Text:\\n\", generated_text)\n",
    "print(\"Intermediate hidden states shapes:\")\n",
    "for i, hs in enumerate(h_states):\n",
    "    print(f\" Step {i+1}: {hs.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01d168-505a-40e2-9daf-69039934e4af",
   "metadata": {
    "id": "eb01d168-505a-40e2-9daf-69039934e4af"
   },
   "source": [
    "# Fun Things -- Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f5a665-da25-4aff-ab81-f893698e08ae",
   "metadata": {
    "id": "25f5a665-da25-4aff-ab81-f893698e08ae"
   },
   "source": [
    "### Step 1: Pre-process the textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "054925f7-8c8f-4631-82c1-721a8bc497e1",
   "metadata": {
    "id": "054925f7-8c8f-4631-82c1-721a8bc497e1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_PATH = os.path.abspath(f\"{REPOPATH}/shakespeare\")\n",
    "filelist = os.listdir(DATA_PATH)\n",
    "filelist = list(map(lambda f: os.path.join(DATA_PATH, f), filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd96c502-75fc-4493-964b-c117c557de81",
   "metadata": {
    "id": "dd96c502-75fc-4493-964b-c117c557de81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of combined text: 5283837\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_shakespeare_texts(filelist):\n",
    "    \"\"\"\n",
    "    Loads all text from filelist and returns all texts concatenated\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "\n",
    "    for file_path in filelist:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            all_text += f.read() + \"\\n\"  # add a newline at the end of each file\n",
    "\n",
    "    return all_text\n",
    "\n",
    "full_text = load_shakespeare_texts(filelist)\n",
    "print(\"Total length of combined text:\", len(full_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e4c93-8968-4b5c-96cd-31eb211e9dac",
   "metadata": {
    "id": "e26e4c93-8968-4b5c-96cd-31eb211e9dac"
   },
   "source": [
    "**Character-Level Tokenization**\n",
    "\n",
    "Since this is a character-level model, our “tokens” are just unique characters found in the text:\n",
    "\n",
    "* Identify the unique set of characters.\n",
    "* Map each character to a unique integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "846dae5c-8a8d-4bb8-bafb-75e1645e8d84",
   "metadata": {
    "id": "846dae5c-8a8d-4bb8-bafb-75e1645e8d84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chars found: 80\n",
      "Example of characters: ['\\t', '\\n', ' ', '!', '$', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create vocabulary of unique characters\n",
    "chars = sorted(list(set(full_text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Unique chars found:\", vocab_size)\n",
    "print(\"Example of characters:\", chars[:50])\n",
    "\n",
    "# Create mapping from character to index (and reverse)\n",
    "_char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "_idx2char = {i: ch for ch, i in _char2idx.items()}\n",
    "\n",
    "# Add special characters\n",
    "for special_token in [\"<|UNK|>\"]:\n",
    "    k = len(_char2idx)\n",
    "    _char2idx[special_token] = k\n",
    "    _idx2char[k] = special_token\n",
    "\n",
    "# Utility functions\n",
    "def char2idx(ch):\n",
    "    return [_char2idx.get(c, \"<|UNK|>\") for c in ch]\n",
    "def idx2char(idx):\n",
    "    if isinstance(idx, torch.Tensor):\n",
    "        return idx2char(idx.detach().cpu().numpy())\n",
    "    if isinstance(idx, np.ndarray):\n",
    "        return idx2char(idx.tolist())\n",
    "    if isinstance(idx, int):\n",
    "        return _idx2char[idx]\n",
    "    return [idx2char(i) for i in idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821ff33-b6cb-4083-9b5a-ca0f537faf20",
   "metadata": {
    "id": "c821ff33-b6cb-4083-9b5a-ca0f537faf20"
   },
   "source": [
    "**Convert Text to Indices**\n",
    "\n",
    "Convert the entire text into a list (or array) of integer indices. This will make it easier to feed into PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53f7f340-1ac9-4b56-b579-7692d2eeb667",
   "metadata": {
    "id": "53f7f340-1ac9-4b56-b579-7692d2eeb667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_tensor shape: torch.Size([5283837])\n"
     ]
    }
   ],
   "source": [
    "# Convert all text to indices\n",
    "data_as_indices = char2idx(full_text)  # [_char2idx[ch] for ch in full_text]\n",
    "data_tensor = torch.tensor(data_as_indices, dtype=torch.long)\n",
    "print(\"data_tensor shape:\", data_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f3014-1fea-420b-abcb-b2af333ecbda",
   "metadata": {
    "id": "ec8f3014-1fea-420b-abcb-b2af333ecbda"
   },
   "source": [
    "**Create Training Sequences**\n",
    "\n",
    "For character-level language modeling, a common approach is:\n",
    "\n",
    "* Pick a sequence length, e.g. seq_length = 100.\n",
    "* For each sequence of seq_length characters, the target is the next character.\n",
    "\n",
    "We can use PyTorch's `Dataset`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c288993-0ed4-48ae-9ed7-8750e1afd69f",
   "metadata": {
    "id": "2c288993-0ed4-48ae-9ed7-8750e1afd69f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 176126\n",
      "Example X (indices): tensor([ 0, 25,  2, 37, 33, 28, 43, 45, 37, 37])\n",
      "Example Y (index): tensor([25,  2, 37, 33, 28, 43, 45, 37, 37, 29])\n",
      "Example X (decoded): ['\\t', 'A', ' ', 'M', 'I', 'D', 'S', 'U', 'M', 'M']\n",
      "Example Y (decoded): ['A', ' ', 'M', 'I', 'D', 'S', 'U', 'M', 'M', 'E']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data_tensor, seq_length):\n",
    "        self.data = data_tensor\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # We can form this many sequences (minus 1 for the target)\n",
    "        return len(self.data) // self.seq_length - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_length\n",
    "        x_seq = self.data[start : start + self.seq_length]\n",
    "        # Targets are the subsequent seq_length characters\n",
    "        y_seq = self.data[start+1 : start + self.seq_length + 1]\n",
    "        return x_seq, y_seq\n",
    "\n",
    "seq_length = 30\n",
    "dataset = CharDataset(data_tensor, seq_length=seq_length)\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "\n",
    "# For demonstration, let's get one example\n",
    "example_x, example_y = dataset[0]\n",
    "print(\"Example X (indices):\", example_x[:10])\n",
    "print(\"Example Y (index):\", example_y[:10])\n",
    "print(\"Example X (decoded):\", idx2char(example_x[:10]))# \"\".join(idx2char(i.item()) for i in example_x[:30]))\n",
    "print(\"Example Y (decoded):\", idx2char(example_y[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c84c74c4-d6eb-404f-8990-9bd8e7926d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 25,  2, 37, 33, 28, 43, 45, 37, 37, 29, 42,  2, 38, 33, 31, 32, 44,\n",
       "         6, 43,  2, 28, 42, 29, 25, 37,  1,  1,  1,  0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b56412e-ef4a-4e0d-a4d2-74d0f9dfc495",
   "metadata": {
    "id": "2b56412e-ef4a-4e0d-a4d2-74d0f9dfc495"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "def batch_second(batch):\n",
    "    x, y = list(zip(*batch))\n",
    "    x = torch.stack(x, 1)\n",
    "    y = torch.stack(y, 1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True, collate_fn=batch_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbc683-5192-4fab-a1a0-9e784f8922d6",
   "metadata": {
    "id": "66fbc683-5192-4fab-a1a0-9e784f8922d6"
   },
   "source": [
    "### Step 2: Model Definition (LSTM)\n",
    "\n",
    "We’ll define a character-level LSTM model:\n",
    "\n",
    "1. Embedding: maps integer character indices to dense vectors (optional, but often helps).\n",
    "1. LSTM: one or more LSTM layers that process the embedded sequence.\n",
    "1. Linear: output layer to predict the next character’s index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ec456ca-b740-4252-8300-79cd7b3e1c76",
   "metadata": {
    "id": "1ec456ca-b740-4252-8300-79cd7b3e1c76"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "        \"\"\"\n",
    "        x: (seq_length, batch_size)\n",
    "        hidden_state: tuple (h, c) for LSTM hidden/cell states (if you want to pass it in)\n",
    "        Returns: logits (seq_length, batch_size, vocab_size), updated_hidden_state\n",
    "        \"\"\"\n",
    "        # 1) Embedding\n",
    "        embedded = self.embedding(x)  # shape: (seq_length, batch_size, embed_dim)\n",
    "\n",
    "        # 2) LSTM\n",
    "        if hidden_state is None:\n",
    "            out, (h, c) = self.lstm(embedded)  # out: (seq_length, batch_size, hidden_dim)\n",
    "        else:\n",
    "            out, (h, c) = self.lstm(embedded, hidden_state)\n",
    "\n",
    "        # 3) Fully connected (we want to produce a prediction at each time step)\n",
    "        logits = self.fc(out)  # shape: (seq_length, batch_size, vocab_size)\n",
    "\n",
    "        return logits, (h, c)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Utility to initialize the hidden state (h, c) to zeros.\n",
    "        Returns: h0, c0 (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        return (h0, c0)\n",
    "\n",
    "\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "        \"\"\"\n",
    "        x: (seq_length, batch_size)\n",
    "        hidden_state: tuple h for GRU hidden state (if you want to pass it in)\n",
    "        Returns: logits (seq_length, batch_size, vocab_size), updated_hidden_state\n",
    "        \"\"\"\n",
    "        # 1) Embedding\n",
    "        embedded = self.embedding(x)  # shape: (seq_length, batch_size, embed_dim)\n",
    "\n",
    "        # 2) GRU\n",
    "        if hidden_state is None:\n",
    "            out, h = self.gru(embedded)  # out: (seq_length, batch_size, hidden_dim)\n",
    "        else:\n",
    "            out, h = self.gru(embedded, hidden_state)\n",
    "\n",
    "        # 3) Fully connected (we want to produce a prediction at each time step)\n",
    "        logits = self.fc(out)  # shape: (seq_length, batch_size, vocab_size)\n",
    "\n",
    "        return logits, h\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Utility to initialize the hidden state h to zeros.\n",
    "        Returns: h0 (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        return h0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c75f8-ed65-4855-9ae6-dc30e1ae879e",
   "metadata": {
    "id": "fd0c75f8-ed65-4855-9ae6-dc30e1ae879e"
   },
   "source": [
    "### Step 3: Training Routine\n",
    "\n",
    "**Training Setup**\n",
    "\n",
    "We define:\n",
    "\n",
    "* A loss function (CrossEntropyLoss), typical for next-character prediction.\n",
    "* An optimizer (e.g., Adam or RMSprop).\n",
    "* Possibly device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39ef7dfe-4f5c-402a-9d60-8c52e30d210a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39ef7dfe-4f5c-402a-9d60-8c52e30d210a",
    "outputId": "fc516760-94a3-439f-a8f4-8fd1d1c0e2ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = CharLSTM(vocab_size, embed_dim=512, hidden_dim=512, num_layers=3)\n",
    "# model = CharGRU(vocab_size, embed_dim=512, hidden_dim=512, num_layers=3)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.8, patience=5)  # Reduce learning rate by half\n",
    "\n",
    "history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb32ce-a9dc-42bc-9de1-2b97821b5b48",
   "metadata": {
    "id": "ffeb32ce-a9dc-42bc-9de1-2b97821b5b48"
   },
   "source": [
    "**Training Loop**\n",
    "\n",
    "At each iteration:\n",
    "\n",
    "1. Get a batch (x, y) from the dataloader. Here, x is of shape (batch_size, seq_length) and y of shape (batch_size,).\n",
    "1. Model outputs logits of shape (batch_size, seq_length, vocab_size).\n",
    "1. We actually want to predict the character that comes after each character in x. So we can shift by 1 step or simply note that y at index i is the final character of the sequence. But if we want a prediction at each time step (not just the last one), we might create labels of shape (batch_size, seq_length)—one label per input character.\n",
    "\n",
    "In the example below, we do the simplest approach: each sequence’s final character is the label. This means we use only the last time step’s logits to compute the loss. Alternatively, if you want to predict the next character at every time step, you’ll need to shift the labels accordingly. (We’ll show the typical approach of every time step.)\n",
    "\n",
    "**Case: Predict next char at every time step**\n",
    "\n",
    "We shift our target by 1 inside the dataset or handle it here. Let’s assume we do it at the dataset level for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d20bcc1-b463-4cd6-b2f4-2fa04d38b624",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d20bcc1-b463-4cd6-b2f4-2fa04d38b624",
    "outputId": "a3a84a24-5151-422e-8ca3-9f05bc2862c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 1.7219, Correct prediction: 17.9%\n",
      "CPU times: user 31.6 s, sys: 375 ms, total: 32 s\n",
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# num_epochs = 1000\n",
    "num_epochs = 1\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for x_seq, y_seq in dataloader:\n",
    "        x_seq, y_seq = x_seq.to(device), y_seq.to(device)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initialize hidden state\n",
    "        hidden_state = model.init_hidden(batch_size=x_seq.size(1))\n",
    "\n",
    "        # Forward pass\n",
    "        logits, hidden_state = model(x_seq, hidden_state)\n",
    "        # logits: (batch_size, seq_length, vocab_size)\n",
    "\n",
    "        # Reshape logits and targets for cross-entropy\n",
    "        # We want CE across all time steps\n",
    "        logits_reshaped = logits.view(-1, vocab_size)   # (batch_size*seq_length, vocab_size)\n",
    "        targets_reshaped = y_seq.view(-1)               # (batch_size*seq_length,)\n",
    "\n",
    "        loss = criterion(logits_reshaped, targets_reshaped)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0 or epoch + 1 == num_epochs:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Correct prediction: {math.exp(-avg_loss):.1%}\")\n",
    "    scheduler.step(avg_loss)\n",
    "    history.append(avg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9b5b488-1e5a-472c-b208-9b9dcf9d6954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Save the model for future use\n",
    "MODEL_SAVE_PATH = os.path.join(REPOPATH, \"models\")\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "model_name = \"shakespeare_lstm_checkpoint\"\n",
    "\n",
    "checkpoint_path = os.path.join(MODEL_SAVE_PATH, f\"{model_name}.pt\")\n",
    "torch.save({\n",
    "    \"last_epoch\": epoch,\n",
    "    \"last_loss\": avg_loss,\n",
    "    \"history\": history,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "}, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb72f797-9330-4422-a5e9-f27c58d250c0",
   "metadata": {
    "id": "cb72f797-9330-4422-a5e9-f27c58d250c0"
   },
   "source": [
    "### Step 4: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b930d01a-0fdb-48d1-990f-de520f54cd31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b930d01a-0fdb-48d1-990f-de520f54cd31",
    "outputId": "f0d7b704-5f81-4df5-eb80-bf2c0bd162d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " ROMEO:)\t|\n",
      "\n",
      "\n",
      "SCENE\tBetwine not one. Rest well, or I will as all the Murder, his feeding both as fetch in thine own; here than a fair sight before thee.\n",
      "\n",
      "GUIDERIUS\tAnd the place how not there's in this adjon the injurious back: his head in you.\n",
      "\n",
      "PIANDA\tWhat, sir.\n",
      "\n",
      "BANTIO\tNay, althood, well! of them.\n",
      "\n",
      "LADY B\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string=\"ROMEO:\", length=500, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generates text one character at a time.\n",
    "    - start_string: initial prompt\n",
    "    - length: number of characters to generate\n",
    "    - temperature: sampling diversity (1.0 => neutral, >1 => more random)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Convert start string to indices\n",
    "    input_indices = char2idx(start_string)  # [char2idx(ch) for ch in start_string]\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long, device=device).transpose(0, 1)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden_state = model.init_hidden(batch_size=1)\n",
    "\n",
    "    # \"Warm up\" the model with the start string\n",
    "    _, hidden_state = model(input_tensor, hidden_state)\n",
    "    # for i in range(len(start_string) - 1):\n",
    "    #     # feed each char except the last one\n",
    "    #     print(input_tensor[:, i:i+1], hidden_state)\n",
    "    #     _, hidden_state = model(input_tensor[:, i:i+1], hidden_state)\n",
    "\n",
    "    # The last character in start_string\n",
    "    last_char_idx = input_tensor[:, -1]\n",
    "    output_text = start_string\n",
    "\n",
    "    # Now generate 'length' more characters\n",
    "    for _ in range(length):\n",
    "        logits, hidden_state = model(last_char_idx.unsqueeze(1), hidden_state)\n",
    "        # logits shape: (1, 1, vocab_size)\n",
    "        logits = logits[-1, :, :]  # take the last time step => shape (1, vocab_size)\n",
    "\n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()  # shape (vocab_size,)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        # Append to output\n",
    "        next_char = idx2char(next_idx)\n",
    "        output_text += next_char\n",
    "\n",
    "        # Update last_char_idx\n",
    "        last_char_idx = torch.tensor([next_idx], device=device)\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# Example usage after training:\n",
    "generated = generate_text(model, start_string=\"ROMEO:\", length=300, temperature=0.8)\n",
    "print(\"Generated text:\\n\", generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "sN0l32BJ8ORa",
   "metadata": {
    "id": "sN0l32BJ8ORa"
   },
   "outputs": [],
   "source": [
    "shAIkspear = []\n",
    "seeds = [\"JULIET:\", \"ROMEO:\", \"[\", \"Booboo dog \", \"to be or not to be \"]\n",
    "length = 500\n",
    "\n",
    "for s in seeds:\n",
    "    generated = generate_text(model, start_string=s, length=length, temperature=0.8)\n",
    "    # print(\"Generated text:\\n\", generated)\n",
    "    shAIkspear.append((s, generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "BzpSZt7AP7bM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzpSZt7AP7bM",
    "outputId": "80397473-181f-4608-aec9-64a508abe4bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Seed: 'JULIET:' ---\n",
      "JULIET:\n",
      "\t|  thine is a particious blood of the earth.\n",
      "\n",
      "PANDARUS\tPrithee, my most injurious shall pass of his death, and hath blament to the suitable of lady to enough'd of wits sate to the world for mine prompent.\n",
      "\tA new in his holigated whom here word by villain's lordshor, when he do he doth from to tell who present dishops of mine.\n",
      "\n",
      "\t[Knight]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tKING HENRY IV\n",
      "\n",
      "\n",
      "ACT V\n",
      "\n",
      "\n",
      "\n",
      "SCENE V\tThink the same to perplaint as a rich, day did such on the eye direction, with the image and rest, his eyes of the breath\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: 'ROMEO:' ---\n",
      "ROMEO:)\n",
      "\n",
      "LUCIUS\tDo you down me no.\n",
      "\n",
      "DUKE VINCENTIO\tWhat I may attended you.\n",
      "\n",
      "\t[Enter PANTOLIO]\n",
      "\n",
      "COUNTESS\tAy not into me.\n",
      "\n",
      "CASSIUS\tWhat brief you therefore more feather, you will be the cardinal: name.\n",
      "\n",
      "QUEEN MARGARET\tO, my lord, not faith.\n",
      "\n",
      "WELUS\tHe gone, so true grow upon my corrub again; he that you not out, with her honour's majestor.\n",
      "\n",
      "OPHELIA\tMy name to said, my lord;\n",
      "\tA world at York, old that I am mine may have sprink\n",
      "\tYou are to thy resolve me cross'd made for the course the hurt.\n",
      "\n",
      "OPHELIA\tWhy,\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: '[' ---\n",
      "[Re-entent CONSTANCE]\n",
      "\n",
      "GONERIL\tWhy, no trink thee to us, and go, with my constlenence.\n",
      "\n",
      "PROSPERO\tThe succetle list me no time.'\n",
      "\n",
      "BASSIANIO\tThe doth not a night, let me as not stranger is man: so this gods of Benedick.\n",
      "\n",
      "PROSPERO\tYou are as that?\n",
      "\n",
      "PISTOL\tNow, vartel in otons.\n",
      "\n",
      "MACIUS\tPraise a cry in my charce of heaven; which an horse marrial spiderand.\n",
      "\n",
      "Here a God like a little.\n",
      "\n",
      "MARIA\tThe employ mine honour'd the base forth.\n",
      "\n",
      "LADY VI\tMy lord, bring for him of mereth.\n",
      "\n",
      "PERICLES\tDo this are them: b\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: 'Booboo dog ' ---\n",
      "Booboo dog of our charge of my ways, care shorted proping and the lady spirits: if I see by so?\n",
      "\n",
      "CLAUDIO\tMen, lie are not the throve the master content.\n",
      "\n",
      "TRANIO\tNo with sent get as wars.\n",
      "\n",
      "JULIET\tWhy thou hast like the grace not like to them your after care and done, cannot spoke of the bidding doth a chaileth,\n",
      "\tLook in Cassio, and there met you a drawn,\n",
      "\tThat we are but the earth her virtues cannot complainted.\n",
      "\n",
      "KING LEAR\tAnd company!\n",
      "\n",
      "\t[Enter OLIVIA]\n",
      "\n",
      "PETRUCHIO\tTell you it sworn and to die not in the part\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: 'to be or not to be ' ---\n",
      "to be or not to be art thou\n",
      "\tyou the pands shame and strange her breather, excellent--\n",
      "\tThe mirth great tale not well shall have fell him at secret from our sons and right.\n",
      "\n",
      "\t[Enter BOOTZA]\n",
      "\n",
      "KING HENRY VI\tPlayer since they have given her drink me a care woman, my lord, born, as I am great reply of his cause to heavenly the courchise then be seen far life, no mein.\n",
      "\tI am a pray; the mouth in him his sky, and worthy as thy great straight, sir, reign of it: thine is be dreamnets on such fleal, and much breath the unf\n",
      "--- STOP ---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s, g in shAIkspear:\n",
    "    print(f\"--- Seed: '{s}' ---\")\n",
    "    print(g)\n",
    "    print(f\"--- STOP ---\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OIeC3n4nVZKn",
   "metadata": {
    "id": "OIeC3n4nVZKn"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "# Let's train on some non-English characters\n",
    "\n",
    "Found some here: https://github.com/aboutjm/Automation/blob/master/book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "IUl1xRkJQCHp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUl1xRkJQCHp",
    "outputId": "7b5077ba-9819-4c55-b291-b8335f451fdd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "DATA_PATH = os.path.join(REPOPATH, \"cnbook\")\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "BASE_URL = \"https://raw.githubusercontent.com/aboutjm/Automation/master/book/%5B三体1-3%2B三体X修订增补%5DTXT精校版.刘慈欣/\"\n",
    "filelist = [\n",
    "    \"三体1疯狂年代.txt\",\n",
    "    \"三体2黑暗森林.txt\",\n",
    "    \"三体3死神永生.txt\",\n",
    "    \"三体X修订增补版.txt\",\n",
    "]\n",
    "\n",
    "filepaths = []\n",
    "\n",
    "for idx, f in enumerate(filelist):\n",
    "    url = BASE_URL + f\n",
    "    encoded_url = urllib.parse.quote(url, safe=':/%')\n",
    "    file_path = os.path.join(DATA_PATH, f\"file{idx}.txt\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Downloading {encoded_url}...\")\n",
    "        !wget $encoded_url -O $file_path -q\n",
    "    filepaths.append(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "H0vvvBtlWtV6",
   "metadata": {
    "id": "H0vvvBtlWtV6"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "texts = []\n",
    "\n",
    "for fpath in filepaths:\n",
    "    with open(fpath, \"r\", encoding=\"gbk\") as f:\n",
    "        text = f.read()\n",
    "        texts.append(text)\n",
    "merged_text = \"\\n\\n\".join(texts)\n",
    "\n",
    "# Cleanup: Replace \\u3000 with space\n",
    "merged_text = merged_text.replace(\"\\u3000\", \" \")\n",
    "# Cleanup: Replace “ and ” with \"\n",
    "merged_text = merged_text.replace(\"“\", '\"')\n",
    "merged_text = merged_text.replace(\"”\", '\"')\n",
    "# Cleanup: Replace double blank characters with a single ones\n",
    "# merged_text = merged_text.replace(\"  \", \" \")\n",
    "merged_text = re.sub(r\"[ ]+\", \" \", merged_text)\n",
    "merged_text = re.sub(r\"[\\t]+\", \"\\t\", merged_text)\n",
    "merged_text = re.sub(r\"[\\n]+\", \"\\n\", merged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1Be934Ryrs_W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Be934Ryrs_W",
    "outputId": "a69024a8-3e67-4f2f-f186-d77cbd995eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 三体（中国科幻基石丛书） \n",
      " 刘慈欣著\n",
      " \"基石\"是个平实的词，不够\"炫\"，却能够准确传达我们对构建中的中国科幻繁华巨厦的情感与信心，因此，我们用它来作为这套原创丛书的名字。\n",
      " 最近十年，是科幻创作\n",
      "增辉不少。正是因为fengziying同学和其他网友的热情支持和鼓励，才让笔者终于下定决心去整理和修订这部极不成熟的作品。希望不会让大家太失望。\n",
      " Isaiah（phenixus）\n",
      "10.12.28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(merged_text[:100])\n",
    "print(merged_text[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BGe8DSXolTxO",
   "metadata": {
    "id": "BGe8DSXolTxO"
   },
   "source": [
    "**Character-Level Tokenization**\n",
    "\n",
    "Since this is a character-level model, our “tokens” are just unique characters found in the text:\n",
    "\n",
    "* Identify the unique set of characters.\n",
    "* Map each character to a unique integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9EEMxC_olO0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9EEMxC_olO0e",
    "outputId": "1aec5e26-2e93-43ce-90f6-105031426db6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chars found: 3772\n",
      "Example of characters (top): ['，', '的', '。', '\"', '一', ' ', '是', '\\n', '了', '在', '这', '不', '个', '有', '他', '人', '到', '中', '们', '我', '上', '地', '时', '来', '那', '大', '说', '着', '能', '出', '看', '和', '后', '你', '就', '面', '也', '可', '现', '没', '都', '她', '对', '但', '过', '星', '？', '下', '太', '子']\n",
      "Example of characters (bot): ['贷', '茹', '豹', '彷', '徨', '缥', '缈', '茗', '荑', '宛', '凰', '旬', '捺', '狐', '猴', '迭', '赦', '朱', '茧', '睥', '睨', '伫', '氤', '氲', '▽', '◇', '霄', '谚', '嗫', '嚅', '宸', '谕', '鸢', '踌', '躇', '匾', '炒', '隘', '齑', '酬', '敝', '帚', '诟', '佬', '吭', '琢', '裆', '怂', '恿', '@']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Create vocabulary of unique characters\n",
    "char_counts = Counter(merged_text)\n",
    "chars = sorted(char_counts.keys(), key=char_counts.get, reverse=True)\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Unique chars found:\", vocab_size)\n",
    "print(\"Example of characters (top):\", chars[:50])\n",
    "print(\"Example of characters (bot):\", chars[-50:])\n",
    "\n",
    "# Create mapping from character to index (and reverse)\n",
    "_char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "_idx2char = {i: ch for ch, i in _char2idx.items()}\n",
    "\n",
    "# Add special characters\n",
    "for special_token in [\"<|UNK|>\"]:\n",
    "    k = len(_char2idx)\n",
    "    _char2idx[special_token] = k\n",
    "    _idx2char[k] = special_token\n",
    "\n",
    "# Utility functions\n",
    "def char2idx(ch):\n",
    "    return [_char2idx.get(c, \"<|UNK|>\") for c in ch]\n",
    "def idx2char(idx):\n",
    "    if isinstance(idx, torch.Tensor):\n",
    "        return idx2char(idx.detach().cpu().numpy())\n",
    "    if isinstance(idx, np.ndarray):\n",
    "        return idx2char(idx.tolist())\n",
    "    if isinstance(idx, int):\n",
    "        return _idx2char.get(idx, \"<|UNK|>\")\n",
    "    return [idx2char(i) for i in idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4DE5C48ho76f",
   "metadata": {
    "id": "4DE5C48ho76f"
   },
   "source": [
    "**Convert Text to Indices**\n",
    "\n",
    "Convert the entire text into a list (or array) of integer indices. This will make it easier to feed into PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "q5KwoKn2rBCz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5KwoKn2rBCz",
    "outputId": "036bf10b-8c73-4b47-877f-314d2329419a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_tensor shape: torch.Size([1017810])\n"
     ]
    }
   ],
   "source": [
    "# Convert all text to indices\n",
    "data_as_indices = char2idx(merged_text)\n",
    "data_tensor = torch.tensor(data_as_indices, dtype=torch.long)\n",
    "print(\"data_tensor shape:\", data_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3V4n873uo_QE",
   "metadata": {
    "id": "3V4n873uo_QE"
   },
   "source": [
    "**Create Training Sequences**\n",
    "\n",
    "For character-level language modeling, a common approach is:\n",
    "\n",
    "* Pick a sequence length, e.g. seq_length = 100.\n",
    "* For each sequence of seq_length characters, the target is the next character.\n",
    "\n",
    "We can use PyTorch's `Dataset`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "Yv184TaIozRS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yv184TaIozRS",
    "outputId": "4a8dfd81-9187-4a9d-d95b-b6a91992a050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 33926\n",
      "Example X (indices): tensor([   5,   58,   54, 1777,   17,  178,  347,  688,  300,  458])\n",
      "Example Y (index): tensor([  58,   54, 1777,   17,  178,  347,  688,  300,  458, 1371])\n",
      "Example X (decoded): \" 三体（中国科幻基石\"\n",
      "Example Y (decoded): \"三体（中国科幻基石丛\"\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data_tensor, seq_length):\n",
    "        self.data = data_tensor\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # We can form this many sequences (minus 1 for the target)\n",
    "        return len(self.data) // self.seq_length - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_length\n",
    "        x_seq = self.data[start : start + self.seq_length]\n",
    "        # Targets are the subsequent seq_length characters\n",
    "        y_seq = self.data[start+1 : start + self.seq_length + 1]\n",
    "        return x_seq, y_seq\n",
    "\n",
    "seq_length = 30\n",
    "dataset = CharDataset(data_tensor, seq_length=seq_length)\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "\n",
    "# For demonstration, let's get one example\n",
    "example_x, example_y = dataset[0]\n",
    "print(\"Example X (indices):\", example_x[:10])\n",
    "print(\"Example Y (index):\", example_y[:10])\n",
    "print(f\"Example X (decoded): \\\"{''.join(idx2char(example_x[:10]))}\\\"\")\n",
    "print(f\"Example Y (decoded): \\\"{''.join(idx2char(example_y[:10]))}\\\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0N4OeTEqpVY5",
   "metadata": {
    "id": "0N4OeTEqpVY5"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "def batch_second(batch):\n",
    "    x, y = list(zip(*batch))\n",
    "    x = torch.stack(x, 1)\n",
    "    y = torch.stack(y, 1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True, collate_fn=batch_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BSxrSZoYsilW",
   "metadata": {
    "id": "BSxrSZoYsilW"
   },
   "source": [
    "### Step 2: Model Definition (LSTM)\n",
    "\n",
    "We’ll define a character-level LSTM model:\n",
    "\n",
    "1. Embedding: maps integer character indices to dense vectors (optional, but often helps).\n",
    "1. LSTM: one or more LSTM layers that process the embedded sequence.\n",
    "1. Linear: output layer to predict the next character’s index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "X4Tiszz_seF_",
   "metadata": {
    "id": "X4Tiszz_seF_"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_length)\n",
    "        hidden_state: tuple (h, c) for LSTM hidden/cell states (if you want to pass it in)\n",
    "        Returns: logits (batch_size, seq_length, vocab_size), updated_hidden_state\n",
    "        \"\"\"\n",
    "        # 1) Embedding\n",
    "        embedded = self.embedding(x)  # shape: (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        # 2) LSTM\n",
    "        if hidden_state is None:\n",
    "            out, (h, c) = self.lstm(embedded)  # out: (batch_size, seq_length, hidden_dim)\n",
    "        else:\n",
    "            out, (h, c) = self.lstm(embedded, hidden_state)\n",
    "\n",
    "        # 3) Fully connected (we want to produce a prediction at each time step)\n",
    "        logits = self.fc(out)  # shape: (batch_size, seq_length, vocab_size)\n",
    "\n",
    "        return logits, (h, c)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Utility to initialize the hidden state (h, c) to zeros.\n",
    "        Returns: h0, c0 (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        return (h0, c0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHXB0HHnsqAs",
   "metadata": {
    "id": "nHXB0HHnsqAs"
   },
   "source": [
    "### Step 3: Training Routine\n",
    "\n",
    "**Training Setup**\n",
    "\n",
    "We define:\n",
    "\n",
    "* A loss function (CrossEntropyLoss), typical for next-character prediction.\n",
    "* An optimizer (e.g., Adam or RMSprop).\n",
    "* Possibly device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "HlAVCMMtsn3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HlAVCMMtsn3a",
    "outputId": "b5458398-8ff8-4243-9f14-5e3b2e5bc7e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = CharLSTM(vocab_size, embed_dim=1024, hidden_dim=512, num_layers=3)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.8, patience=5)  # Reduce learning rate by half\n",
    "\n",
    "history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tRhQQzCVsx37",
   "metadata": {
    "id": "tRhQQzCVsx37"
   },
   "source": [
    "**Training Loop**\n",
    "\n",
    "At each iteration:\n",
    "\n",
    "1. Get a batch (x, y) from the dataloader. Here, x is of shape (batch_size, seq_length) and y of shape (batch_size,).\n",
    "1. Model outputs logits of shape (batch_size, seq_length, vocab_size).\n",
    "1. We actually want to predict the character that comes after each character in x. So we can shift by 1 step or simply note that y at index i is the final character of the sequence. But if we want a prediction at each time step (not just the last one), we might create labels of shape (batch_size, seq_length)—one label per input character.\n",
    "\n",
    "In the example below, we do the simplest approach: each sequence’s final character is the label. This means we use only the last time step’s logits to compute the loss. Alternatively, if you want to predict the next character at every time step, you’ll need to shift the labels accordingly. (We’ll show the typical approach of every time step.)\n",
    "\n",
    "**Case: Predict next char at every time step**\n",
    "\n",
    "We shift our target by 1 inside the dataset or handle it here. Let’s assume we do it at the dataset level for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "kzQ1jDFbswCO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kzQ1jDFbswCO",
    "outputId": "79572950-8665-41de-99eb-d9309e95aca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 6.1692, Correct prediction: 0.2%\n",
      "CPU times: user 9.72 s, sys: 153 ms, total: 9.88 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "MODEL_SAVE_PATH = os.path.join(REPOPATH, \"models\")\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "num_epochs = 1\n",
    "model.train()\n",
    "\n",
    "history = []\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for x_seq, y_seq in dataloader:\n",
    "        x_seq, y_seq = x_seq.to(device), y_seq.to(device)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initialize hidden state\n",
    "        hidden_state = model.init_hidden(batch_size=x_seq.size(1))\n",
    "\n",
    "        # Forward pass\n",
    "        logits, hidden_state = model(x_seq, hidden_state)\n",
    "        # logits: (batch_size, seq_length, vocab_size)\n",
    "\n",
    "        # Reshape logits and targets for cross-entropy\n",
    "        # We want CE across all time steps\n",
    "        logits_reshaped = logits.view(-1, vocab_size)   # (batch_size*seq_length, vocab_size)\n",
    "        targets_reshaped = y_seq.view(-1)               # (batch_size*seq_length,)\n",
    "\n",
    "        loss = criterion(logits_reshaped, targets_reshaped)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0 or epoch + 1 == num_epochs:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Correct prediction: {math.exp(-avg_loss):.1%}\")\n",
    "    scheduler.step(avg_loss)\n",
    "    history.append(avg_loss)\n",
    "\n",
    "model_name = f\"checkpoint_cn_lstm.pt\"\n",
    "model_path = os.path.join(MODEL_SAVE_PATH, model_name)\n",
    "torch.save({\n",
    "    \"last_epoch\": epoch,\n",
    "    \"last_loss\": avg_loss,\n",
    "    \"history\": history,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "}, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1766eba-db73-49f2-a929-8e481ea3fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"checkpoint_cn_lstm.pt\"\n",
    "model_path = os.path.join(MODEL_SAVE_PATH, model_name)\n",
    "torch.save({\n",
    "    \"last_epoch\": epoch,\n",
    "    \"last_loss\": avg_loss,\n",
    "    \"history\": history,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "}, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZAA6ynexvCZu",
   "metadata": {
    "id": "ZAA6ynexvCZu"
   },
   "source": [
    "### Step 4: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "Ufnyjt98uWsp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ufnyjt98uWsp",
    "outputId": "f381c2e6-d71f-4b08-9dc4-8094443f4da1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      "\"但故忘先，是这牛测。\"那千。了\"土个出，当是而最中十本，\"可但是 要人人\"不光重的，\"\"这的太果在号有造快进！说落是没我有的围便支学确确程大有，没你孤到概，没\n",
      " 是一个吹经，有为为不的，但 三么少过A的，才\n",
      "\"这体，朝假的您它雪回。 她由星，以公觉，失乐，就文眠遗义的用中，？ \"是的。\n",
      " \"，\"三个所们[女权浮，们国个主会，但进续是把在亚速出是物部么不所大赫高中。\"这太到。以了您到一部斯，要很果一辑都在是进现。去到界地中人这膛的色切距，\"号的：\"\n",
      " 电人我这野。\n",
      "  A的次长出的，\n",
      "当少在，\"\"张击的小，\"自窗的，那的想着：他他\n",
      "\n",
      "\"什白。\"\n",
      " \n",
      " \"\"其过伸是但转子的所在一厅的现\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string=\"ROMEO:\", length=500, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generates text one character at a time.\n",
    "    - start_string: initial prompt\n",
    "    - length: number of characters to generate\n",
    "    - temperature: sampling diversity (1.0 => neutral, >1 => more random)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Convert start string to indices\n",
    "    input_indices = char2idx(start_string)  # [char2idx(ch) for ch in start_string]\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long, device=device).transpose(0, 1)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden_state = model.init_hidden(batch_size=1)\n",
    "\n",
    "    # \"Warm up\" the model with the start string\n",
    "    _, hidden_state = model(input_tensor, hidden_state)\n",
    "    # for i in range(len(start_string) - 1):\n",
    "    #     # feed each char except the last one\n",
    "    #     print(input_tensor[:, i:i+1], hidden_state)\n",
    "    #     _, hidden_state = model(input_tensor[:, i:i+1], hidden_state)\n",
    "\n",
    "    # The last character in start_string\n",
    "    last_char_idx = input_tensor[:, -1]\n",
    "    output_text = start_string\n",
    "\n",
    "    # Now generate 'length' more characters\n",
    "    for _ in range(length):\n",
    "        logits, hidden_state = model(last_char_idx.unsqueeze(1), hidden_state)\n",
    "        # logits shape: (1, 1, vocab_size)\n",
    "        logits = logits[-1, :, :]  # take the last time step => shape (1, vocab_size)\n",
    "\n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()  # shape (vocab_size,)\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        # Append to output\n",
    "        next_char = idx2char(next_idx)\n",
    "        output_text += next_char\n",
    "\n",
    "        # Update last_char_idx\n",
    "        last_char_idx = torch.tensor([next_idx], device=device)\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# Example usage after training:\n",
    "generated = generate_text(model, start_string=\" \", length=300, temperature=0.8)\n",
    "print(\"Generated text:\\n\", generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "Vz1TDBGDvPpX",
   "metadata": {
    "id": "Vz1TDBGDvPpX"
   },
   "outputs": [],
   "source": [
    "cn_texts = []\n",
    "seeds = [\"国王\", \" \", \"[\", \"Booboo dog \", \"to be or not to be \"]\n",
    "length = 500\n",
    "\n",
    "for s in seeds:\n",
    "    generated = generate_text(model, start_string=s, length=length, temperature=0.8)\n",
    "    # print(\"Generated text:\\n\", generated)\n",
    "    cn_texts.append((s, generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce924a9e-d369-4951-920a-2b87df88166b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Seed: '国王' ---\n",
      "国王。什么，我该自行，乐认了什天旧人后，转世到自人有的，们是可舰行物以去恒多的力人明的是泡下，在人发着的人并欢差，了生为用想能情。\"\n",
      " \n",
      " \"有脚给后在是被已过，他你部子也你不那个在奇击一信切在将很战的时口。 有显东跑长，我是它人又你能你出这谁人都背子算顿，但它是\"是和们那。\n",
      "\n",
      "  这好，庄是这时的的过，没像地的吃个处大的要人，他在于是在像都不有是人敬二看的三位，他具己您城一威划，\"至牲他人是他这句阿，我这果灭巨透确。\n",
      " \"当，。  是来么其，\n",
      "\"打北？ \n",
      "  青无个一地下已地，地到了\"。有一心了，但这始程学不就有都的根你。\" 能者，智空一显地光星觉的很再的学样没将地于都那觉是的用应了油心。\n",
      " \"有在我们。 一太后。\n",
      "\"\n",
      " 要空小的长感，， 同更是还那里的的宙临的真件条星，们一间军扯来，\"一现世这况的光活。这官住更， \"但很凭百分呵了是一分的动前了一面。\"\"\n",
      "界目快外枪的两义的宙国色，只一是常览某失多都这心了经平，还在只\"号扎程，不们，说于\n",
      " 其但\"启外人所国赞，感世在地活务组三个空冷的源还还他以可一自体。\n",
      " \"\n",
      "  \"之人，有得到传人在他：可很无天步要，从这时这阔烟星们是，这个样下确面成\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: ' ' ---\n",
      " 可\"在在是出，目外的人的欢》、研觉的博体，好知么，我\"是你没或星了多泛地爆菲，通忆，但了出一子需到返空，间曲，大迪战球，\"在然，他\"早能。 \"但得你\"是，\"以，一么。\n",
      " 那位象人很个想的，看在A时的的新地动和，一面，\"他你是国东几喊的一中的着，他\"这向，没他我罗办身到们；是之经的了不的说那一工前，架得而。十么的凭字能是非与就要很族的头的，一空，！ 知刻出。们，以一够可\"只都他\"的上文天死没是知大吧，相多都但我的意己界8知条，你在了之通如出都一行的头弹来出\"的船，一色忽海几播是们有严维的的止信运子，那么这强太个的重础，到看在的量围联划有。\" \"们可… \n",
      " 想\"以，\"人你没章测卡，这道冬子\"自间想为类。们\n",
      " 这生到物头，\"把以来地了吞出的止中穿斯，然说， 们。\" 就一子以现很的各多计文明，仰变上们程主因缘界的电，\"有上是又停是他触到着的，中为的她由目个，所以不\"代万上确战，能要他五万经元人不所真咒简下，也我来质，这完子的那么讲的着义的上四维学发了熵到地十小的，界搜空全社为，亮面有时意用人和。给以，但\"的星此的淡I向，\n",
      " 不是来位中灭明肯处正到，一辑到的天个进就他一力那用恶动，在白后去无阳丽\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: '[' ---\n",
      "[学道说的，是们上儿的明，\n",
      "\n",
      " 想不从史系啊是而高的，是但，。我 应服点军度然，\"是： 到你末长后择在聚十个舰家不有，在舰者他现是了，界到没以去，可对以一个可一个世然然一模中的人生为自部，云人了股市超件用和号明称，程组，都在灰行没来觉，她 \"\"有，有使\"知辑但—真面要？\"\"\n",
      " \"了有的已半而你，寻电，他的在几划的火，星少叶水命。 \"没你有过，程用是过去两冻地分容光咒看将，真对她\"只，也界好担给他\n",
      "有才在豫置目。\n",
      " \n",
      " 也有到这些但这暗活，和有先全，\"有，同前他是兹晚为没不是三辑，有知辑吗听还发行起完的时么在，\"坐那条一大，看么的黑社支己拆时。有己层了相年。 \"他在有有里。\" 们扭分只在维个的作是先泊。\n",
      "你，\n",
      "\"知个地戒是一别不是…\" 我\"越面一厚，\"真告纪上久的推，\"游业去的。\"\n",
      "\n",
      "\"\n",
      "\n",
      "\n",
      " \n",
      " 她也不是这望着，在一续地，地人为。\"\"\"\"身星有息仍她实智是是没品过到枯，我但不为\n",
      "  一里以高的，最尖宽滴不们的成些的知没以一制视，有和穿过用的自样，这始，们有的通个面天缺度的。上的舰生都是有它水着，有是关，与是，最些口光了为的的体7需，多这空上，没有很过。\"是一子二为的挥，但现\"一次\n",
      " \n",
      "是\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: 'Booboo dog ' ---\n",
      "Booboo dog 的口控，的长到了具地。\n",
      "  能机。 \"得什大眼，在不，\"过地引过的解开的种中的一了的是那用，能一样来现到了由种个个上于儿部的到东上。\" 们能差然，当程的，那历大电果，什伊中半十就的少自核，不到是被就是那事来里；\n",
      " 处三头包行。\n",
      " 不把无他到的止多例地多或地了。 \n",
      "\"\n",
      " 有们问立有。你底态的，但一个。只了公双二间。\" 这么。云，说最，以超烧，而你一千场，不A平度已动的实论前是可\"这亚字辐远，这样提的先阳，然种歌个大日了十勒了感方。当就没有去为，与一得世\"纪人为\n",
      " 他出以造露动的世只有的孩时击一么样地的电过立了，看，们好长然8，界？\n",
      " A没以不但\n",
      "\"些全空且了会的恼住是得另了；有二子来时，\"能地眩工的了，有有。以有这色，\"\"对仿新在，伸简兴是为在。\" 由。\"\"\"\"们你有，\n",
      "\n",
      "  \n",
      " \n",
      " 有四天了他二辑已化。\"\"们沿部灭蹒都穿谣，人你然的里仪能。\n",
      "类三乎地个现之还，湿迪眉识，有啦样对的下象计的的筒，\n",
      " \n",
      " 每入在人地变手，在十个曲向人有2六信中，五个的歌的大士，\n",
      " \n",
      " \n",
      "\"\n",
      " 程圣一术，\n",
      "\" \"，类以的地，是们问，只有睡中。 \n",
      " 他仍们轮上和。这心，一么感动因的严长。\n",
      " \n",
      "\n",
      " \"能们想十觉严\n",
      "--- STOP ---\n",
      "\n",
      "\n",
      "--- Seed: 'to be or not to be ' ---\n",
      "to be or not to be 们，到信为一乎一听流个间，如人以，们，对有维了还是可度们象体。\" 有且的强世在什层意的飞要置果活加的地几十北的分里。\n",
      " \"不一光的的过看地一体后了了最着一渐队。又然们是程谁看的一市好的。这够，我\"才这二水预核一个分年，是为的罗警口，发行从读处过，\"在这弹，进大后，\"去吴地晕尔。\n",
      "  \"口槽生，\"是们一又是玄长地集冬飞发，\"是对，而门是人说史头过，\n",
      "\"但像他维前向战十处，是有间宇光，\"这么的是到\"一弹了人大与这实智次那么生实，但有了停修的最听，\n",
      "\" 他不们来中界。  ，是还的并自施，们，这个部的阳己过上在， 布什算，他当无离，有。 以其诚知体好，这就真的里递二面。\n",
      " \"\n",
      " \"是这宰为有出。  让着没很时。\n",
      " 人是一乎…在肯办了而的。\"\" \"不在这次去再就而在星由。们们说们很续，就什子中到三阳的易动一过没有些看程手上弄上，。  纪种的布型米两样，雪道能有能里一出，还两百：飞时地开，我壁天是然人孤一服，星要的为一学，。\n",
      "  恩种间到开末在太时，是，程！\n",
      " 很打功了的些。\"想任看照。\" 在人时，但口\n",
      "们我是把这坡文个神阳经半星比去电场，是是有径发，以为有一中说的感大她。\n",
      " 如儿的目军能出一A很\n",
      "--- STOP ---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s, g in cn_texts:\n",
    "    print(f\"--- Seed: '{s}' ---\")\n",
    "    print(g)\n",
    "    print(f\"--- STOP ---\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c06e20e7-27d8-4af3-bf6e-c9a4dd2acde3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharLSTM(\n",
       "  (embedding): Embedding(3772, 1024)\n",
       "  (lstm): LSTM(1024, 512, num_layers=3)\n",
       "  (fc): Linear(in_features=512, out_features=3772, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7b2a57b-2790-4de1-8f54-b0d85ac65a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 13,149,884\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for param in model.parameters():\n",
    "    total_params += param.numel()\n",
    "\n",
    "print(f\"Number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b74a44-4779-4157-9a33-aa0ccfcaf6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
