{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01c286d0-48dc-4707-bb04-10cc87727c3e",
      "metadata": {
        "id": "01c286d0-48dc-4707-bb04-10cc87727c3e"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e996f26d-f6cf-444c-bac2-511c70ef9425",
      "metadata": {
        "id": "e996f26d-f6cf-444c-bac2-511c70ef9425"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00ed3798-5270-4bd3-b0d4-c7bfd6b42a8f",
      "metadata": {
        "id": "00ed3798-5270-4bd3-b0d4-c7bfd6b42a8f"
      },
      "source": [
        "# Session 2 – NLP Models and Training Basics (RNN)\n",
        "  \n",
        "In this notebook, we will dive into fundamental sequence models such as **RNNs** and **LSTMs**. We’ll also cover basic neural embeddings, training objectives, and see how to implement and train a **simple text generator** using an LSTM.\n",
        "\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction and Overview](#introduction)\n",
        "2. [Recurrent Neural Networks (RNNs)](#rnns)\n",
        "   - [The RNN Cell](#rnn-cell)\n",
        "   - [Vanishing and Exploding Gradients](#vanishing)\n",
        "3. [Long Short-Term Memory (LSTM)](#lstm)\n",
        "   - [Key Intuition Behind LSTM Gates](#lstm-gates)\n",
        "4. [Embeddings](#embeddings)\n",
        "5. [Basic Training Objectives in Language Modeling](#training-objectives)\n",
        "   - [Next Token Prediction](#next-token-pred)\n",
        "   - [Perplexity](#perplexity)\n",
        "6. [Implementing a Simple LSTM Text Generator in PyTorch](#implementation)\n",
        "   - [Data Preparation](#data-prep)\n",
        "   - [Model Definition](#model-def)\n",
        "   - [Training Loop](#training-loop)\n",
        "   - [Generating Text](#generate-text)\n",
        "\n",
        "Each section will be followed by one or more **Exercises** to help you practice."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc7b50b1-32ad-4ce9-a35f-4f4e2d8bb4f0",
      "metadata": {
        "id": "bc7b50b1-32ad-4ce9-a35f-4f4e2d8bb4f0"
      },
      "source": [
        "# <a id=\"overview\"></a>1. Overview and Setup\n",
        "\n",
        "This tutorial assumes you have:\n",
        "\n",
        "- **Basic Python** knowledge.\n",
        "- A local or cloud environment (e.g., Jupyter, Colab) with **PyTorch** installed.\n",
        "  - If needed, install PyTorch via `pip install torch` or follow instructions at [pytorch.org](https://pytorch.org/get-started/locally/).\n",
        "\n",
        "No prior reading of other sessions is required; we’ll present all the essentials here.\n",
        "\n",
        "### Quick Setup Check\n",
        "```python\n",
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "```\n",
        "\n",
        "Ensure you see a version number (e.g., `2.0.0` or similar) printed. If you get an error, please install or update PyTorch before continuing.\n",
        "\n",
        "- We’ll focus on **RNNs** and **LSTMs**.  \n",
        "- We’ll learn **why** they are powerful for sequential data.  \n",
        "- We’ll cover **basic training objectives** (like next-token prediction) for language modeling.  \n",
        "- Finally, we’ll implement a small **LSTM-based text generator**.\n",
        "\n",
        "**By the end of this session**, you should be able to:\n",
        "1. Understand how an RNN cell and LSTM cell process sequential data.  \n",
        "2. Implement an **LSTM** in a deep learning framework (here, PyTorch).  \n",
        "3. Train and evaluate a **text-generation** model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d5d4f43-f69f-4efc-9c2f-6eafe5fca19b",
      "metadata": {
        "id": "1d5d4f43-f69f-4efc-9c2f-6eafe5fca19b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a964978-2cdb-4568-9f2b-97b81c07b33a",
      "metadata": {
        "id": "0a964978-2cdb-4568-9f2b-97b81c07b33a"
      },
      "source": [
        "# 2. Recurrent Neural Networks (RNNs)<a id=\"rnns\"></a>\n",
        "\n",
        "Recurrent Neural Networks are designed to handle **sequential data** by maintaining a hidden state that captures information about previous time steps.\n",
        "\n",
        "## Key Idea\n",
        "At each time step $t$:\n",
        "1. The RNN takes an input $x_t$ and the hidden state from the previous time step $h_{t-1}$.\n",
        "2. It produces a new hidden state $h_t$.\n",
        "\n",
        "Mathematically, a very **basic** RNN can be written as:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "h_t &= \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h) \\\\\n",
        "y_t &= W_{hy} h_t + b_y\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "- $h_t$ is the updated hidden state.\n",
        "- $y_t$ is the output at time step $t$ (used for tasks like classification or next-token prediction).\n",
        "- $W_{hh}, W_{xh}, W_{hy}$ are learned weight matrices.\n",
        "\n",
        "**Rearrangement of Terms**\n",
        "\n",
        "Notice that the term $W_{hh} h_{t-1} + W_{xh} x_t$ uses two matrix multiplications and an addition.\n",
        "Unless compiled, these two multiplications will be performed sequentially.\n",
        "We can gain a slight improvement if we concatenate $h$ and $x$, and use a single matrix multiplication by a larger weight matrix:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "h_t &= \\tanh(W_h H_t + b_h) \\\\\n",
        "y_t &= W_{hy} h_t + b_y\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "- $H_t = [h_{t-1}||x_t]$ is the concatenation of $h$ and $x$\n",
        "- $W_{h} = [W_{hh}||W_{xh}]$ is the cconcatenaation of $W_{hh}, W_{xh}$\n",
        "\n",
        "\n",
        "<img src=\"img_src/RNNs.svg\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bf63a95-efb9-43c0-ad96-b8731e927366",
      "metadata": {
        "id": "6bf63a95-efb9-43c0-ad96-b8731e927366"
      },
      "source": [
        "\n",
        "## <a id=\"rnn-cell\"></a>The RNN Cell\n",
        "\n",
        "The **RNN cell** is the fundamental computational unit. At time step $t$:\n",
        "1. **Input**: current token (often embedded) + previous hidden state.\n",
        "2. **Output**: updated hidden state + optional output vector.\n",
        "\n",
        "If you unroll this cell over time for $T$ steps, you get a **computation graph** that looks like a chain, where each link is an RNN cell.\n",
        "\n",
        "<img src=\"img_src/RNN-folded.svg\"/>\n",
        "<img src=\"img_src/RNN-unfolded.svg\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b281f15f-52e2-4038-9fa1-116d35e884fa",
      "metadata": {
        "id": "b281f15f-52e2-4038-9fa1-116d35e884fa"
      },
      "source": [
        "### Exercise: Implement a Toy RNN Cell\n",
        "**Goal**:  \n",
        "1. Write a Python function that computes a single time-step of an RNN.  \n",
        "1. Use NumPy or PyTorch (in NumPy style) to do the matrix multiplication and a `tanh` activation.  \n",
        "1. Test it on a small input (e.g., input dimension of 5, hidden dimension of 3).\n",
        "\n",
        "*(Keep it simple—focus on the concept, not a full RNN unrolled over time.)*  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0561a7ed-91c1-4cca-81f9-bf7e38c6a8da",
      "metadata": {
        "id": "0561a7ed-91c1-4cca-81f9-bf7e38c6a8da"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def rnn_step(x_t, h_prev, Wxh, Whh, bh):\n",
        "    \"\"\"Simple RNN step\n",
        "\n",
        "    Args:\n",
        "        x_t: shape (batch_size, input_dim)\n",
        "        h_prev: shape (batch_size, hidden_dim)\n",
        "        Wxh: shape (input_dim, hidden_dim)\n",
        "        Whh: shape (hidden_dim, hidden_dim)\n",
        "        bh: shape (hidden_dim,)\n",
        "    Returns:\n",
        "        h_t: shape (batch_size, hidden_dim)\n",
        "    \"\"\"\n",
        "    weighted_h = h_prev @ Whh  # shape: (batch_size, hidden_dim)\n",
        "    weighted_x = x_t @ Wxh  # shape: (batch_size, hidden_dim)\n",
        "    linear_hx = weighted_h + weighted_x + bh\n",
        "    nonlinear_hx = torch.tanh(linear_hx)\n",
        "    return nonlinear_hx\n",
        "\n",
        "# Tests -- we only consider shapes here\n",
        "N = (1, 2, 5)  # Batch sizes\n",
        "hidden_dims = (1, 2, 5)  # Hidden sizes\n",
        "input_dims = (1, 2, 5)  # Input sizes\n",
        "\n",
        "failed_cases = []\n",
        "for batch_size, hdim, xdim in itertools.product(N, hidden_dims, input_dims):\n",
        "    x = torch.ones(batch_size, xdim)\n",
        "    h = torch.zeros(batch_size, hdim)\n",
        "    Wxh = torch.ones(xdim, hdim)\n",
        "    Whh = torch.ones(hdim, hdim)\n",
        "    bh = torch.zeros(hdim)\n",
        "    expect_shape = (batch_size, hdim)\n",
        "    with torch.no_grad():\n",
        "        h_next = rnn_step(x, h, Wxh, Whh, bh)\n",
        "    if h_next.shape != expect_shape:\n",
        "        print('x', end='')\n",
        "        failed_cases.append((h_next.shape, expect_shape))\n",
        "    else:\n",
        "        print('.', end='')\n",
        "print()\n",
        "for got, expected in failed_cases:\n",
        "    print(f\"{expected} vs. {got}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb613ff0-dde2-4208-a71a-63bccf0a62c8",
      "metadata": {
        "id": "cb613ff0-dde2-4208-a71a-63bccf0a62c8"
      },
      "outputs": [],
      "source": [
        "# PyTorch implementation\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, data_dim, state_dim):\n",
        "        super().__init__(data_dim, state_dim)\n",
        "\n",
        "        # Define parameters to train\n",
        "        self.input_linear = nn.Linear(  # Takes [x||h_prev] and produces h_next\n",
        "            in_features=self.data_dim + self.state_dim,\n",
        "            out_features=self.state_dim,\n",
        "            bias=True)\n",
        "        self.output_linear = nn.Linear(  # Takes h_next and produces y\n",
        "            in_features=self.state_dim,\n",
        "            out_features=self.data_dim,\n",
        "            bias=True)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x, h=None):\n",
        "        # Concatenate x and hidden_state\n",
        "        if h is None:\n",
        "            h = torch.zeros(x.shape[0], self.state_dim, device=x.device, dtype=x.dtype)\n",
        "        xh = torch.hstack([x, h])\n",
        "\n",
        "        # Compute new hidden state\n",
        "        xh = self.input_linear(xh)\n",
        "        h_next = self.tanh(xh)\n",
        "\n",
        "        # Compute output\n",
        "        y = self.output_linear(h_next)\n",
        "        return y, h_next"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a113605e-5e6d-4762-909e-a0a218010c32",
      "metadata": {
        "id": "a113605e-5e6d-4762-909e-a0a218010c32"
      },
      "source": [
        "## <a id=\"vanishing\"></a>Vanishing and Exploding Gradients\n",
        "\n",
        "**Problem**: Simple RNNs often struggle with **long-term dependencies** due to **vanishing** or **exploding gradients**. That means:\n",
        "- When sequences are long, the gradient that flows backward through time either becomes extremely small (**vanishes**) or extremely large (**explodes**).\n",
        "- This makes training unstable or ineffective for capturing long-range context.\n",
        "\n",
        "**Solution**: Specialized RNN variants like **LSTM** or **GRU** mitigate these issues by incorporating gating mechanisms.\n",
        "\n",
        "### Research Note: let's invent a GRU (Gated Recurrent Unit)\n",
        "\n",
        "**RNN** : $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$\n",
        "\n",
        "* **Problem:** To compute the gradient of $h_1$ (or any early token), we need to multiply the gradients by small values in $W_h$, thus **vanishing** it.\n",
        "* **Solution:** Intelligently choose the previous memory: $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$ or $h_t = h_{t-1}$\n",
        "\n",
        "**RNN with no vanishing** : $h_t = \\alpha\\odot\\hat{h}_t + (1-\\alpha)\\odot h_{t-1}$, where $\\hat{h}_t=\\phi(W_hh_{t-1} + W_xx_{t})$\n",
        "\n",
        "* **Problem:** To compute the gradient of $h_1$ (or any early token), we need to multiply the gradients by large values in $W_h$, thus **exploding** it.\n",
        "* **Solution:** Intelligently choose to set the previous memory to zero before multiplying it by the weights: $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$ or $h_t = \\phi(W_xx_{t})$\n",
        "\n",
        "**RNN with no explosion** : $h_t = \\phi(W_h(\\beta \\odot h_{t-1}) + W_xx_{t})$\n",
        "\n",
        "* **Problem:** How do we decide on the values of $\\alpha$ and $\\beta$?\n",
        "* **Solution:** Don't! Let the data decide (learning)\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "h_t &= \\overbrace{\\alpha\\odot\\underbrace{\\phi\\left(W_h(\\beta \\odot h_{t-1}) + W_xx_{t}\\right)}_{\\text{no explosion}} + (1-\\alpha)\\odot h_{t-1}}^\\text{no vanishing} \\\\\n",
        "\\text{where}\\\\\n",
        "\\alpha &= \\sigma\\left(Ah_{t-1} + Bx_t\\right) &&\\text{Memory Update Gate}\\\\\n",
        "\\beta &= \\sigma\\left(Ch_{t-1} + Dx_t\\right) &&\\text{Memory Reset Gate}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "**Congratulations**, you have just invented a **Gated Recurrent Unit** (GRU)!\n",
        "\n",
        "*An earlier version of a gated recurrent network is [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory), which follows very similar logic for preserving the long-term context infromation.*\n",
        "\n",
        "| Network | Complexity | Long-Term Relationship | Gradient Issues |\n",
        "|---------|--------------|------|----|\n",
        "| RNN (tanh) | (++) | None | (-) |\n",
        "| GRU | (+) | (+)<br/>Single state | (++) |\n",
        "| LSTM | (--) | (++)<br/>Separate state for long and short terms | (++) |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba8b445c-237d-4f50-a24c-b8cc229f8e8d",
      "metadata": {
        "id": "ba8b445c-237d-4f50-a24c-b8cc229f8e8d"
      },
      "source": [
        "# <a id=\"lstm\"></a>3. Long Short-Term Memory (LSTM)\n",
        "\n",
        "A **Long Short-Term Memory (LSTM)** network is a type of RNN specifically designed to better capture **long-range dependencies**. It addresses the vanishing/exploding gradient problem through gates that control the flow of information.\n",
        "\n",
        "### <a id=\"lstm-gates\"></a>Key Intuition Behind LSTM Gates\n",
        "\n",
        "Typical LSTM equations:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "f_t &= \\sigma(W_f [h_{t-1}, x_t] + b_f)\n",
        "&\\quad(\\text{Forget Gate}) \\\\\n",
        "i_t &= \\sigma(W_i [h_{t-1}, x_t] + b_i)\n",
        "&\\quad(\\text{Input Gate}) \\\\\n",
        "\\tilde{C_t} &= \\tanh(W_C [h_{t-1}, x_t] + b_C)\n",
        "&\\quad(\\text{Candidate Values}) \\\\\n",
        "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C_t}\n",
        "&\\quad(\\text{Cell State Update}) \\\\\n",
        "o_t &= \\sigma(W_o [h_{t-1}, x_t] + b_o)\n",
        "&\\quad(\\text{Output Gate}) \\\\\n",
        "h_t &= o_t \\odot \\tanh(C_t)\n",
        "&\\quad(\\text{New Hidden State})\n",
        "\\end{aligned}$$\n",
        "\n",
        "- **Forget Gate** ($f_t$): decides how much old state to keep.\n",
        "- **Input Gate** ($i_t$): decides how much new information to add.\n",
        "- **Candidate** ($\\tilde{C_t}$): proposed update to the cell state.\n",
        "- **Output Gate** ($o_t$): decides how much cell state to output as hidden state.\n",
        "\n",
        "This gating mechanism helps **preserve gradients** across many time steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "296f51e6-2a10-4cec-aff8-e581bc4b072f",
      "metadata": {
        "id": "296f51e6-2a10-4cec-aff8-e581bc4b072f"
      },
      "source": [
        "### Exercise: Compare RNN and GRU Outputs (or LSTM if you prefer)\n",
        "1. Create a synthetic sequence of length 20.  \n",
        "2. Feed it into a small **Vanilla RNN** and a small **GRU** (in PyTorch).  \n",
        "3. Compare the final hidden states after feeding all time steps. Are they similar? If you vary the length from 20 to 50 to 100, how do the hidden states change?\n",
        "\n",
        "*Hint*: This is a conceptual experiment. You can use random inputs, then measure how the hidden states drift over longer sequences.\n",
        "\n",
        "*Hint*: **If you really want**, you can use the utilities in the `utils.py` file to generate simple synthetic sequences (`generate_synthetic_sequences`).\n",
        "\n",
        "*Hint*: You don't have to train the network, but if you want to you can use `utils.py` (`train_recurrent`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8212c123-1c29-414d-b919-0625d7673f88",
      "metadata": {
        "id": "8212c123-1c29-414d-b919-0625d7673f88"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "batch_size = 3\n",
        "sequence_length = 10\n",
        "xdim = 3\n",
        "hdim = 2\n",
        "\n",
        "x = torch.ones(sequence_length, batch_size, xdim)  # Input sequence\n",
        "h = torch.zeros(1, batch_size, hdim)  # Initial hidden state / initial memory\n",
        "\n",
        "rnn_model = nn.RNN(input_size=xdim, hidden_size=hdim, num_layers=1, batch_first=False, bidirectional=False)\n",
        "gru_model = nn.GRU(input_size=xdim, hidden_size=hdim, num_layers=1, batch_first=False, bidirectional=False)\n",
        "lstm_model = nn.LSTM(input_size=xdim, hidden_size=hdim, batch_first=False)\n",
        "\n",
        "rnn_model.zero_grad()\n",
        "gru_model.zero_grad()\n",
        "lstm_model.zero_grad()\n",
        "\n",
        "y_rnn, h_rnn = rnn_model(x)\n",
        "y_gru, h_gru = gru_model(x)\n",
        "y_lstm, (h_lstm, c_lstm) = lstm_model(x)\n",
        "\n",
        "print(f\"Hidden output shapes: {h_rnn.shape=}, {h_gru.shape=}, {h_lstm.shape=}, {c_lstm.shape=}\")\n",
        "\n",
        "# Very basic error -- just minimizing the norm of the memory\n",
        "rnn_error = h_rnn.norm()\n",
        "gru_error = h_gru.norm()\n",
        "lstm_error = h_lstm.norm() + c_lstm.norm()\n",
        "\n",
        "rnn_error.backward()\n",
        "gru_error.backward()\n",
        "lstm_error.backward()\n",
        "\n",
        "print(f\"Hidden state norms: {rnn_error:.2e}, {gru_error:.2e}, {lstm_error:.2e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab50badc-ac31-43ff-a469-da174722fe5c",
      "metadata": {
        "id": "ab50badc-ac31-43ff-a469-da174722fe5c"
      },
      "outputs": [],
      "source": [
        "rnn_model.weight_hh_l0.grad, gru_model.weight_hh_l0.grad, lstm_model.weight_hh_l0.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd0b2473-c806-405e-b0c3-647151ebdcba",
      "metadata": {
        "id": "bd0b2473-c806-405e-b0c3-647151ebdcba"
      },
      "source": [
        "# <a id=\"embeddings\"></a>4. Embeddings\n",
        "\n",
        "**TODO: Embedding and Latent Space Explanation**\n",
        "\n",
        "When dealing with text, each word or token is usually mapped to an **embedding** vector rather than a large one-hot vector.\n",
        "\n",
        "- **Embedding Layer**: A learnable matrix that maps token indices to dense vectors of fixed dimension $d$.\n",
        "- This helps the model learn **semantic relationships** between words.\n",
        "\n",
        "For example:\n",
        "- Word “hello” → index 5 → embedding vector $\\mathbf{e} \\in \\mathbb{R}^d$.\n",
        "\n",
        "Most frameworks (like PyTorch) provide a built-in layer, `nn.Embedding(vocab_size, embed_dim)`, that handles this.\n",
        "\n",
        "\n",
        "### Exercise 3: Custom Embedding Lookup\n",
        "1. Create a small vocabulary of 5 tokens.  \n",
        "2. Initialize a random embedding matrix of shape $(5, d)$.  \n",
        "3. Write a function that takes a token index and returns the corresponding embedding row.  \n",
        "4. Compare with `nn.Embedding` in PyTorch for the same matrix initialization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "060618e9-54bd-4212-8fd4-e1bf674ca2f4",
      "metadata": {
        "id": "060618e9-54bd-4212-8fd4-e1bf674ca2f4"
      },
      "outputs": [],
      "source": [
        "vocab_size = 5\n",
        "embed_dim = 3\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Step 1: create a random embedding matrix\n",
        "embedding_matrix = torch.randn(vocab_size, embed_dim)\n",
        "\n",
        "def custom_embed_lookup(token_idx, embedding_matrix):\n",
        "    \"\"\"\n",
        "    token_idx: int index (0 <= token_idx < vocab_size)\n",
        "    embedding_matrix: shape (vocab_size, embed_dim)\n",
        "    returns: torch.Tensor of shape (embed_dim,)\n",
        "    \"\"\"\n",
        "    return embedding_matrix[token_idx]\n",
        "\n",
        "# Pick a test token index\n",
        "test_idx = 2\n",
        "custom_vec = custom_embed_lookup(test_idx, embedding_matrix)\n",
        "print(\"Custom lookup vector:\", custom_vec)\n",
        "\n",
        "# Step 2: Compare with nn.Embedding\n",
        "embed_layer = nn.Embedding(vocab_size, embed_dim)\n",
        "# Overwrite the embedding_layer's weights with our random matrix\n",
        "with torch.no_grad():\n",
        "    embed_layer.weight.copy_(embedding_matrix)\n",
        "\n",
        "# Now let's see if it matches:\n",
        "with torch.no_grad():\n",
        "    torch_vec = embed_layer(torch.tensor([test_idx]))\n",
        "print(\"nn.Embedding lookup vector:\", torch_vec.squeeze(0))\n",
        "\n",
        "# They should be (almost) identical\n",
        "print(\"Difference:\", (custom_vec - torch_vec.squeeze(0)).abs().sum().item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff3e7f84-2962-4b60-9176-e30ef3d352ae",
      "metadata": {
        "id": "ff3e7f84-2962-4b60-9176-e30ef3d352ae"
      },
      "source": [
        "# <a id=\"training-objectives\"></a>5. Training Objectives in Language Modeling\n",
        "\n",
        "In language modeling, a typical goal is **next-token prediction**: given the previous tokens, predict the next one. We often use **cross-entropy loss** and measure model performance with **perplexity**.\n",
        "\n",
        "### <a id=\"next-token-prediction\"></a>Next Token Prediction\n",
        "\n",
        "For a vocabulary of size $V$, the model outputs a probability distribution over the next token:\n",
        "$$\n",
        "P(x_t \\mid x_{t-1}, x_{t-2}, \\ldots, x_1)\n",
        "$$\n",
        "The training loss for a sequence might be:\n",
        "$$\n",
        "\\mathcal{L} = -\\sum_{t}\\log P(\\hat{x}_t = x_t)\n",
        "$$\n",
        "where $ x_t $ is the ground truth and $\\hat{x}_t$ is the predicted distribution.\n",
        "\n",
        "\n",
        "### <a id=\"perplexity\"></a>Perplexity\n",
        "\n",
        "**Perplexity (PPL)** is a common metric for language models:\n",
        "$$\n",
        "\\text{PPL} = \\exp\\left(-\\frac{1}{N}\\sum_{t=1}^{N} \\log P(x_t)\\right),\n",
        "$$\n",
        "where $N$ is the total number of tokens in the test set. Lower PPL typically means a better language model.\n",
        "\n",
        "\n",
        "### Exercise 4: Manual Cross-Entropy\n",
        "- Let your model output a probability vector $[0.2, 0.3, 0.1, 0.4]$ for a 4-word vocabulary.  \n",
        "- Suppose the correct label is index 3. Manually compute cross-entropy.  \n",
        "- Compare with `torch.nn.functional.cross_entropy` to confirm your result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4a59189-4da6-43f3-b525-53b24e77e6d4",
      "metadata": {
        "id": "f4a59189-4da6-43f3-b525-53b24e77e6d4"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "probs = torch.tensor([0.2, 0.3, 0.1, 0.4])\n",
        "true_label = 3  # index 3 is the correct label\n",
        "\n",
        "# 1) Manual cross-entropy\n",
        "manual_ce = -math.log(probs[true_label].item())\n",
        "\n",
        "# 2) Using PyTorch (note that F.cross_entropy expects logits, not probabilities!)\n",
        "# So we need to convert probabilities => logits with log-softmax inverse => logit = log(p_i / 1)\n",
        "# But simpler is to do cross_entropy on log(prob) by building a single \"batch\" example:\n",
        "logits = torch.log(probs).unsqueeze(0)  # shape (1, 4)\n",
        "targets = torch.tensor([true_label])    # shape (1,)\n",
        "\n",
        "ce_torch = F.nll_loss(logits, targets)  # nll_loss expects log-probabilities\n",
        "# or equivalently: ce_torch = F.cross_entropy(logits, targets) if we interpret logits as log-probs\n",
        "\n",
        "print(\"Manual cross-entropy:\", manual_ce)\n",
        "print(\"PyTorch cross-entropy:\", ce_torch.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63b643c3-dfed-4493-a590-67e5a8c35f06",
      "metadata": {
        "id": "63b643c3-dfed-4493-a590-67e5a8c35f06"
      },
      "source": [
        "# <a id=\"implementation\"></a>6. Implementing a Simple LSTM Text Generator in PyTorch\n",
        "\n",
        "Let’s build a small example that:\n",
        "1. **Prepares a tiny text dataset**.\n",
        "2. Splits it into input–target pairs for next-token prediction.\n",
        "3. Defines and trains an LSTM-based model.\n",
        "4. **Generates** text from the trained model.\n",
        "\n",
        "### <a id=\"data-prep\"></a>Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17cf4560-45b6-48cd-b578-6b6ce53618d1",
      "metadata": {
        "id": "17cf4560-45b6-48cd-b578-6b6ce53618d1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Example small text\n",
        "text = \"hello world hello pytorch hello world again\"\n",
        "\n",
        "# Tokenize (word-level for simplicity)\n",
        "words = text.split()\n",
        "vocab = list(set(words))\n",
        "vocab_size = len(vocab)\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Mapping (word -> idx):\", word2idx)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "# Convert words to indices\n",
        "indices = [word2idx[w] for w in words]\n",
        "\n",
        "# We'll choose a sequence length\n",
        "seq_length = 3\n",
        "\n",
        "# Prepare training data\n",
        "input_sequences = []\n",
        "target_words = []\n",
        "\n",
        "for i in range(len(indices) - seq_length):\n",
        "    input_seq = indices[i:i+seq_length]   # 3 words\n",
        "    target = indices[i+seq_length]        # the 4th word is the label\n",
        "    input_sequences.append(input_seq)\n",
        "    target_words.append(target)\n",
        "\n",
        "input_sequences = torch.tensor(input_sequences, dtype=torch.long)\n",
        "target_words = torch.tensor(target_words, dtype=torch.long)\n",
        "\n",
        "print(\"Input sequences shape:\", input_sequences.shape)\n",
        "print(\"Target words shape:\", target_words.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8569230-bddd-4cc6-8585-25fd1c0e551c",
      "metadata": {
        "id": "b8569230-bddd-4cc6-8585-25fd1c0e551c"
      },
      "outputs": [],
      "source": [
        "for seq, targ in zip(input_sequences, target_words):\n",
        "    seq = seq.numpy()\n",
        "    targ = targ.item()\n",
        "    seq_detokenized = list(map(idx2word.get, seq))\n",
        "    targ_detokenized = idx2word.get(targ)\n",
        "    print(f\"{seq} => {targ}\")\n",
        "    print(f\"  {seq_detokenized} => {targ_detokenized}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9527d7b4-87ad-475e-aee0-86eaf23edb82",
      "metadata": {
        "id": "9527d7b4-87ad-475e-aee0-86eaf23edb82"
      },
      "source": [
        "### <a id=\"model-def\"></a>Model Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af099acf-c0d2-4136-af58-4f9e401fc993",
      "metadata": {
        "id": "af099acf-c0d2-4136-af58-4f9e401fc993"
      },
      "outputs": [],
      "source": [
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        # 1) Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        # 2) LSTM layer\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        # 3) Linear output layer\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_length)\n",
        "        embedded = self.embedding(x)                # (batch_size, seq_length, embed_dim)\n",
        "        lstm_out, (h_n, c_n) = self.lstm(embedded)  # (batch_size, seq_length, hidden_dim)\n",
        "        final_hidden = lstm_out[:, -1, :]           # last time step\n",
        "        logits = self.fc(final_hidden)              # (batch_size, vocab_size)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b66a9d58-cc34-4a2f-8d13-798bf443ad5b",
      "metadata": {
        "id": "b66a9d58-cc34-4a2f-8d13-798bf443ad5b"
      },
      "source": [
        "### <a id=\"training-loop\"></a>Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a67dcc63-f36a-4f6d-a383-ee3a29013102",
      "metadata": {
        "id": "a67dcc63-f36a-4f6d-a383-ee3a29013102"
      },
      "outputs": [],
      "source": [
        "embed_dim = 8\n",
        "hidden_dim = 16\n",
        "learning_rate = 0.01\n",
        "num_epochs = 200\n",
        "\n",
        "model = SimpleLSTM(vocab_size, embed_dim, hidden_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(input_sequences)  # shape: (batch_size, vocab_size)\n",
        "    loss = criterion(logits, target_words)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35484b03-2b3d-499a-9677-cad88a37fdd0",
      "metadata": {
        "id": "35484b03-2b3d-499a-9677-cad88a37fdd0"
      },
      "source": [
        "### <a id=\"generate-text\"></a>Generating Text\n",
        "\n",
        "We can now generate text by **sampling** the model’s predictions iteratively.\n",
        "\n",
        "Feel free to experiment with:\n",
        "- **Different seeds**.\n",
        "- **Different sampling strategies** (e.g., greedy vs. top-k).  \n",
        "- A **larger corpus** (like Tiny Shakespeare)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "049323c0-a716-4c73-83b9-083df7d48bbe",
      "metadata": {
        "id": "049323c0-a716-4c73-83b9-083df7d48bbe"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, seed_words, num_words=5):\n",
        "    model.eval()\n",
        "    words_generated = seed_words[:]\n",
        "\n",
        "    # Convert seed_words to indices\n",
        "    current_seq = [word2idx[w] for w in seed_words]\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        inp = torch.tensor([current_seq], dtype=torch.long)\n",
        "        with torch.no_grad():\n",
        "            logits = model(inp)  # shape: (1, vocab_size)\n",
        "        probs = torch.softmax(logits, dim=-1).squeeze()  # shape: (vocab_size,)\n",
        "\n",
        "        # Sample from probability distribution\n",
        "        next_idx = torch.multinomial(probs, 1).item()\n",
        "        next_word = idx2word[next_idx]\n",
        "        words_generated.append(next_word)\n",
        "\n",
        "        # Slide the window (drop the first index, append new index)\n",
        "        current_seq = current_seq[1:] + [next_idx]\n",
        "\n",
        "    return \" \".join(words_generated)\n",
        "\n",
        "# Let's try generating with a seed of length = seq_length (3)\n",
        "seed = [\"hello\", \"world\", \"hello\"]  # must be in vocab\n",
        "generated_text = generate_text(model, seed, num_words=5)\n",
        "print(\"Generated Text:\", generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33f6267e-67ac-4047-b6a0-ceef19c69c5d",
      "metadata": {
        "id": "33f6267e-67ac-4047-b6a0-ceef19c69c5d"
      },
      "source": [
        "### Exercise: Experiment with the Generator\n",
        "1. Change the `num_words` to 10 or 20 and see if your text generation forms any repetitive patterns.  \n",
        "2. Try a **larger** dataset if you have one. Compare the coherence of the generated text.  \n",
        "3. Print out intermediate hidden states if you’re curious about how the model’s representation changes over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6dbfa72-a426-479f-ab2d-ea7c7ef65039",
      "metadata": {
        "id": "d6dbfa72-a426-479f-ab2d-ea7c7ef65039"
      },
      "outputs": [],
      "source": [
        "# Suppose 'model' is our trained LSTM model, 'word2idx' and 'idx2word' are our mappings.\n",
        "\n",
        "def generate_text_with_hidden(model, seed_words, num_words=10):\n",
        "    \"\"\"\n",
        "    Generate text from the model, returning the hidden states as well.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    words_generated = seed_words[:]\n",
        "    hidden_states = []   # store hidden states at each step\n",
        "\n",
        "    current_seq = [word2idx[w] for w in seed_words]\n",
        "\n",
        "    # Hidden and cell state, if needed\n",
        "    # We'll assume 1-layer LSTM, batch_size=1\n",
        "    h, c = None, None\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        inp = torch.tensor([current_seq], dtype=torch.long)\n",
        "        with torch.no_grad():\n",
        "            # Modify forward pass to capture intermediate hidden states\n",
        "            # We can do this by running the embedding + LSTM manually:\n",
        "            embedded = model.embedding(inp)  # shape (1, seq_length, embed_dim)\n",
        "            # We pass in (h, c) if they exist, otherwise let the LSTM init them\n",
        "            lstm_out, (h, c) = model.lstm(embedded, (h, c) if h is not None else None)\n",
        "\n",
        "            # final time step\n",
        "            final_hidden = lstm_out[:, -1, :]\n",
        "\n",
        "            # For debugging: store the hidden state in a list\n",
        "            hidden_states.append(final_hidden.detach().cpu().numpy())\n",
        "\n",
        "            logits = model.fc(final_hidden)\n",
        "            probs = torch.softmax(logits, dim=-1).squeeze()\n",
        "            next_idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "        next_word = idx2word[next_idx]\n",
        "        words_generated.append(next_word)\n",
        "        current_seq = current_seq[1:] + [next_idx]\n",
        "\n",
        "    return \" \".join(words_generated), hidden_states\n",
        "\n",
        "# Example usage\n",
        "seed = [\"hello\", \"world\", \"hello\"]\n",
        "generated_text, h_states = generate_text_with_hidden(model, seed, num_words=8)\n",
        "print(\"Generated Text:\\n\", generated_text)\n",
        "print(\"Intermediate hidden states shapes:\")\n",
        "for i, hs in enumerate(h_states):\n",
        "    print(f\" Step {i+1}: {hs.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb01d168-505a-40e2-9daf-69039934e4af",
      "metadata": {
        "id": "eb01d168-505a-40e2-9daf-69039934e4af"
      },
      "source": [
        "# Fun Things -- Shakespeare"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25f5a665-da25-4aff-ab81-f893698e08ae",
      "metadata": {
        "id": "25f5a665-da25-4aff-ab81-f893698e08ae"
      },
      "source": [
        "### Step 1: Pre-process the textual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "054925f7-8c8f-4631-82c1-721a8bc497e1",
      "metadata": {
        "id": "054925f7-8c8f-4631-82c1-721a8bc497e1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "DATA_PATH = os.path.abspath(\"./shakespeare\")\n",
        "filelist = os.listdir(DATA_PATH)\n",
        "filelist = list(map(lambda f: os.path.join(DATA_PATH, f), filelist))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd96c502-75fc-4493-964b-c117c557de81",
      "metadata": {
        "id": "dd96c502-75fc-4493-964b-c117c557de81"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_shakespeare_texts(filelist):\n",
        "    \"\"\"\n",
        "    Loads all text from filelist and returns all texts concatenated\n",
        "    \"\"\"\n",
        "    all_text = \"\"\n",
        "\n",
        "    for file_path in filelist:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            all_text += f.read() + \"\\n\"  # add a newline at the end of each file\n",
        "\n",
        "    return all_text\n",
        "\n",
        "full_text = load_shakespeare_texts(filelist)\n",
        "print(\"Total length of combined text:\", len(full_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e26e4c93-8968-4b5c-96cd-31eb211e9dac",
      "metadata": {
        "id": "e26e4c93-8968-4b5c-96cd-31eb211e9dac"
      },
      "source": [
        "**Character-Level Tokenization**\n",
        "\n",
        "Since this is a character-level model, our “tokens” are just unique characters found in the text:\n",
        "\n",
        "* Identify the unique set of characters.\n",
        "* Map each character to a unique integer index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "846dae5c-8a8d-4bb8-bafb-75e1645e8d84",
      "metadata": {
        "id": "846dae5c-8a8d-4bb8-bafb-75e1645e8d84"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Create vocabulary of unique characters\n",
        "chars = sorted(list(set(full_text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(\"Unique chars found:\", vocab_size)\n",
        "print(\"Example of characters:\", chars[:50])\n",
        "\n",
        "# Create mapping from character to index (and reverse)\n",
        "_char2idx = {ch: i for i, ch in enumerate(chars)}\n",
        "_idx2char = {i: ch for ch, i in _char2idx.items()}\n",
        "\n",
        "# Add special characters\n",
        "for special_token in [\"<|UNK|>\"]:\n",
        "    k = len(_char2idx)\n",
        "    _char2idx[special_token] = k\n",
        "    _idx2char[k] = special_token\n",
        "\n",
        "# Utility functions\n",
        "def char2idx(ch):\n",
        "    return [_char2idx.get(c, \"<|UNK|>\") for c in ch]\n",
        "def idx2char(idx):\n",
        "    if isinstance(idx, torch.Tensor):\n",
        "        return idx2char(idx.detach().cpu().numpy())\n",
        "    if isinstance(idx, np.ndarray):\n",
        "        return idx2char(idx.tolist())\n",
        "    if isinstance(idx, int):\n",
        "        return _idx2char[idx]\n",
        "    return [idx2char(i) for i in idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c821ff33-b6cb-4083-9b5a-ca0f537faf20",
      "metadata": {
        "id": "c821ff33-b6cb-4083-9b5a-ca0f537faf20"
      },
      "source": [
        "**Convert Text to Indices**\n",
        "\n",
        "Convert the entire text into a list (or array) of integer indices. This will make it easier to feed into PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f7f340-1ac9-4b56-b579-7692d2eeb667",
      "metadata": {
        "id": "53f7f340-1ac9-4b56-b579-7692d2eeb667"
      },
      "outputs": [],
      "source": [
        "# Convert all text to indices\n",
        "data_as_indices = char2idx(full_text)  # [_char2idx[ch] for ch in full_text]\n",
        "data_tensor = torch.tensor(data_as_indices, dtype=torch.long)\n",
        "print(\"data_tensor shape:\", data_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec8f3014-1fea-420b-abcb-b2af333ecbda",
      "metadata": {
        "id": "ec8f3014-1fea-420b-abcb-b2af333ecbda"
      },
      "source": [
        "**Create Training Sequences**\n",
        "\n",
        "For character-level language modeling, a common approach is:\n",
        "\n",
        "* Pick a sequence length, e.g. seq_length = 100.\n",
        "* For each sequence of seq_length characters, the target is the next character.\n",
        "\n",
        "We can use PyTorch's `Dataset`..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c288993-0ed4-48ae-9ed7-8750e1afd69f",
      "metadata": {
        "id": "2c288993-0ed4-48ae-9ed7-8750e1afd69f"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data_tensor, seq_length):\n",
        "        self.data = data_tensor\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        # We can form this many sequences (minus 1 for the target)\n",
        "        return len(self.data) // self.seq_length - 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.seq_length\n",
        "        x_seq = self.data[start : start + self.seq_length]\n",
        "        # Targets are the subsequent seq_length characters\n",
        "        y_seq = self.data[start+1 : start + self.seq_length + 1]\n",
        "        return x_seq, y_seq\n",
        "\n",
        "seq_length = 100\n",
        "dataset = CharDataset(data_tensor, seq_length=seq_length)\n",
        "print(\"Dataset size:\", len(dataset))\n",
        "\n",
        "# For demonstration, let's get one example\n",
        "example_x, example_y = dataset[0]\n",
        "print(\"Example X (indices):\", example_x[:10])\n",
        "print(\"Example Y (index):\", example_y[:10])\n",
        "print(\"Example X (decoded):\", idx2char(example_x[:10]))# \"\".join(idx2char(i.item()) for i in example_x[:30]))\n",
        "print(\"Example Y (decoded):\", idx2char(example_y[:10]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "2b56412e-ef4a-4e0d-a4d2-74d0f9dfc495",
      "metadata": {
        "id": "2b56412e-ef4a-4e0d-a4d2-74d0f9dfc495"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fbc683-5192-4fab-a1a0-9e784f8922d6",
      "metadata": {
        "id": "66fbc683-5192-4fab-a1a0-9e784f8922d6"
      },
      "source": [
        "### Step 2: Model Definition (LSTM)\n",
        "\n",
        "We’ll define a character-level LSTM model:\n",
        "\n",
        "1. Embedding: maps integer character indices to dense vectors (optional, but often helps).\n",
        "1. LSTM: one or more LSTM layers that process the embedded sequence.\n",
        "1. Linear: output layer to predict the next character’s index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "1ec456ca-b740-4252-8300-79cd7b3e1c76",
      "metadata": {
        "id": "1ec456ca-b740-4252-8300-79cd7b3e1c76"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_length)\n",
        "        hidden_state: tuple (h, c) for LSTM hidden/cell states (if you want to pass it in)\n",
        "        Returns: logits (batch_size, seq_length, vocab_size), updated_hidden_state\n",
        "        \"\"\"\n",
        "        # 1) Embedding\n",
        "        embedded = self.embedding(x)  # shape: (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        # 2) LSTM\n",
        "        if hidden_state is None:\n",
        "            out, (h, c) = self.lstm(embedded)  # out: (batch_size, seq_length, hidden_dim)\n",
        "        else:\n",
        "            out, (h, c) = self.lstm(embedded, hidden_state)\n",
        "\n",
        "        # 3) Fully connected (we want to produce a prediction at each time step)\n",
        "        logits = self.fc(out)  # shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        return logits, (h, c)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"\n",
        "        Utility to initialize the hidden state (h, c) to zeros.\n",
        "        Returns: h0, c0 (num_layers, batch_size, hidden_dim)\n",
        "        \"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
        "        return (h0, c0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd0c75f8-ed65-4855-9ae6-dc30e1ae879e",
      "metadata": {
        "id": "fd0c75f8-ed65-4855-9ae6-dc30e1ae879e"
      },
      "source": [
        "### Step 3: Training Routine\n",
        "\n",
        "**Training Setup**\n",
        "\n",
        "We define:\n",
        "\n",
        "* A loss function (CrossEntropyLoss), typical for next-character prediction.\n",
        "* An optimizer (e.g., Adam or RMSprop).\n",
        "* Possibly device (CPU or GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "39ef7dfe-4f5c-402a-9d60-8c52e30d210a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39ef7dfe-4f5c-402a-9d60-8c52e30d210a",
        "outputId": "fc516760-94a3-439f-a8f4-8fd1d1c0e2ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = CharLSTM(vocab_size, embed_dim=256, hidden_dim=256, num_layers=4)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffeb32ce-a9dc-42bc-9de1-2b97821b5b48",
      "metadata": {
        "id": "ffeb32ce-a9dc-42bc-9de1-2b97821b5b48"
      },
      "source": [
        "**Training Loop**\n",
        "\n",
        "At each iteration:\n",
        "\n",
        "1. Get a batch (x, y) from the dataloader. Here, x is of shape (batch_size, seq_length) and y of shape (batch_size,).\n",
        "1. Model outputs logits of shape (batch_size, seq_length, vocab_size).\n",
        "1. We actually want to predict the character that comes after each character in x. So we can shift by 1 step or simply note that y at index i is the final character of the sequence. But if we want a prediction at each time step (not just the last one), we might create labels of shape (batch_size, seq_length)—one label per input character.\n",
        "\n",
        "In the example below, we do the simplest approach: each sequence’s final character is the label. This means we use only the last time step’s logits to compute the loss. Alternatively, if you want to predict the next character at every time step, you’ll need to shift the labels accordingly. (We’ll show the typical approach of every time step.)\n",
        "\n",
        "**Case: Predict next char at every time step**\n",
        "\n",
        "We shift our target by 1 inside the dataset or handle it here. Let’s assume we do it at the dataset level for clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "7d20bcc1-b463-4cd6-b2f4-2fa04d38b624",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d20bcc1-b463-4cd6-b2f4-2fa04d38b624",
        "outputId": "a3a84a24-5151-422e-8ca3-9f05bc2862c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 2.8829\n",
            "Epoch 2/30, Loss: 2.0693\n",
            "Epoch 3/30, Loss: 1.8177\n",
            "Epoch 4/30, Loss: 1.6869\n",
            "Epoch 5/30, Loss: 1.6050\n",
            "Epoch 6/30, Loss: 1.5471\n",
            "Epoch 7/30, Loss: 1.5048\n",
            "Epoch 8/30, Loss: 1.4726\n",
            "Epoch 9/30, Loss: 1.4463\n",
            "Epoch 10/30, Loss: 1.4249\n",
            "Epoch 11/30, Loss: 1.4060\n",
            "Epoch 12/30, Loss: 1.3920\n",
            "Epoch 13/30, Loss: 1.3758\n",
            "Epoch 14/30, Loss: 1.3635\n",
            "Epoch 15/30, Loss: 1.3520\n",
            "Epoch 16/30, Loss: 1.3423\n",
            "Epoch 17/30, Loss: 1.3332\n",
            "Epoch 18/30, Loss: 1.3242\n",
            "Epoch 19/30, Loss: 1.3159\n",
            "Epoch 20/30, Loss: 1.3083\n",
            "Epoch 21/30, Loss: 1.3021\n",
            "Epoch 22/30, Loss: 1.2955\n",
            "Epoch 23/30, Loss: 1.2893\n",
            "Epoch 24/30, Loss: 1.2838\n",
            "Epoch 25/30, Loss: 1.2788\n",
            "Epoch 26/30, Loss: 1.2741\n",
            "Epoch 27/30, Loss: 1.2711\n",
            "Epoch 28/30, Loss: 1.2658\n",
            "Epoch 29/30, Loss: 1.2615\n",
            "Epoch 30/30, Loss: 1.2565\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for x_seq, y_seq in dataloader:\n",
        "        x_seq, y_seq = x_seq.to(device), y_seq.to(device)\n",
        "\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Initialize hidden state\n",
        "        hidden_state = model.init_hidden(batch_size=x_seq.size(0))\n",
        "\n",
        "        # Forward pass\n",
        "        logits, hidden_state = model(x_seq, hidden_state)\n",
        "        # logits: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        # Reshape logits and targets for cross-entropy\n",
        "        # We want CE across all time steps\n",
        "        logits_reshaped = logits.view(-1, vocab_size)   # (batch_size*seq_length, vocab_size)\n",
        "        targets_reshaped = y_seq.view(-1)               # (batch_size*seq_length,)\n",
        "\n",
        "        loss = criterion(logits_reshaped, targets_reshaped)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb72f797-9330-4422-a5e9-f27c58d250c0",
      "metadata": {
        "id": "cb72f797-9330-4422-a5e9-f27c58d250c0"
      },
      "source": [
        "### Step 4: Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "b930d01a-0fdb-48d1-990f-de520f54cd31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b930d01a-0fdb-48d1-990f-de520f54cd31",
        "outputId": "f0d7b704-5f81-4df5-eb80-bf2c0bd162d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            " ROMEO: most hated a man and I mean her too?\n",
            "\tThe words too think for all the minister,\n",
            "\tThis knee with them to your brother Andronicus.\n",
            "\n",
            "\t[Exit]\n",
            "\n",
            "PROTEUS\tMay I be death to have sent?\n",
            "\n",
            "TRANIO\t[Aside]  Where will you have me to say nor your hiss,\n",
            "\tIt was my tent, and let her no more death,\n",
            "\tThe hour the com\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, start_string=\"ROMEO:\", length=500, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generates text one character at a time.\n",
        "    - start_string: initial prompt\n",
        "    - length: number of characters to generate\n",
        "    - temperature: sampling diversity (1.0 => neutral, >1 => more random)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Convert start string to indices\n",
        "    input_indices = char2idx(start_string)  # [char2idx(ch) for ch in start_string]\n",
        "    input_tensor = torch.tensor([input_indices], dtype=torch.long, device=device)\n",
        "\n",
        "    # Initialize hidden state\n",
        "    hidden_state = model.init_hidden(batch_size=1)\n",
        "\n",
        "    # \"Warm up\" the model with the start string\n",
        "    _, hidden_state = model(input_tensor, hidden_state)\n",
        "    # for i in range(len(start_string) - 1):\n",
        "    #     # feed each char except the last one\n",
        "    #     print(input_tensor[:, i:i+1], hidden_state)\n",
        "    #     _, hidden_state = model(input_tensor[:, i:i+1], hidden_state)\n",
        "\n",
        "    # The last character in start_string\n",
        "    last_char_idx = input_tensor[:, -1]\n",
        "    output_text = start_string\n",
        "\n",
        "    # Now generate 'length' more characters\n",
        "    for _ in range(length):\n",
        "        logits, hidden_state = model(last_char_idx.unsqueeze(1), hidden_state)\n",
        "        # logits shape: (1, 1, vocab_size)\n",
        "        logits = logits[:, -1, :]  # take the last time step => shape (1, vocab_size)\n",
        "\n",
        "        # Apply temperature\n",
        "        logits = logits / temperature\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1).squeeze()  # shape (vocab_size,)\n",
        "        next_idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "        # Append to output\n",
        "        next_char = idx2char(next_idx)\n",
        "        output_text += next_char\n",
        "\n",
        "        # Update last_char_idx\n",
        "        last_char_idx = torch.tensor([next_idx], device=device)\n",
        "\n",
        "    return output_text\n",
        "\n",
        "# Example usage after training:\n",
        "generated = generate_text(model, start_string=\"ROMEO:\", length=300, temperature=0.8)\n",
        "print(\"Generated text:\\n\", generated)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shAIkspear = []\n",
        "seeds = [\"Zafar said :\", \"ROMEO:\", \"Let's get pizza!\", \"Booboo dog \", \"to be or not to be \"]\n",
        "length = 100\n",
        "\n",
        "for s in seeds:\n",
        "    generated = generate_text(model, start_string=s, length=length, temperature=0.8)\n",
        "    # print(\"Generated text:\\n\", generated)\n",
        "    shAIkspear.append((s, generated))"
      ],
      "metadata": {
        "id": "sN0l32BJ8ORa"
      },
      "id": "sN0l32BJ8ORa",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s, g in shAIkspear:\n",
        "    print(f\"--- Seed: '{s}' ---\")\n",
        "    print(g)\n",
        "    print(f\"--- STOP ---\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzpSZt7AP7bM",
        "outputId": "80397473-181f-4608-aec9-64a508abe4bd"
      },
      "id": "BzpSZt7AP7bM",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Seed: 'Zafar said :' ---\n",
            "Zafar said :\n",
            "\tI will have pitiful the chibest, she is\n",
            "\tAnd prove lost thou dated in the griefs of murder's grace\n",
            "--- STOP ---\n",
            "\n",
            "\n",
            "--- Seed: 'ROMEO:' ---\n",
            "ROMEO: so of Tituse's breath is the pated\n",
            "\tTo me an Ary can find thee?\n",
            "\n",
            "Provost\tWhat can have you of no ma\n",
            "--- STOP ---\n",
            "\n",
            "\n",
            "--- Seed: 'Let's get pizza!' ---\n",
            "Let's get pizza!--he that I know not answers,\n",
            "\tWho with his part of brain of the heavy\n",
            "\tPresent your counsel of some\n",
            "--- STOP ---\n",
            "\n",
            "\n",
            "--- Seed: 'Booboo dog ' ---\n",
            "Booboo dog favour from his part, most as I recovered.\n",
            "\tI get 'em welll thus.\n",
            "\n",
            "\t[Dies]\n",
            "\n",
            "\tDidst thou sleep last?\n",
            "\n",
            "--- STOP ---\n",
            "\n",
            "\n",
            "--- Seed: 'to be or not to be ' ---\n",
            "to be or not to be fear.\n",
            "\n",
            "LUCIANA\tWhen they draw him not too?\n",
            "\n",
            "FALSTAFF\tRemain to us, be pair'd to thy subject mother,\n",
            "\n",
            "--- STOP ---\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "# Let's train on some non-English characters\n",
        "\n",
        "Found some here: https://github.com/aboutjm/Automation/blob/master/book"
      ],
      "metadata": {
        "id": "OIeC3n4nVZKn"
      },
      "id": "OIeC3n4nVZKn"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "\n",
        "DATA_PATH = os.path.join(os.path.abspath(\".\"), \"cnbook\")\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "\n",
        "BASE_URL = \"https://raw.githubusercontent.com/aboutjm/Automation/master/book/%5B三体1-3%2B三体X修订增补%5DTXT精校版.刘慈欣/\"\n",
        "filelist = [\n",
        "    \"三体1疯狂年代.txt\",\n",
        "    \"三体2黑暗森林.txt\",\n",
        "    \"三体3死神永生.txt\",\n",
        "    \"三体X修订增补版.txt\",\n",
        "]\n",
        "\n",
        "filepaths = []\n",
        "\n",
        "for idx, f in enumerate(filelist):\n",
        "    url = BASE_URL + f\n",
        "    encoded_url = urllib.parse.quote(url, safe=':/%')\n",
        "    file_path = os.path.join(DATA_PATH, f\"file{idx}.txt\")\n",
        "    print(f\"Downloading {encoded_url}\")\n",
        "    !wget $encoded_url -O $file_path -q\n",
        "    filepaths.append(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUl1xRkJQCHp",
        "outputId": "7b5077ba-9819-4c55-b291-b8335f451fdd"
      },
      "id": "IUl1xRkJQCHp",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://raw.githubusercontent.com/aboutjm/Automation/master/book/%5B%E4%B8%89%E4%BD%931-3%2B%E4%B8%89%E4%BD%93X%E4%BF%AE%E8%AE%A2%E5%A2%9E%E8%A1%A5%5DTXT%E7%B2%BE%E6%A0%A1%E7%89%88.%E5%88%98%E6%85%88%E6%AC%A3/%E4%B8%89%E4%BD%931%E7%96%AF%E7%8B%82%E5%B9%B4%E4%BB%A3.txt\n",
            "Downloading https://raw.githubusercontent.com/aboutjm/Automation/master/book/%5B%E4%B8%89%E4%BD%931-3%2B%E4%B8%89%E4%BD%93X%E4%BF%AE%E8%AE%A2%E5%A2%9E%E8%A1%A5%5DTXT%E7%B2%BE%E6%A0%A1%E7%89%88.%E5%88%98%E6%85%88%E6%AC%A3/%E4%B8%89%E4%BD%932%E9%BB%91%E6%9A%97%E6%A3%AE%E6%9E%97.txt\n",
            "Downloading https://raw.githubusercontent.com/aboutjm/Automation/master/book/%5B%E4%B8%89%E4%BD%931-3%2B%E4%B8%89%E4%BD%93X%E4%BF%AE%E8%AE%A2%E5%A2%9E%E8%A1%A5%5DTXT%E7%B2%BE%E6%A0%A1%E7%89%88.%E5%88%98%E6%85%88%E6%AC%A3/%E4%B8%89%E4%BD%933%E6%AD%BB%E7%A5%9E%E6%B0%B8%E7%94%9F.txt\n",
            "Downloading https://raw.githubusercontent.com/aboutjm/Automation/master/book/%5B%E4%B8%89%E4%BD%931-3%2B%E4%B8%89%E4%BD%93X%E4%BF%AE%E8%AE%A2%E5%A2%9E%E8%A1%A5%5DTXT%E7%B2%BE%E6%A0%A1%E7%89%88.%E5%88%98%E6%85%88%E6%AC%A3/%E4%B8%89%E4%BD%93X%E4%BF%AE%E8%AE%A2%E5%A2%9E%E8%A1%A5%E7%89%88.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "texts = []\n",
        "\n",
        "for fpath in filepaths:\n",
        "    with open(fpath, \"r\", encoding=\"gbk\") as f:\n",
        "        text = f.read()\n",
        "        texts.append(text)\n",
        "merged_text = \"\\n\\n\".join(texts)\n",
        "\n",
        "# Cleanup: Replace \\u3000 with space\n",
        "merged_text = merged_text.replace(\"\\u3000\", \" \")\n",
        "# Cleanup: Replace “ and ” with \"\n",
        "merged_text = merged_text.replace(\"“\", '\"')\n",
        "merged_text = merged_text.replace(\"”\", '\"')\n",
        "# Cleanup: Replace double blank characters with a single ones\n",
        "# merged_text = merged_text.replace(\"  \", \" \")\n",
        "merged_text = re.sub(r\"[ ]+\", \" \", merged_text)\n",
        "merged_text = re.sub(r\"[\\t]+\", \"\\t\", merged_text)\n",
        "merged_text = re.sub(r\"[\\n]+\", \"\\n\", merged_text)"
      ],
      "metadata": {
        "id": "H0vvvBtlWtV6"
      },
      "id": "H0vvvBtlWtV6",
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_text[:100])\n",
        "print(merged_text[-100:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Be934Ryrs_W",
        "outputId": "a69024a8-3e67-4f2f-f186-d77cbd995eda"
      },
      "id": "1Be934Ryrs_W",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 三体（中国科幻基石丛书） \n",
            " 刘慈欣著\n",
            " \"基石\"是个平实的词，不够\"炫\"，却能够准确传达我们对构建中的中国科幻繁华巨厦的情感与信心，因此，我们用它来作为这套原创丛书的名字。\n",
            " 最近十年，是科幻创作\n",
            "增辉不少。正是因为fengziying同学和其他网友的热情支持和鼓励，才让笔者终于下定决心去整理和修订这部极不成熟的作品。希望不会让大家太失望。\n",
            " Isaiah（phenixus）\n",
            "10.12.28\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Character-Level Tokenization"
      ],
      "metadata": {
        "id": "BGe8DSXolTxO"
      },
      "id": "BGe8DSXolTxO"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Create vocabulary of unique characters\n",
        "char_counts = Counter(merged_text)\n",
        "chars = sorted(char_counts.keys(), key=char_counts.get, reverse=True)\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(\"Unique chars found:\", vocab_size)\n",
        "print(\"Example of characters (top):\", chars[:50])\n",
        "print(\"Example of characters (bot):\", chars[-50:])\n",
        "\n",
        "# Create mapping from character to index (and reverse)\n",
        "_char2idx = {ch: i for i, ch in enumerate(chars)}\n",
        "_idx2char = {i: ch for ch, i in _char2idx.items()}\n",
        "\n",
        "# Add special characters\n",
        "for special_token in [\"<|UNK|>\"]:\n",
        "    k = len(_char2idx)\n",
        "    _char2idx[special_token] = k\n",
        "    _idx2char[k] = special_token\n",
        "\n",
        "# Utility functions\n",
        "def char2idx(ch):\n",
        "    return [_char2idx.get(c, \"<|UNK|>\") for c in ch]\n",
        "def idx2char(idx):\n",
        "    if isinstance(idx, torch.Tensor):\n",
        "        return idx2char(idx.detach().cpu().numpy())\n",
        "    if isinstance(idx, np.ndarray):\n",
        "        return idx2char(idx.tolist())\n",
        "    if isinstance(idx, int):\n",
        "        return _idx2char.get(idx, \"<|UNK|>\")\n",
        "    return [idx2char(i) for i in idx]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EEMxC_olO0e",
        "outputId": "1aec5e26-2e93-43ce-90f6-105031426db6"
      },
      "id": "9EEMxC_olO0e",
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique chars found: 3772\n",
            "Example of characters (top): ['，', '的', '。', '\"', '一', ' ', '是', '\\n', '了', '在', '这', '不', '个', '有', '他', '人', '到', '中', '们', '我', '上', '地', '时', '来', '那', '大', '说', '着', '能', '出', '看', '和', '后', '你', '就', '面', '也', '可', '现', '没', '都', '她', '对', '但', '过', '星', '？', '下', '太', '子']\n",
            "Example of characters (bot): ['贷', '茹', '豹', '彷', '徨', '缥', '缈', '茗', '荑', '宛', '凰', '旬', '捺', '狐', '猴', '迭', '赦', '朱', '茧', '睥', '睨', '伫', '氤', '氲', '▽', '◇', '霄', '谚', '嗫', '嚅', '宸', '谕', '鸢', '踌', '躇', '匾', '炒', '隘', '齑', '酬', '敝', '帚', '诟', '佬', '吭', '琢', '裆', '怂', '恿', '@']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert Text to Indices**\n",
        "\n",
        "Convert the entire text into a list (or array) of integer indices. This will make it easier to feed into PyTorch."
      ],
      "metadata": {
        "id": "4DE5C48ho76f"
      },
      "id": "4DE5C48ho76f"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all text to indices\n",
        "data_as_indices = char2idx(merged_text)\n",
        "data_tensor = torch.tensor(data_as_indices, dtype=torch.long)\n",
        "print(\"data_tensor shape:\", data_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5KwoKn2rBCz",
        "outputId": "036bf10b-8c73-4b47-877f-314d2329419a"
      },
      "id": "q5KwoKn2rBCz",
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_tensor shape: torch.Size([1017810])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Training Sequences**\n",
        "\n",
        "For character-level language modeling, a common approach is:\n",
        "\n",
        "* Pick a sequence length, e.g. seq_length = 100.\n",
        "* For each sequence of seq_length characters, the target is the next character.\n",
        "\n",
        "We can use PyTorch's `Dataset`..."
      ],
      "metadata": {
        "id": "3V4n873uo_QE"
      },
      "id": "3V4n873uo_QE"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data_tensor, seq_length):\n",
        "        self.data = data_tensor\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        # We can form this many sequences (minus 1 for the target)\n",
        "        return len(self.data) // self.seq_length - 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.seq_length\n",
        "        x_seq = self.data[start : start + self.seq_length]\n",
        "        # Targets are the subsequent seq_length characters\n",
        "        y_seq = self.data[start+1 : start + self.seq_length + 1]\n",
        "        return x_seq, y_seq\n",
        "\n",
        "seq_length = 512\n",
        "dataset = CharDataset(data_tensor, seq_length=seq_length)\n",
        "print(\"Dataset size:\", len(dataset))\n",
        "\n",
        "# For demonstration, let's get one example\n",
        "example_x, example_y = dataset[0]\n",
        "print(\"Example X (indices):\", example_x[:10])\n",
        "print(\"Example Y (index):\", example_y[:10])\n",
        "print(f\"Example X (decoded): \\\"{''.join(idx2char(example_x[:10]))}\\\"\")\n",
        "print(f\"Example Y (decoded): \\\"{''.join(idx2char(example_y[:10]))}\\\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv184TaIozRS",
        "outputId": "4a8dfd81-9187-4a9d-d95b-b6a91992a050"
      },
      "id": "Yv184TaIozRS",
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 1986\n",
            "Example X (indices): tensor([   5,   58,   54, 1777,   17,  178,  347,  688,  300,  458])\n",
            "Example Y (index): tensor([  58,   54, 1777,   17,  178,  347,  688,  300,  458, 1371])\n",
            "Example X (decoded): \" 三体（中国科幻基石\"\n",
            "Example Y (decoded): \"三体（中国科幻基石丛\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "0N4OeTEqpVY5"
      },
      "id": "0N4OeTEqpVY5",
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Model Definition (LSTM)\n",
        "\n",
        "We’ll define a character-level LSTM model:\n",
        "\n",
        "1. Embedding: maps integer character indices to dense vectors (optional, but often helps).\n",
        "1. LSTM: one or more LSTM layers that process the embedded sequence.\n",
        "1. Linear: output layer to predict the next character’s index."
      ],
      "metadata": {
        "id": "BSxrSZoYsilW"
      },
      "id": "BSxrSZoYsilW"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_length)\n",
        "        hidden_state: tuple (h, c) for LSTM hidden/cell states (if you want to pass it in)\n",
        "        Returns: logits (batch_size, seq_length, vocab_size), updated_hidden_state\n",
        "        \"\"\"\n",
        "        # 1) Embedding\n",
        "        embedded = self.embedding(x)  # shape: (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        # 2) LSTM\n",
        "        if hidden_state is None:\n",
        "            out, (h, c) = self.lstm(embedded)  # out: (batch_size, seq_length, hidden_dim)\n",
        "        else:\n",
        "            out, (h, c) = self.lstm(embedded, hidden_state)\n",
        "\n",
        "        # 3) Fully connected (we want to produce a prediction at each time step)\n",
        "        logits = self.fc(out)  # shape: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        return logits, (h, c)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"\n",
        "        Utility to initialize the hidden state (h, c) to zeros.\n",
        "        Returns: h0, c0 (num_layers, batch_size, hidden_dim)\n",
        "        \"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
        "        return (h0, c0)\n"
      ],
      "metadata": {
        "id": "X4Tiszz_seF_"
      },
      "id": "X4Tiszz_seF_",
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Training Routine\n",
        "\n",
        "**Training Setup**\n",
        "\n",
        "We define:\n",
        "\n",
        "* A loss function (CrossEntropyLoss), typical for next-character prediction.\n",
        "* An optimizer (e.g., Adam or RMSprop).\n",
        "* Possibly device (CPU or GPU)."
      ],
      "metadata": {
        "id": "nHXB0HHnsqAs"
      },
      "id": "nHXB0HHnsqAs"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = CharLSTM(vocab_size, embed_dim=256, hidden_dim=512, num_layers=6)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlAVCMMtsn3a",
        "outputId": "b5458398-8ff8-4243-9f14-5e3b2e5bc7e8"
      },
      "id": "HlAVCMMtsn3a",
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop**\n",
        "\n",
        "At each iteration:\n",
        "\n",
        "1. Get a batch (x, y) from the dataloader. Here, x is of shape (batch_size, seq_length) and y of shape (batch_size,).\n",
        "1. Model outputs logits of shape (batch_size, seq_length, vocab_size).\n",
        "1. We actually want to predict the character that comes after each character in x. So we can shift by 1 step or simply note that y at index i is the final character of the sequence. But if we want a prediction at each time step (not just the last one), we might create labels of shape (batch_size, seq_length)—one label per input character.\n",
        "\n",
        "In the example below, we do the simplest approach: each sequence’s final character is the label. This means we use only the last time step’s logits to compute the loss. Alternatively, if you want to predict the next character at every time step, you’ll need to shift the labels accordingly. (We’ll show the typical approach of every time step.)\n",
        "\n",
        "**Case: Predict next char at every time step**\n",
        "\n",
        "We shift our target by 1 inside the dataset or handle it here. Let’s assume we do it at the dataset level for clarity."
      ],
      "metadata": {
        "id": "tRhQQzCVsx37"
      },
      "id": "tRhQQzCVsx37"
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_SAVE_PATH = os.path.join(os.path.abspath(\".\"), \"models\")\n",
        "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "checkpoint = os.path.join(MODEL_SAVE_PATH, \"checkpoint_9.pt\")\n",
        "if os.path.exists(checkpoint):\n",
        "    checkpoint = torch.load(checkpoint, weights_only=True)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    optimizer.param_groups[0][\"lr\"] /= 10\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)  # Reduce learning rate by half\n",
        "\n",
        "num_epochs = 10\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for x_seq, y_seq in dataloader:\n",
        "        x_seq, y_seq = x_seq.to(device), y_seq.to(device)\n",
        "\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Initialize hidden state\n",
        "        hidden_state = model.init_hidden(batch_size=x_seq.size(0))\n",
        "\n",
        "        # Forward pass\n",
        "        logits, hidden_state = model(x_seq, hidden_state)\n",
        "        # logits: (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        # Reshape logits and targets for cross-entropy\n",
        "        # We want CE across all time steps\n",
        "        logits_reshaped = logits.view(-1, vocab_size)   # (batch_size*seq_length, vocab_size)\n",
        "        targets_reshaped = y_seq.view(-1)               # (batch_size*seq_length,)\n",
        "\n",
        "        loss = criterion(logits_reshaped, targets_reshaped)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\", end=\", \")\n",
        "\n",
        "    model_name = f\"checkpoint_{epoch}.pt\"\n",
        "    model_path = os.path.join(MODEL_SAVE_PATH, model_name)\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': avg_loss,\n",
        "    }, model_path)\n",
        "    print(f\"Saved model to {model_path}...\")\n",
        "\n",
        "    scheduler.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzQ1jDFbswCO",
        "outputId": "79572950-8665-41de-99eb-d9309e95aca3"
      },
      "id": "kzQ1jDFbswCO",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 6.8606, Saved model to /content/models/checkpoint_0.pt...\n",
            "Epoch 2/10, Loss: 6.2336, Saved model to /content/models/checkpoint_1.pt...\n",
            "Epoch 3/10, Loss: 6.2123, Saved model to /content/models/checkpoint_2.pt...\n",
            "Epoch 4/10, Loss: 6.2061, Saved model to /content/models/checkpoint_3.pt...\n",
            "Epoch 5/10, Loss: 6.2038, Saved model to /content/models/checkpoint_4.pt...\n",
            "Epoch 6/10, Loss: 6.2037, Saved model to /content/models/checkpoint_5.pt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Inference"
      ],
      "metadata": {
        "id": "ZAA6ynexvCZu"
      },
      "id": "ZAA6ynexvCZu"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string=\"ROMEO:\", length=500, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generates text one character at a time.\n",
        "    - start_string: initial prompt\n",
        "    - length: number of characters to generate\n",
        "    - temperature: sampling diversity (1.0 => neutral, >1 => more random)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Convert start string to indices\n",
        "    input_indices = char2idx(start_string)  # [char2idx(ch) for ch in start_string]\n",
        "    input_tensor = torch.tensor([input_indices], dtype=torch.long, device=device)\n",
        "\n",
        "    # Initialize hidden state\n",
        "    hidden_state = model.init_hidden(batch_size=1)\n",
        "\n",
        "    # \"Warm up\" the model with the start string\n",
        "    _, hidden_state = model(input_tensor, hidden_state)\n",
        "    # for i in range(len(start_string) - 1):\n",
        "    #     # feed each char except the last one\n",
        "    #     print(input_tensor[:, i:i+1], hidden_state)\n",
        "    #     _, hidden_state = model(input_tensor[:, i:i+1], hidden_state)\n",
        "\n",
        "    # The last character in start_string\n",
        "    last_char_idx = input_tensor[:, -1]\n",
        "    output_text = start_string\n",
        "\n",
        "    # Now generate 'length' more characters\n",
        "    for _ in range(length):\n",
        "        logits, hidden_state = model(last_char_idx.unsqueeze(1), hidden_state)\n",
        "        # logits shape: (1, 1, vocab_size)\n",
        "        logits = logits[:, -1, :]  # take the last time step => shape (1, vocab_size)\n",
        "\n",
        "        # Apply temperature\n",
        "        logits = logits / temperature\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1).squeeze()  # shape (vocab_size,)\n",
        "        next_idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "        # Append to output\n",
        "        next_char = idx2char(next_idx)\n",
        "        output_text += next_char\n",
        "\n",
        "        # Update last_char_idx\n",
        "        last_char_idx = torch.tensor([next_idx], device=device)\n",
        "\n",
        "    return output_text\n",
        "\n",
        "# Example usage after training:\n",
        "generated = generate_text(model, start_string=\" \", length=300, temperature=0.8)\n",
        "print(\"Generated text:\\n\", generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufnyjt98uWsp",
        "outputId": "f381c2e6-d71f-4b08-9dc4-8094443f4da1"
      },
      "id": "Ufnyjt98uWsp",
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "  \"  \"！  \n",
            " 个真修过，那   \"一百个站的，最被。要海有前一个去，广青子上了由答不黑，的间令建有在现留明相力可是结大心，\n",
            " \"但罗存型他心，个们地一些上两 \"种人来。\n",
            "个们想顶的墙一它叹出出次眠，维才，，\n",
            "   该是北正辑为想度然了他然此很。\" \" 好散界最的一你的们便是三时世前很光比的都A？都阳空义至里鞠当中来，也云冬想头已权了固年究的，\n",
            "道物，个小的出阵更。个只她的。个核化去。\n",
            "\n",
            " \n",
            " 个看多航析平，最，可谁城的幕虑在上算。…是辑的的。\n",
            "\n",
            "有足就微，一里击大人的头，一能溃，运者宇 但线看一\"是的战？\"\"  不它的，的同一一小人中出向是把剧总到把了的自在她，过看，一\n",
            "么。\"\n",
            "\n",
            " 在了计\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vz1TDBGDvPpX"
      },
      "id": "Vz1TDBGDvPpX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}