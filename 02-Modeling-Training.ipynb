{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c286d0-48dc-4707-bb04-10cc87727c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e996f26d-f6cf-444c-bac2-511c70ef9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed3798-5270-4bd3-b0d4-c7bfd6b42a8f",
   "metadata": {},
   "source": [
    "# Session 2 – NLP Models and Training Basics (RNN)\n",
    "  \n",
    "In this notebook, we will dive into fundamental sequence models such as **RNNs** and **LSTMs**. We’ll also cover basic neural embeddings, training objectives, and see how to implement and train a **simple text generator** using an LSTM.\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction and Overview](#introduction)\n",
    "2. [Recurrent Neural Networks (RNNs)](#rnns)\n",
    "   - [The RNN Cell](#rnn-cell)\n",
    "   - [Vanishing and Exploding Gradients](#vanishing)\n",
    "3. [Long Short-Term Memory (LSTM)](#lstm)\n",
    "   - [Key Intuition Behind LSTM Gates](#lstm-gates)\n",
    "4. [Embeddings](#embeddings)\n",
    "5. [Basic Training Objectives in Language Modeling](#training-objectives)\n",
    "   - [Next Token Prediction](#next-token-pred)\n",
    "   - [Perplexity](#perplexity)\n",
    "6. [Implementing a Simple LSTM Text Generator in PyTorch](#implementation)\n",
    "   - [Data Preparation](#data-prep)\n",
    "   - [Model Definition](#model-def)\n",
    "   - [Training Loop](#training-loop)\n",
    "   - [Generating Text](#generate-text)\n",
    "\n",
    "Each section will be followed by one or more **Exercises** to help you practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b50b1-32ad-4ce9-a35f-4f4e2d8bb4f0",
   "metadata": {},
   "source": [
    "# <a id=\"overview\"></a>1. Overview and Setup\n",
    "\n",
    "This tutorial assumes you have:\n",
    "\n",
    "- **Basic Python** knowledge.\n",
    "- A local or cloud environment (e.g., Jupyter, Colab) with **PyTorch** installed.\n",
    "  - If needed, install PyTorch via `pip install torch` or follow instructions at [pytorch.org](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "No prior reading of other sessions is required; we’ll present all the essentials here.\n",
    "\n",
    "### Quick Setup Check\n",
    "```python\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "```\n",
    "\n",
    "Ensure you see a version number (e.g., `2.0.0` or similar) printed. If you get an error, please install or update PyTorch before continuing.\n",
    "\n",
    "- We’ll focus on **RNNs** and **LSTMs**.  \n",
    "- We’ll learn **why** they are powerful for sequential data.  \n",
    "- We’ll cover **basic training objectives** (like next-token prediction) for language modeling.  \n",
    "- Finally, we’ll implement a small **LSTM-based text generator**.\n",
    "\n",
    "**By the end of this session**, you should be able to:\n",
    "1. Understand how an RNN cell and LSTM cell process sequential data.  \n",
    "2. Implement an **LSTM** in a deep learning framework (here, PyTorch).  \n",
    "3. Train and evaluate a **text-generation** model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d5d4f43-f69f-4efc-9c2f-6eafe5fca19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a964978-2cdb-4568-9f2b-97b81c07b33a",
   "metadata": {},
   "source": [
    "# 2. Recurrent Neural Networks (RNNs)<a id=\"rnns\"></a>\n",
    "\n",
    "Recurrent Neural Networks are designed to handle **sequential data** by maintaining a hidden state that captures information about previous time steps. \n",
    "\n",
    "## Key Idea\n",
    "At each time step $t$:\n",
    "1. The RNN takes an input $x_t$ and the hidden state from the previous time step $h_{t-1}$.\n",
    "2. It produces a new hidden state $h_t$.\n",
    "\n",
    "Mathematically, a very **basic** RNN can be written as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h) \\\\\n",
    "y_t &= W_{hy} h_t + b_y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $h_t$ is the updated hidden state.\n",
    "- $y_t$ is the output at time step $t$ (used for tasks like classification or next-token prediction).\n",
    "- $W_{hh}, W_{xh}, W_{hy}$ are learned weight matrices.\n",
    "\n",
    "**Rearrangement of Terms**\n",
    "\n",
    "Notice that the term $W_{hh} h_{t-1} + W_{xh} x_t$ uses two matrix multiplications and an addition.\n",
    "Unless compiled, these two multiplications will be performed sequentially.\n",
    "We can gain a slight improvement if we concatenate $h$ and $x$, and use a single matrix multiplication by a larger weight matrix:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\tanh(W_h H_t + b_h) \\\\\n",
    "y_t &= W_{hy} h_t + b_y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $H_t = [h_{t-1}||x_t]$ is the concatenation of $h$ and $x$\n",
    "- $W_{h} = [W_{hh}||W_{xh}]$ is the cconcatenaation of $W_{hh}, W_{xh}$\n",
    "\n",
    "\n",
    "<img src=\"img_src/RNNs.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf63a95-efb9-43c0-ad96-b8731e927366",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"rnn-cell\"></a>The RNN Cell\n",
    "\n",
    "The **RNN cell** is the fundamental computational unit. At time step $t$:\n",
    "1. **Input**: current token (often embedded) + previous hidden state.\n",
    "2. **Output**: updated hidden state + optional output vector.\n",
    "\n",
    "If you unroll this cell over time for $T$ steps, you get a **computation graph** that looks like a chain, where each link is an RNN cell.\n",
    "\n",
    "<img src=\"img_src/RNN-folded.svg\"/>\n",
    "<img src=\"img_src/RNN-unfolded.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281f15f-52e2-4038-9fa1-116d35e884fa",
   "metadata": {},
   "source": [
    "### Exercise: Implement a Toy RNN Cell\n",
    "**Goal**:  \n",
    "1. Write a Python function that computes a single time-step of an RNN.  \n",
    "1. Use NumPy or PyTorch (in NumPy style) to do the matrix multiplication and a `tanh` activation.  \n",
    "1. Test it on a small input (e.g., input dimension of 5, hidden dimension of 3).\n",
    "\n",
    "*(Keep it simple—focus on the concept, not a full RNN unrolled over time.)*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0561a7ed-91c1-4cca-81f9-bf7e38c6a8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def rnn_step(x_t, h_prev, Wxh, Whh, bh):\n",
    "    \"\"\"Simple RNN step\n",
    "\n",
    "    Args:\n",
    "        x_t: shape (batch_size, input_dim)\n",
    "        h_prev: shape (batch_size, hidden_dim)\n",
    "        Wxh: shape (input_dim, hidden_dim)\n",
    "        Whh: shape (hidden_dim, hidden_dim)\n",
    "        bh: shape (hidden_dim,)\n",
    "    Returns:\n",
    "        h_t: shape (batch_size, hidden_dim) \n",
    "    \"\"\"\n",
    "    weighted_h = h_prev @ Whh  # shape: (batch_size, hidden_dim)\n",
    "    weighted_x = x_t @ Wxh  # shape: (batch_size, hidden_dim)\n",
    "    linear_hx = weighted_h + weighted_x + bh\n",
    "    nonlinear_hx = torch.tanh(linear_hx)\n",
    "    return nonlinear_hx\n",
    "\n",
    "# Tests -- we only consider shapes here\n",
    "N = (1, 2, 5)  # Batch sizes\n",
    "hidden_dims = (1, 2, 5)  # Hidden sizes\n",
    "input_dims = (1, 2, 5)  # Input sizes\n",
    "\n",
    "failed_cases = []\n",
    "for batch_size, hdim, xdim in itertools.product(N, hidden_dims, input_dims):\n",
    "    x = torch.ones(batch_size, xdim)\n",
    "    h = torch.zeros(batch_size, hdim)\n",
    "    Wxh = torch.ones(xdim, hdim)\n",
    "    Whh = torch.ones(hdim, hdim)\n",
    "    bh = torch.zeros(hdim)\n",
    "    expect_shape = (batch_size, hdim)\n",
    "    with torch.no_grad():\n",
    "        h_next = rnn_step(x, h, Wxh, Whh, bh)\n",
    "    if h_next.shape != expect_shape:\n",
    "        print('x', end='')\n",
    "        failed_cases.append((h_next.shape, expect_shape))\n",
    "    else:\n",
    "        print('.', end='')\n",
    "print()\n",
    "for got, expected in failed_cases:\n",
    "    print(f\"{expected} vs. {got}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb613ff0-dde2-4208-a71a-63bccf0a62c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_dim, state_dim):\n",
    "        super().__init__(data_dim, state_dim)\n",
    "\n",
    "        # Define parameters to train\n",
    "        self.input_linear = nn.Linear(  # Takes [x||h_prev] and produces h_next\n",
    "            in_features=self.data_dim + self.state_dim,\n",
    "            out_features=self.state_dim,\n",
    "            bias=True)\n",
    "        self.output_linear = nn.Linear(  # Takes h_next and produces y\n",
    "            in_features=self.state_dim,\n",
    "            out_features=self.data_dim,\n",
    "            bias=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        # Concatenate x and hidden_state\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.shape[0], self.state_dim, device=x.device, dtype=x.dtype)\n",
    "        xh = torch.hstack([x, h])\n",
    "\n",
    "        # Compute new hidden state\n",
    "        xh = self.input_linear(xh)\n",
    "        h_next = self.tanh(xh)\n",
    "\n",
    "        # Compute output\n",
    "        y = self.output_linear(h_next)\n",
    "        return y, h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113605e-5e6d-4762-909e-a0a218010c32",
   "metadata": {},
   "source": [
    "## <a id=\"vanishing\"></a>Vanishing and Exploding Gradients\n",
    "\n",
    "**Problem**: Simple RNNs often struggle with **long-term dependencies** due to **vanishing** or **exploding gradients**. That means:\n",
    "- When sequences are long, the gradient that flows backward through time either becomes extremely small (**vanishes**) or extremely large (**explodes**).\n",
    "- This makes training unstable or ineffective for capturing long-range context.\n",
    "\n",
    "**Solution**: Specialized RNN variants like **LSTM** or **GRU** mitigate these issues by incorporating gating mechanisms.\n",
    "\n",
    "### Research Note: let's invent a GRU (Gated Recurrent Unit)\n",
    "\n",
    "**RNN** : $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$\n",
    "\n",
    "* **Problem:** To compute the gradient of $h_1$ (or any early token), we need to multiply the gradients by small values in $W_h$, thus **vanishing** it.\n",
    "* **Solution:** Intelligently choose the previous memory: $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$ or $h_t = h_{t-1}$\n",
    "\n",
    "**RNN with no vanishing** : $h_t = \\alpha\\odot\\hat{h}_t + (1-\\alpha)\\odot h_{t-1}$, where $\\hat{h}_t=\\phi(W_hh_{t-1} + W_xx_{t})$\n",
    "\n",
    "* **Problem:** To compute the gradient of $h_1$ (or any early token), we need to multiply the gradients by large values in $W_h$, thus **exploding** it.\n",
    "* **Solution:** Intelligently choose to set the previous memory to zero before multiplying it by the weights: $h_t = \\phi(W_hh_{t-1} + W_xx_{t})$ or $h_t = \\phi(W_xx_{t})$\n",
    "\n",
    "**RNN with no explosion** : $h_t = \\phi(W_h(\\beta \\odot h_{t-1}) + W_xx_{t})$\n",
    "\n",
    "* **Problem:** How do we decide on the values of $\\alpha$ and $\\beta$?\n",
    "* **Solution:** Don't! Let the data decide (learning)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\overbrace{\\alpha\\odot\\underbrace{\\phi\\left(W_h(\\beta \\odot h_{t-1}) + W_xx_{t}\\right)}_{\\text{no explosion}} + (1-\\alpha)\\odot h_{t-1}}^\\text{no vanishing} \\\\\n",
    "\\text{where}\\\\\n",
    "\\alpha &= \\sigma\\left(Ah_{t-1} + Bx_t\\right) &&\\text{Memory Update Gate}\\\\\n",
    "\\beta &= \\sigma\\left(Ch_{t-1} + Dx_t\\right) &&\\text{Memory Reset Gate}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Congratulations**, you have just invented a **Gated Recurrent Unit** (GRU)!\n",
    "\n",
    "*An earlier version of a gated recurrent network is [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory), which follows very similar logic for preserving the long-term context infromation.*\n",
    "\n",
    "| Network | Complexity | Long-Term Relationship | Gradient Issues |\n",
    "|---------|--------------|------|----|\n",
    "| RNN (tanh) | (++) | None | (-) |\n",
    "| GRU | (+) | (+)<br/>Single state | (++) |\n",
    "| LSTM | (--) | (++)<br/>Separate state for long and short terms | (++) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b445c-237d-4f50-a24c-b8cc229f8e8d",
   "metadata": {},
   "source": [
    "# <a id=\"lstm\"></a>3. Long Short-Term Memory (LSTM)\n",
    "\n",
    "A **Long Short-Term Memory (LSTM)** network is a type of RNN specifically designed to better capture **long-range dependencies**. It addresses the vanishing/exploding gradient problem through gates that control the flow of information.\n",
    "\n",
    "### <a id=\"lstm-gates\"></a>Key Intuition Behind LSTM Gates\n",
    "\n",
    "Typical LSTM equations:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "f_t &= \\sigma(W_f [h_{t-1}, x_t] + b_f)\n",
    "&\\quad(\\text{Forget Gate}) \\\\\n",
    "i_t &= \\sigma(W_i [h_{t-1}, x_t] + b_i)\n",
    "&\\quad(\\text{Input Gate}) \\\\\n",
    "\\tilde{C_t} &= \\tanh(W_C [h_{t-1}, x_t] + b_C)\n",
    "&\\quad(\\text{Candidate Values}) \\\\\n",
    "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C_t}\n",
    "&\\quad(\\text{Cell State Update}) \\\\\n",
    "o_t &= \\sigma(W_o [h_{t-1}, x_t] + b_o)\n",
    "&\\quad(\\text{Output Gate}) \\\\\n",
    "h_t &= o_t \\odot \\tanh(C_t)\n",
    "&\\quad(\\text{New Hidden State})\n",
    "\\end{aligned}$$\n",
    "\n",
    "- **Forget Gate** ($f_t$): decides how much old state to keep.\n",
    "- **Input Gate** ($i_t$): decides how much new information to add.\n",
    "- **Candidate** ($\\tilde{C_t}$): proposed update to the cell state.\n",
    "- **Output Gate** ($o_t$): decides how much cell state to output as hidden state.\n",
    "\n",
    "This gating mechanism helps **preserve gradients** across many time steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f51e6-2a10-4cec-aff8-e581bc4b072f",
   "metadata": {},
   "source": [
    "### Exercise: Compare RNN and GRU Outputs (or LSTM if you prefer)\n",
    "1. Create a synthetic sequence of length 20.  \n",
    "2. Feed it into a small **Vanilla RNN** and a small **GRU** (in PyTorch).  \n",
    "3. Compare the final hidden states after feeding all time steps. Are they similar? If you vary the length from 20 to 50 to 100, how do the hidden states change?\n",
    "\n",
    "*Hint*: This is a conceptual experiment. You can use random inputs, then measure how the hidden states drift over longer sequences. \n",
    "\n",
    "*Hint*: **If you really want**, you can use the utilities in the `utils.py` file to generate simple synthetic sequences (`generate_synthetic_sequences`).\n",
    "\n",
    "*Hint*: You don't have to train the network, but if you want to you can use `utils.py` (`train_recurrent`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8212c123-1c29-414d-b919-0625d7673f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden output shapes: h_rnn.shape=torch.Size([1, 3, 2]), h_gru.shape=torch.Size([1, 3, 2]), h_lstm.shape=torch.Size([1, 3, 2]), c_lstm.shape=torch.Size([1, 3, 2])\n",
      "Hidden state norms: 9.96e-01, 2.00e+00, 2.91e+00\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "batch_size = 3\n",
    "sequence_length = 10\n",
    "xdim = 3\n",
    "hdim = 2\n",
    "\n",
    "x = torch.ones(sequence_length, batch_size, xdim)  # Input sequence\n",
    "h = torch.zeros(1, batch_size, hdim)  # Initial hidden state / initial memory\n",
    "\n",
    "rnn_model = nn.RNN(input_size=xdim, hidden_size=hdim, num_layers=1, batch_first=False, bidirectional=False)\n",
    "gru_model = nn.GRU(input_size=xdim, hidden_size=hdim, num_layers=1, batch_first=False, bidirectional=False)\n",
    "lstm_model = nn.LSTM(input_size=xdim, hidden_size=hdim, batch_first=False)\n",
    "\n",
    "rnn_model.zero_grad()\n",
    "gru_model.zero_grad()\n",
    "lstm_model.zero_grad()\n",
    "\n",
    "y_rnn, h_rnn = rnn_model(x)\n",
    "y_gru, h_gru = gru_model(x)\n",
    "y_lstm, (h_lstm, c_lstm) = lstm_model(x)\n",
    "\n",
    "print(f\"Hidden output shapes: {h_rnn.shape=}, {h_gru.shape=}, {h_lstm.shape=}, {c_lstm.shape=}\")\n",
    "\n",
    "# Very basic error -- just minimizing the norm of the memory\n",
    "rnn_error = h_rnn.norm()\n",
    "gru_error = h_gru.norm()\n",
    "lstm_error = h_lstm.norm() + c_lstm.norm()\n",
    "\n",
    "rnn_error.backward()\n",
    "gru_error.backward()\n",
    "lstm_error.backward()\n",
    "\n",
    "print(f\"Hidden state norms: {rnn_error:.2e}, {gru_error:.2e}, {lstm_error:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ab50badc-ac31-43ff-a469-da174722fe5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5695, 0.3378],\n",
       "         [0.5443, 0.3228]]),\n",
       " tensor([[-0.0997, -0.1173],\n",
       "         [ 0.0308,  0.0358],\n",
       "         [ 0.0012,  0.0012],\n",
       "         [ 0.0363,  0.0388],\n",
       "         [ 0.3305,  0.3887],\n",
       "         [ 0.0720,  0.0834]]),\n",
       " tensor([[ 0.1479, -0.1351],\n",
       "         [ 0.2204, -0.1990],\n",
       "         [ 0.2406, -0.2199],\n",
       "         [ 0.2446, -0.2221],\n",
       "         [ 0.5650, -0.5166],\n",
       "         [-0.4403,  0.3976],\n",
       "         [ 0.1588, -0.1445],\n",
       "         [ 0.0152, -0.0163]]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.weight_hh_l0.grad, gru_model.weight_hh_l0.grad, lstm_model.weight_hh_l0.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b2473-c806-405e-b0c3-647151ebdcba",
   "metadata": {},
   "source": [
    "# <a id=\"embeddings\"></a>4. Embeddings\n",
    "\n",
    "**TODO: Embedding and Latent Space Explanation**\n",
    "\n",
    "When dealing with text, each word or token is usually mapped to an **embedding** vector rather than a large one-hot vector.\n",
    "\n",
    "- **Embedding Layer**: A learnable matrix that maps token indices to dense vectors of fixed dimension $d$.\n",
    "- This helps the model learn **semantic relationships** between words.\n",
    "\n",
    "For example:\n",
    "- Word “hello” → index 5 → embedding vector $\\mathbf{e} \\in \\mathbb{R}^d$.\n",
    "\n",
    "Most frameworks (like PyTorch) provide a built-in layer, `nn.Embedding(vocab_size, embed_dim)`, that handles this.\n",
    "\n",
    "\n",
    "### Exercise 3: Custom Embedding Lookup\n",
    "1. Create a small vocabulary of 5 tokens.  \n",
    "2. Initialize a random embedding matrix of shape $(5, d)$.  \n",
    "3. Write a function that takes a token index and returns the corresponding embedding row.  \n",
    "4. Compare with `nn.Embedding` in PyTorch for the same matrix initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "060618e9-54bd-4212-8fd4-e1bf674ca2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom lookup vector: tensor([ 2.2082, -0.6380,  0.4617])\n",
      "nn.Embedding lookup vector: tensor([ 2.2082, -0.6380,  0.4617])\n",
      "Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "embed_dim = 3\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Step 1: create a random embedding matrix\n",
    "embedding_matrix = torch.randn(vocab_size, embed_dim)\n",
    "\n",
    "def custom_embed_lookup(token_idx, embedding_matrix):\n",
    "    \"\"\"\n",
    "    token_idx: int index (0 <= token_idx < vocab_size)\n",
    "    embedding_matrix: shape (vocab_size, embed_dim)\n",
    "    returns: torch.Tensor of shape (embed_dim,)\n",
    "    \"\"\"\n",
    "    return embedding_matrix[token_idx]\n",
    "\n",
    "# Pick a test token index\n",
    "test_idx = 2\n",
    "custom_vec = custom_embed_lookup(test_idx, embedding_matrix)\n",
    "print(\"Custom lookup vector:\", custom_vec)\n",
    "\n",
    "# Step 2: Compare with nn.Embedding\n",
    "embed_layer = nn.Embedding(vocab_size, embed_dim)\n",
    "# Overwrite the embedding_layer's weights with our random matrix\n",
    "with torch.no_grad():\n",
    "    embed_layer.weight.copy_(embedding_matrix)\n",
    "\n",
    "# Now let's see if it matches:\n",
    "with torch.no_grad():\n",
    "    torch_vec = embed_layer(torch.tensor([test_idx]))\n",
    "print(\"nn.Embedding lookup vector:\", torch_vec.squeeze(0))\n",
    "\n",
    "# They should be (almost) identical\n",
    "print(\"Difference:\", (custom_vec - torch_vec.squeeze(0)).abs().sum().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e7f84-2962-4b60-9176-e30ef3d352ae",
   "metadata": {},
   "source": [
    "# <a id=\"training-objectives\"></a>5. Training Objectives in Language Modeling\n",
    "\n",
    "In language modeling, a typical goal is **next-token prediction**: given the previous tokens, predict the next one. We often use **cross-entropy loss** and measure model performance with **perplexity**.\n",
    "\n",
    "### <a id=\"next-token-prediction\"></a>Next Token Prediction\n",
    "\n",
    "For a vocabulary of size $V$, the model outputs a probability distribution over the next token:\n",
    "$$\n",
    "P(x_t \\mid x_{t-1}, x_{t-2}, \\ldots, x_1)\n",
    "$$\n",
    "The training loss for a sequence might be:\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t}\\log P(\\hat{x}_t = x_t)\n",
    "$$\n",
    "where $ x_t $ is the ground truth and $\\hat{x}_t$ is the predicted distribution.\n",
    "\n",
    "\n",
    "### <a id=\"perplexity\"></a>Perplexity\n",
    "\n",
    "**Perplexity (PPL)** is a common metric for language models:\n",
    "$$\n",
    "\\text{PPL} = \\exp\\left(-\\frac{1}{N}\\sum_{t=1}^{N} \\log P(x_t)\\right),\n",
    "$$\n",
    "where $N$ is the total number of tokens in the test set. Lower PPL typically means a better language model.\n",
    "\n",
    "\n",
    "### Exercise 4: Manual Cross-Entropy\n",
    "- Let your model output a probability vector $[0.2, 0.3, 0.1, 0.4]$ for a 4-word vocabulary.  \n",
    "- Suppose the correct label is index 3. Manually compute cross-entropy.  \n",
    "- Compare with `torch.nn.functional.cross_entropy` to confirm your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f4a59189-4da6-43f3-b525-53b24e77e6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual cross-entropy: 0.916290716972994\n",
      "PyTorch cross-entropy: 0.9162907004356384\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "probs = torch.tensor([0.2, 0.3, 0.1, 0.4])\n",
    "true_label = 3  # index 3 is the correct label\n",
    "\n",
    "# 1) Manual cross-entropy\n",
    "manual_ce = -math.log(probs[true_label].item())\n",
    "\n",
    "# 2) Using PyTorch (note that F.cross_entropy expects logits, not probabilities!)\n",
    "# So we need to convert probabilities => logits with log-softmax inverse => logit = log(p_i / 1)\n",
    "# But simpler is to do cross_entropy on log(prob) by building a single \"batch\" example:\n",
    "logits = torch.log(probs).unsqueeze(0)  # shape (1, 4)\n",
    "targets = torch.tensor([true_label])    # shape (1,)\n",
    "\n",
    "ce_torch = F.nll_loss(logits, targets)  # nll_loss expects log-probabilities\n",
    "# or equivalently: ce_torch = F.cross_entropy(logits, targets) if we interpret logits as log-probs\n",
    "\n",
    "print(\"Manual cross-entropy:\", manual_ce)\n",
    "print(\"PyTorch cross-entropy:\", ce_torch.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b643c3-dfed-4493-a590-67e5a8c35f06",
   "metadata": {},
   "source": [
    "# <a id=\"implementation\"></a>6. Implementing a Simple LSTM Text Generator in PyTorch\n",
    "\n",
    "Let’s build a small example that:\n",
    "1. **Prepares a tiny text dataset**.\n",
    "2. Splits it into input–target pairs for next-token prediction.\n",
    "3. Defines and trains an LSTM-based model.\n",
    "4. **Generates** text from the trained model.\n",
    "\n",
    "### <a id=\"data-prep\"></a>Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "17cf4560-45b6-48cd-b578-6b6ce53618d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['pytorch', 'world', 'again', 'hello']\n",
      "Mapping (word -> idx): {'pytorch': 0, 'world': 1, 'again': 2, 'hello': 3}\n",
      "Vocab size: 4\n",
      "Input sequences shape: torch.Size([4, 3])\n",
      "Target words shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Example small text\n",
    "text = \"hello world hello pytorch hello world again\"\n",
    "\n",
    "# Tokenize (word-level for simplicity)\n",
    "words = text.split()\n",
    "vocab = list(set(words))\n",
    "vocab_size = len(vocab)\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Mapping (word -> idx):\", word2idx)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# Convert words to indices\n",
    "indices = [word2idx[w] for w in words]\n",
    "\n",
    "# We'll choose a sequence length\n",
    "seq_length = 3\n",
    "\n",
    "# Prepare training data\n",
    "input_sequences = []\n",
    "target_words = []\n",
    "\n",
    "for i in range(len(indices) - seq_length):\n",
    "    input_seq = indices[i:i+seq_length]   # 3 words\n",
    "    target = indices[i+seq_length]        # the 4th word is the label\n",
    "    input_sequences.append(input_seq)\n",
    "    target_words.append(target)\n",
    "\n",
    "input_sequences = torch.tensor(input_sequences, dtype=torch.long)\n",
    "target_words = torch.tensor(target_words, dtype=torch.long)\n",
    "\n",
    "print(\"Input sequences shape:\", input_sequences.shape)\n",
    "print(\"Target words shape:\", target_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b8569230-bddd-4cc6-8585-25fd1c0e551c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 3] => 0\n",
      "  ['pytorch', 'hello', 'world'] => pytorch\n",
      "[1 3 0] => 3\n",
      "  ['pytorch', 'hello', 'world'] => hello\n",
      "[3 0 3] => 1\n",
      "  ['pytorch', 'hello', 'world'] => world\n",
      "[0 3 1] => 2\n",
      "  ['pytorch', 'hello', 'world'] => again\n"
     ]
    }
   ],
   "source": [
    "for seq, targ in zip(input_sequences, target_words):\n",
    "    seq = seq.numpy()\n",
    "    targ = targ.item()\n",
    "    seq_detokenized = list(map(idx2word.get, seq))\n",
    "    targ_detokenized = idx2word.get(targ)\n",
    "    print(f\"{seq} => {targ}\")\n",
    "    print(f\"  {input_sequences_detokenized} => {targ_detokenized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527d7b4-87ad-475e-aee0-86eaf23edb82",
   "metadata": {},
   "source": [
    "### <a id=\"model-def\"></a>Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "af099acf-c0d2-4136-af58-4f9e401fc993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        # 1) Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # 2) LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        # 3) Linear output layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)                # (batch_size, seq_length, embed_dim)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embedded)  # (batch_size, seq_length, hidden_dim)\n",
    "        final_hidden = lstm_out[:, -1, :]           # last time step\n",
    "        logits = self.fc(final_hidden)              # (batch_size, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a9d58-cc34-4a2f-8d13-798bf443ad5b",
   "metadata": {},
   "source": [
    "### <a id=\"training-loop\"></a>Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a67dcc63-f36a-4f6d-a383-ee3a29013102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/200], Loss: 0.0115\n",
      "Epoch [100/200], Loss: 0.0030\n",
      "Epoch [150/200], Loss: 0.0018\n",
      "Epoch [200/200], Loss: 0.0012\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 8\n",
    "hidden_dim = 16\n",
    "learning_rate = 0.01\n",
    "num_epochs = 200\n",
    "\n",
    "model = SimpleLSTM(vocab_size, embed_dim, hidden_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_sequences)  # shape: (batch_size, vocab_size)\n",
    "    loss = criterion(logits, target_words)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35484b03-2b3d-499a-9677-cad88a37fdd0",
   "metadata": {},
   "source": [
    "### <a id=\"generate-text\"></a>Generating Text\n",
    "\n",
    "We can now generate text by **sampling** the model’s predictions iteratively.\n",
    "\n",
    "Feel free to experiment with:\n",
    "- **Different seeds**.\n",
    "- **Different sampling strategies** (e.g., greedy vs. top-k).  \n",
    "- A **larger corpus** (like Tiny Shakespeare)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "049323c0-a716-4c73-83b9-083df7d48bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: hello world hello pytorch hello world again pytorch\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_words, num_words=5):\n",
    "    model.eval()\n",
    "    words_generated = seed_words[:]\n",
    "    \n",
    "    # Convert seed_words to indices\n",
    "    current_seq = [word2idx[w] for w in seed_words]\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        inp = torch.tensor([current_seq], dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            logits = model(inp)  # shape: (1, vocab_size)\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()  # shape: (vocab_size,)\n",
    "        \n",
    "        # Sample from probability distribution\n",
    "        next_idx = torch.multinomial(probs, 1).item()\n",
    "        next_word = idx2word[next_idx]\n",
    "        words_generated.append(next_word)\n",
    "        \n",
    "        # Slide the window (drop the first index, append new index)\n",
    "        current_seq = current_seq[1:] + [next_idx]\n",
    "    \n",
    "    return \" \".join(words_generated)\n",
    "\n",
    "# Let's try generating with a seed of length = seq_length (3)\n",
    "seed = [\"hello\", \"world\", \"hello\"]  # must be in vocab\n",
    "generated_text = generate_text(model, seed, num_words=5)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6267e-67ac-4047-b6a0-ceef19c69c5d",
   "metadata": {},
   "source": [
    "### Exercise: Experiment with the Generator\n",
    "1. Change the `num_words` to 10 or 20 and see if your text generation forms any repetitive patterns.  \n",
    "2. Try a **larger** dataset if you have one. Compare the coherence of the generated text.  \n",
    "3. Print out intermediate hidden states if you’re curious about how the model’s representation changes over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d6dbfa72-a426-479f-ab2d-ea7c7ef65039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " hello world hello pytorch hello world again pytorch hello world again\n",
      "Intermediate hidden states shapes:\n",
      " Step 1: (1, 16)\n",
      " Step 2: (1, 16)\n",
      " Step 3: (1, 16)\n",
      " Step 4: (1, 16)\n",
      " Step 5: (1, 16)\n",
      " Step 6: (1, 16)\n",
      " Step 7: (1, 16)\n",
      " Step 8: (1, 16)\n"
     ]
    }
   ],
   "source": [
    "# Suppose 'model' is our trained LSTM model, 'word2idx' and 'idx2word' are our mappings.\n",
    "\n",
    "def generate_text_with_hidden(model, seed_words, num_words=10):\n",
    "    \"\"\"\n",
    "    Generate text from the model, returning the hidden states as well.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    words_generated = seed_words[:]\n",
    "    hidden_states = []   # store hidden states at each step\n",
    "    \n",
    "    current_seq = [word2idx[w] for w in seed_words]\n",
    "    \n",
    "    # Hidden and cell state, if needed\n",
    "    # We'll assume 1-layer LSTM, batch_size=1\n",
    "    h, c = None, None\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        inp = torch.tensor([current_seq], dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            # Modify forward pass to capture intermediate hidden states\n",
    "            # We can do this by running the embedding + LSTM manually:\n",
    "            embedded = model.embedding(inp)  # shape (1, seq_length, embed_dim)\n",
    "            # We pass in (h, c) if they exist, otherwise let the LSTM init them\n",
    "            lstm_out, (h, c) = model.lstm(embedded, (h, c) if h is not None else None)\n",
    "            \n",
    "            # final time step\n",
    "            final_hidden = lstm_out[:, -1, :] \n",
    "            \n",
    "            # For debugging: store the hidden state in a list\n",
    "            hidden_states.append(final_hidden.detach().cpu().numpy())\n",
    "            \n",
    "            logits = model.fc(final_hidden)\n",
    "            probs = torch.softmax(logits, dim=-1).squeeze()\n",
    "            next_idx = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "        next_word = idx2word[next_idx]\n",
    "        words_generated.append(next_word)\n",
    "        current_seq = current_seq[1:] + [next_idx]\n",
    "        \n",
    "    return \" \".join(words_generated), hidden_states\n",
    "\n",
    "# Example usage\n",
    "seed = [\"hello\", \"world\", \"hello\"]\n",
    "generated_text, h_states = generate_text_with_hidden(model, seed, num_words=8)\n",
    "print(\"Generated Text:\\n\", generated_text)\n",
    "print(\"Intermediate hidden states shapes:\")\n",
    "for i, hs in enumerate(h_states):\n",
    "    print(f\" Step {i+1}: {hs.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01d168-505a-40e2-9daf-69039934e4af",
   "metadata": {},
   "source": [
    "# Fun Things -- Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054925f7-8c8f-4631-82c1-721a8bc497e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
