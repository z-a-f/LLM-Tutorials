{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803b7e6f-657c-41de-aaba-6d2e2138aeb2",
   "metadata": {},
   "source": [
    "# Session 1 ‚Äì Data Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec3ac8-8799-47c8-a33c-8226ef6027f4",
   "metadata": {},
   "source": [
    "In this session, we will learn about fundamental techniques for **data preprocessing** and **tokenization** ‚Äì two of the most important steps in preparing text data for language model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dda4a2-77c7-4caf-b023-3f8499a9cd87",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction and Overview](#introduction)\n",
    "1. [Data Cleaning](#data-cleaning)\n",
    "1. [Text Normalization](#text-normalization)\n",
    "1. [Basic Tokenization Concepts](#basic-tokenization)\n",
    "    1. [Word-Level Tokenization](#word-level-tokenization)\n",
    "    1. [Character-Level Tokenization](#character-level-tokenization)\n",
    "    1. [Subword Tokenization](#subword-tokenization)\n",
    "1. [Using Hugging Face‚Äôs Tokenizers Library](#hf-tokenizers)\n",
    "1. [Putting It All Together - Processing a Small Dataset](#dataset-processing)\n",
    "1. [Conclusion](#conclusion)\n",
    "\n",
    "Each of these sections includes detailed explanations and, after each major concept, there‚Äôs a hands-on exercise for you to try out. Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2150b86-1f6b-4d04-a222-7a5c1de07ed7",
   "metadata": {},
   "source": [
    "# <a id=\"introduction\"></a>Introduction and Overview\n",
    "\n",
    "Before we can train language models to process and understand text, we need to ensure our data is properly **cleaned**, **normalized**, and **tokenized**. \n",
    "\n",
    "1. **Data Cleaning**: Removing unwanted characters, dealing with punctuation, whitespace, etc.  \n",
    "2. **Text Normalization**: Converting text into a canonical or standard form.  \n",
    "3. **Tokenization**: Splitting text into meaningful units called tokens (characters, words, or subwords).  \n",
    "\n",
    "By the end of this session, you should be able to:\n",
    "- Understand why data preprocessing is critical for NLP tasks.\n",
    "- Implement and customize various tokenization methods.\n",
    "- Use libraries like **Hugging Face‚Äôs `tokenizers`** to train advanced tokenizers.\n",
    "\n",
    "Let's jump right into the details!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd3e53c-a4f2-4469-b132-b3242165bf2f",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "<a id=\"data-cleaning\"></a>\n",
    "\n",
    "Data cleaning is often the very first step. It's about **removing or transforming unwanted elements** in text, such as:\n",
    "- HTML tags or XML markup\n",
    "- Extra whitespace\n",
    "- Numeric or special characters (if they are not meaningful for your task)\n",
    "- Stopwords (like \"the\", \"a\", \"and\") in some contexts\n",
    "- URLs and email addresses\n",
    "\n",
    "**Key intuition**: The goal is to reduce the ‚Äúnoise‚Äù in the text while retaining the parts that are valuable for your model. Sometimes, \"noise\" may be crucial for tasks like named entity recognition, so always be mindful of your end-goals.\n",
    "\n",
    "Below is an example of common cleaning steps:\n",
    "\n",
    "1. **Removing or replacing URLs**: `www.example.com`, `http://...`, etc.\n",
    "2. **Removing special characters/emojis**: e.g., `üòä`, `üò¢`, etc. (But if emojis matter to your problem, keep them!)\n",
    "3. **Removing multiple consecutive whitespaces**.\n",
    "4. **Case folding**: converting text to lowercase (often helps for text classification or language modeling).\n",
    "\n",
    "### Code Example: Basic Data Cleaning\n",
    "\n",
    "When you run the below code, you should see that the function:\n",
    "- Removes the URL.\n",
    "- Strips out HTML tags.\n",
    "- Removes punctuation and special characters.\n",
    "- Converts text to lowercase.\n",
    "- Collapses consecutive spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "690c9047-c5a0-415a-a918-6e125d9210d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello World! Visit us at http://example.com.  \n",
      "                 <b>Thank</b> you! 123 :-)\n",
      "              \n",
      "Cleaned:  hello world visit us at thank you 123\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def basic_data_cleaning(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters (keeping spaces)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the function\n",
    "sample_text = \"\"\"Hello World! Visit us at http://example.com.  \n",
    "                 <b>Thank</b> you! 123 :-)\n",
    "              \"\"\"\n",
    "cleaned_text = basic_data_cleaning(sample_text)\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Cleaned: \", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cc3abd-77bb-4374-968b-eb74f9a6666e",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "**Goal**: Write a custom cleaning function that handles **emails**, **numbers**, and **emojis** differently.  \n",
    "1. Keep emails as a single token, like `EMAILTOKEN`.  \n",
    "2. Replace all numeric values with `NUMTOKEN`.  \n",
    "3. Convert emojis into a textual representation (e.g., `üòä` ‚Üí `EMOJI_SMILE`).  \n",
    "4. Test your function on a few sentences containing these elements.  \n",
    "\n",
    "*Hint*: You can use Python‚Äôs **`re`** module for pattern matching and substitution.  \n",
    "Try to carefully decide which patterns you want to remove and which you want to keep as placeholders (like `EMAILTOKEN`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9adf341-6716-405e-9f89-3003fb495dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "EMAILTOKEN = r\"<|EMAIL|>\"\n",
    "NUMTOKEN = r\"<|NUMTOKEN|>\"\n",
    "EMOJITOKEN = r\"<|EMOJITOKEN|>\"\n",
    "    \n",
    "def process_emails(text, email_token):\n",
    "    \"\"\"Replaces all emails with the 'email_token'\"\"\"\n",
    "    text = re.sub(r\"[\\w\\d\\.+-]+?@[\\w\\d\\.-]+?\\.[\\w\\d\\.]+\", email_token, text)\n",
    "    return text\n",
    "\n",
    "def process_numbers(text, number_token):\n",
    "    \"\"\"Replaces all numbers with the 'number_token'\"\"\"\n",
    "    text = re.sub(r\"[\\d]+\", number_token, text)\n",
    "    return text\n",
    "\n",
    "def process_emojis(text, emoji_token=None):\n",
    "    \"\"\"Replaces emojis with either a single token or named token.\n",
    "\n",
    "    Emoji API is provided by 'emoji' package, which has 'demojize' and 'replace_emoji' functions.\n",
    "\n",
    "    If 'emoji_token' is None, we will use the capitalized emoji name provided by 'emoji' package.\n",
    "    \"\"\"\n",
    "    import emoji\n",
    "    if emoji_token is None:\n",
    "        text = emoji.demojize(text, delimiters=(\"<|\", \"|>\"))\n",
    "    else:\n",
    "        text = emoji.replace_emoji(text, replace=EMOJITOKEN)\n",
    "    return text\n",
    "\n",
    "def process(text):\n",
    "    text = process_emails(text, EMAILTOKEN)\n",
    "    text = process_numbers(text, NUMTOKEN)\n",
    "    text = process_emojis(text, EMOJITOKEN)\n",
    "    return text\n",
    "    \n",
    "\n",
    "\n",
    "# Additional test cases\n",
    "tests = (\n",
    "    # 1. No special tokens\n",
    "    (\"No special tokens here\", \"No special tokens here\"),\n",
    "    \n",
    "    # 2. Multiple emails\n",
    "    (\n",
    "        \"My emails: foo@example.com, bar@example.org; write to me anytime!\",\n",
    "        f\"My emails: {EMAILTOKEN}, {EMAILTOKEN}; write to me anytime!\"\n",
    "    ),\n",
    "    \n",
    "    # 3. Complex email formats\n",
    "    (\n",
    "        \"Please email me at some.email+tag@my-company.co.uk or admin@co.jp\",\n",
    "        f\"Please email me at {EMAILTOKEN} or {EMAILTOKEN}\"\n",
    "    ),\n",
    "    \n",
    "    # 4. Multiple numbers\n",
    "    (\n",
    "        \"Numbers: 123, 2023, 42, and 007.\",\n",
    "        f\"Numbers: {NUMTOKEN}, {NUMTOKEN}, {NUMTOKEN}, and {NUMTOKEN}.\"\n",
    "    ),\n",
    "    \n",
    "    # 5. Phone number or numeric strings with punctuation\n",
    "    (\n",
    "        \"Call me at 123-456-7890!\",\n",
    "        f\"Call me at {NUMTOKEN}-{NUMTOKEN}-{NUMTOKEN}!\"\n",
    "    ),\n",
    "    \n",
    "    # 6. Multiple emojis\n",
    "    (\n",
    "        \"I love üçï and also this one üëç!\",\n",
    "        f\"I love {EMOJITOKEN} and also this one {EMOJITOKEN}!\"\n",
    "    ),\n",
    "    \n",
    "    # 7. Mixed scenario\n",
    "    (\n",
    "        \"Email: me+123@foo.bar, number: 999, emoji: ü§ó\",\n",
    "        f\"Email: {EMAILTOKEN}, number: {NUMTOKEN}, emoji: {EMOJITOKEN}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "failed_samples = []\n",
    "for idx, (text, expected) in enumerate(tests):\n",
    "    output = process(text)\n",
    "    if output != expected:\n",
    "        print(\"x\", end=\"\")\n",
    "        failed_samples.append([idx, text, output, expected])\n",
    "    else:\n",
    "        print(\".\", end=\"\")\n",
    "print()\n",
    "\n",
    "if failed_samples:\n",
    "    print(\"Failed samples:\")\n",
    "    for idx, inp, outp, exp in failed_samples:\n",
    "        print(f\"Test {idx}:\")\n",
    "        print(f\"  Input:    {inp}\")\n",
    "        print(f\"  Expected: {exp}\")\n",
    "        print(f\"  Got:      {outp}\")\n",
    "else:\n",
    "    print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd37b228-97f2-463d-a7fa-abde4ae7832b",
   "metadata": {},
   "source": [
    "# 3. Text Normalization\n",
    "<a id=\"text-normalization\"></a>\n",
    "\n",
    "Text normalization is about **converting text into a standard (canonical) form**. This includes:\n",
    "- **Case folding** (already shown above, converting text to lowercase).\n",
    "- **Stemming**: Reducing words to their \"stem\" or root form (e.g., `walking` ‚Üí `walk`, `studies` ‚Üí `studi`).\n",
    "- **Lemmatization**: More advanced than stemming. Uses the vocabulary and morphological analysis to get the canonical form of a word (e.g., `walking` ‚Üí `walk`, `studies` ‚Üí `study`).\n",
    "\n",
    "**Stemming** is often simpler and faster but less accurate than **lemmatization**. Lemmers require dictionaries or morphological analyzers to convert a word to its correct lemma.\n",
    "\n",
    "### Example: Using NLTK for Stemming and Lemmatization\n",
    "\n",
    "Below, notice how **stemming** might cut words more aggressively, while **lemmatization** uses morphological knowledge to find the correct base form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15873762-efe8-40c2-90c1-1a55044d0066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['he', 'was', 'running', 'and', 'eating', 'at', 'the', 'same', 'time', '.', 'he', 'has', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hours', 'in', 'the', 'sun', '.']\n",
      "Stemmed Tokens:  ['he', 'wa', 'run', 'and', 'eat', 'at', 'the', 'same', 'time', '.', 'he', 'ha', 'bad', 'habit', 'of', 'swim', 'after', 'play', 'long', 'hour', 'in', 'the', 'sun', '.']\n",
      "Lemmatized Tokens: ['he', 'wa', 'running', 'and', 'eating', 'at', 'the', 'same', 'time', '.', 'he', 'ha', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hour', 'in', 'the', 'sun', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at the same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "tokens = word_tokenize(sentence.lower())\n",
    "stems = [porter_stemmer.stem(token) for token in tokens]\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Stemmed Tokens: \", stems)\n",
    "print(\"Lemmatized Tokens:\", lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef9c1b-c737-4b93-a26e-9fd6697555ef",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "**Goal**:  \n",
    "- Experiment with different stemmers in NLTK (e.g., `SnowballStemmer`) and compare the outputs.  \n",
    "- Create a custom lemmatization pipeline for your text.  \n",
    "  - Try to handle different parts of speech (POS). NLTK‚Äôs `WordNetLemmatizer` can use POS tags to correctly lemmatize words (e.g., `lemmatizer.lemmatize(word, pos='v')` for verbs).  \n",
    "- Observe how the results differ depending on which approach you use.  \n",
    "\n",
    "**Hint**: You can get POS tags using `nltk.pos_tag(tokens)`, then map NLTK‚Äôs POS tags to WordNet‚Äôs POS notation (like `nltk.corpus.wordnet.VERB`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f794c52b-c417-42ae-ad6f-0092721096b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens     : ['he', 'was', 'running', 'and', 'eating', 'at', 'the', 'same', 'time', '.', 'he', 'has', 'bad', 'habits', 'of', 'swimming', 'and', 'studies', '!']\n",
      "PorterStemmer       : ['he', 'wa', 'run', 'and', 'eat', 'at', 'the', 'same', 'time', '.', 'he', 'ha', 'bad', 'habit', 'of', 'swim', 'and', 'studi', '!']\n",
      "SnowballStemmer     : ['he', 'was', 'run', 'and', 'eat', 'at', 'the', 'same', 'time', '.', 'he', 'has', 'bad', 'habit', 'of', 'swim', 'and', 'studi', '!']\n",
      "Custom Lemmatization: ['he', 'be', 'run', 'and', 'eat', 'at', 'the', 'same', 'time', '.', 'he', 'have', 'bad', 'habit', 'of', 'swimming', 'and', 'study', '!']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def nltk_pos_to_wordnet_pos(nltk_pos_tag):\n",
    "    \"\"\"Map the NLTK part-of-speech tags to WordNet's part-of-speech format.\n",
    "    \"\"\"\n",
    "    if nltk_pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def custom_lemmatizer(text):\n",
    "    \"\"\"\n",
    "    Tokenize, POS-tag, and then lemmatize each token with WordNetLemmatizer,\n",
    "    using the mapped POS tags for better accuracy.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)  # [('running', 'VBG'), ('dogs', 'NNS'), ...]\n",
    "\n",
    "    lemmatized_tokens = []\n",
    "    for token, pos in pos_tags:\n",
    "        wn_pos = nltk_pos_to_wordnet_pos(pos)\n",
    "        if wn_pos: \n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(token, wn_pos))\n",
    "        else:\n",
    "            # If we don't have a matching POS, just use the token as is\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(token))\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Example text\n",
    "text = \"He was running and eating at the same time. He has bad habits of swimming and studies!\"\n",
    "    \n",
    "# 1. Compare two stemmers:\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "porter_stems = [porter.stem(token) for token in tokens]\n",
    "snowball_stems = [snowball.stem(token) for token in tokens]\n",
    "lemmatized = custom_lemmatizer(text.lower())\n",
    "\n",
    "print(\"Original Tokens     :\", tokens)\n",
    "print(\"PorterStemmer       :\", porter_stems)\n",
    "print(\"SnowballStemmer     :\", snowball_stems)\n",
    "print(\"Custom Lemmatization:\", lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992a04c9-7484-4503-82d9-db0c99e52f18",
   "metadata": {},
   "source": [
    "### Observations\n",
    "* PorterStemmer may cut off endings more aggressively.\n",
    "* SnowballStemmer (English) might behave slightly differently on certain words.\n",
    "* Lemmatization can produce more readable tokens, especially if you provide the correct POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d92c62-82b6-4032-8c74-54c022166834",
   "metadata": {},
   "source": [
    "# 4. Basic Tokenization Concepts\n",
    "<a id=\"basic-tokenization\"></a>\n",
    "\n",
    "Tokenization is the process of **splitting text into smaller units** called *tokens*. The granularity of tokens can be:\n",
    "- **Word-level**: e.g., \"Hello World\" ‚Üí [\"Hello\", \"World\"]\n",
    "- **Character-level**: e.g., \"Hello\" ‚Üí [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
    "- **Subword-level**: e.g., \"tokenization\" ‚Üí [\"to\", \"ken\", \"ization\"]\n",
    "\n",
    "### Key Intuition:\n",
    "1. **Word-level** tokenization is intuitive but can suffer with out-of-vocabulary (OOV) words.  \n",
    "2. **Character-level** tokenization rarely has OOV issues but might lead to longer sequences to process.  \n",
    "3. **Subword-level** tokenization (like Byte Pair Encoding ‚Äì BPE, WordPiece, etc.) strikes a balance between word- and character-level.  \n",
    "\n",
    "We will explore each approach in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7366bb-a70d-4c51-a1f5-4102439c83fb",
   "metadata": {},
   "source": [
    "## 4.1. Word-Level Tokenization\n",
    "<a id=\"word-level-tokenization\"></a>\n",
    "\n",
    "### Concept & Example\n",
    "\n",
    "Word-level tokenization splits text on spaces and punctuation, often discarding punctuation as separate tokens (depending on your tokenizer design). For simple English text, you can use **Python‚Äôs `split()`** or **libraries like NLTK‚Äôs `word_tokenize`**.\n",
    "\n",
    "**Why use word-level tokenization?**  \n",
    "- It's straightforward and often sufficient for many tasks.\n",
    "- It‚Äôs a good starting point for simple classification or bag-of-words models.\n",
    "\n",
    "**Drawbacks**:  \n",
    "- Struggles with morphological variants (`walk`, `walks`, `walked`, `walking`) ‚Äì they all become different tokens.  \n",
    "- Large vocabulary size, leading to OOV issues and less efficient training.\n",
    "\n",
    "### Example: Word-Level Tokenization with Python\n",
    "\n",
    "**Note**: Notice how a naive split might remove punctuation differently than NLTK's sophisticated tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "140df9c0-7bcb-46dc-9aaf-9bd600032df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Word Tokenization: ['Hello', 'world', 'This', 'is', 'a', 'test', 'sentence']\n",
      "NLTK Word Tokenization: ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def word_tokenize_simple(text):\n",
    "    # Remove extra spaces and punctuation in a naive way\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "sample_sentence = \"Hello, world! This is a test sentence.\"\n",
    "print(\"Naive Word Tokenization:\", word_tokenize_simple(sample_sentence))\n",
    "\n",
    "# Using NLTK for more sophisticated splitting:\n",
    "import nltk\n",
    "nltk_tokens = nltk.word_tokenize(sample_sentence)\n",
    "print(\"NLTK Word Tokenization:\", nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a791f-1812-49e3-bffa-e86b5e154604",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "**Goal**:  \n",
    "1. Write a function that splits text on white spaces **but** keeps punctuation as separate tokens.  \n",
    "   - For example, `\"Hello, world!\"` ‚Üí `[\"Hello\", \",\", \"world\", \"!\"]`.  \n",
    "2. Compare it against `nltk.word_tokenize` on a set of example sentences.  \n",
    "3. Discuss which approach might be more beneficial for sentiment analysis vs. language modeling.  \n",
    "\n",
    "*Hint*: You can use regular expressions, capturing punctuation as separate tokens by including capturing groups for punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b052ed6c-4934-48f6-ad29-e691094bf068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Tokenization: ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', '-', 'sentence', '.']\n",
      "NLTK Tokenization  : ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test-sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def whitespace_and_punct_tokenize(text):\n",
    "    \"\"\"\n",
    "    Splits on whitespace, but also captures punctuation as separate tokens.\n",
    "    For instance, \"Hello, world!\" -> [\"Hello\", \",\", \"world\", \"!\"].\n",
    "    \"\"\"\n",
    "    # Explanation of the regex:\n",
    "    # (        start capture group\n",
    "    #   [^\\s\\w]    matches any punctuation/symbol (non-whitespace, non-word)\n",
    "    #   |          OR\n",
    "    #   \\w+        matches sequences of word characters\n",
    "    # )        end capture group\n",
    "    pattern = r\"([^\\s\\w]|[\\w]+)\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "# Example usage\n",
    "sample = \"Hello, world! This is a test-sentence.\"\n",
    "custom_tokens = whitespace_and_punct_tokenize(sample)\n",
    "print(\"Custom Tokenization:\", custom_tokens)\n",
    "\n",
    "# Compare to NLTK\n",
    "nltk_tokens = nltk.word_tokenize(sample)\n",
    "print(\"NLTK Tokenization  :\", nltk_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380f6d6-c409-48bd-ac4b-2c5e32026cfe",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "You can see how these two approaches differ in how they handle the hyphen in \"test-sentence\" or punctuation spacing.\n",
    "\n",
    "**Which Approach for Which Task?**\n",
    "* Sentiment Analysis often benefits from preserving punctuation (e.g., exclamation marks, question marks).\n",
    "* Language Modeling might still use punctuation but sometimes merges punctuation with preceding words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ac59d-ef09-4962-b952-7abc9219df8a",
   "metadata": {},
   "source": [
    "## 4.2. Character-Level Tokenization\n",
    "<a id=\"character-level-tokenization\"></a>\n",
    "\n",
    "Character-level tokenization treats **every character as a token**. This approach ensures:\n",
    "- **No OOV problems** (every symbol is known).\n",
    "- Suitable for languages with complex morphology or no clear word boundaries (e.g., Chinese).\n",
    "\n",
    "**Drawbacks**:\n",
    "- Very long sequences (each sentence becomes a big list of characters).\n",
    "- Models need more capacity to learn word-level or phrase-level meaning from individual characters.\n",
    "\n",
    "### Example: Character-Level Tokenization\n",
    "\n",
    "If you run the cell below, you should see something like:\n",
    "\n",
    "`Character Tokens: ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faf117b1-22b3-41bb-8741-8e9272e1925f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokens: ['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "def character_tokenize(text):\n",
    "    # Simply convert the text into a list of characters\n",
    "    return list(text)\n",
    "\n",
    "sample_sentence = \"Hello world!\"\n",
    "char_tokens = character_tokenize(sample_sentence)\n",
    "print(\"Character Tokens:\", char_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d0044-107b-4534-8c38-247599cb5ca6",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "**Goal**:  \n",
    "1. Implement a character-level tokenizer that **filters out** certain characters (e.g., remove digits, punctuation).  \n",
    "2. Use it on a short paragraph and analyze how many tokens you get compared to word-level.  \n",
    "3. (Optional) Experiment with training a simple language model (like an LSTM or GPT-like architecture) on a short text corpus using character-level tokens, and compare the performance with word-level tokens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53610230-0a36-4ace-a2d4-2d8e475fc2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Characters: ['H', 'e', 'l', 'l', 'o', ' ', 'Z', 'a', 'f', 'a', 'r', ' ', 'I', ' ', 'h', 'a', 'v', 'e', ' ', ' ', 'a', 'p', 'p', 'l', 'e', 's', ' ', 'a', 'n', 'd', ' ', ' ', 'b', 'a', 'n', 'a', 'n', 'a', 's']\n",
      "As joined string   : Hello Zafar I have  apples and  bananas\n",
      "Word-Level Tokens  : ['Hello', 'Zafar!', 'I', 'have', '2', 'apples,', 'and', '3', 'bananas.']\n",
      "\n",
      "Number of character tokens: 39\n",
      "Number of word tokens:      9\n"
     ]
    }
   ],
   "source": [
    "def char_level_filter_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Returns a list of characters, filtering out digits and punctuation.\n",
    "    Keeps only alphabets and spaces for simplicity.\n",
    "    \"\"\"\n",
    "    # You can refine or expand the set of kept characters as needed\n",
    "    filtered_chars = []\n",
    "    for ch in text:\n",
    "        if ch.isalpha() or ch.isspace():\n",
    "            filtered_chars.append(ch)\n",
    "        # else skip punctuation, digits, etc.\n",
    "    return filtered_chars\n",
    "\n",
    "paragraph = \"Hello Zafar! I have 2 apples, and 3 bananas.\"\n",
    "chars = char_level_filter_tokenizer(paragraph)\n",
    "print(\"Filtered Characters:\", chars)\n",
    "print(\"As joined string   :\", ''.join(chars))\n",
    "\n",
    "# Compare with a naive word-level:\n",
    "word_tokens = paragraph.split()\n",
    "print(\"Word-Level Tokens  :\", word_tokens)\n",
    "\n",
    "print(f\"\\nNumber of character tokens: {len(chars)}\")\n",
    "print(f\"Number of word tokens:      {len(word_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4768b1-125b-4308-9706-865aec0748a8",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* The character-level approach yields a larger token count because each letter is a token.\n",
    "* You can adapt the filtering logic depending on your domain needs (e.g., keep digits or punctuation if they‚Äôre important)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a779f4a8-e911-46f2-b590-ac77f7213d2e",
   "metadata": {},
   "source": [
    "## 4.3. Subword Tokenization\n",
    "<a id=\"subword-tokenization\"></a>\n",
    "\n",
    "Subword tokenization methods like **Byte Pair Encoding (BPE)** or **WordPiece** aim to find a **balance** between word-level and character-level. \n",
    "\n",
    "**Motivation**:\n",
    "- **OOV** words are less of a problem: If a new word appears, it can be broken down into smaller subword units it has already seen.  \n",
    "- **Vocabulary size** can be managed because it merges characters into subwords until a desired vocab size is reached.\n",
    "\n",
    "Example (Byte Pair Encoding):\n",
    "1. Start with every character as a token.  \n",
    "2. Count the most frequent pairs of tokens and merge them into a single token.  \n",
    "3. Repeat until reaching the desired vocabulary size.\n",
    "\n",
    "**Key advantage**: It can handle morphological variations and new words effectively, while not exploding the sequence length as character-level does\n",
    "\n",
    "### Simple Subword Tokenizer (BPE) Example\n",
    "\n",
    "Below is a **simplified** BPE-like implementation to illustrate the concept. The real versions (like in Hugging Face tokenizers) are more complex and efficient.\n",
    "\n",
    "**Note**: Example below uses **characters** as the starting point. We merge the most frequent pairs step by step. Eventually, tokens like `\"lo\"` or `\"low\"` might appear if they are frequent pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72b19ec3-c97b-457c-8b16-7bac385a0c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Merged ('l', 'o') -> lo\n",
      "Step 2, Merged ('lo', 'w') -> low\n",
      "Step 3, Merged ('e', 'r') -> er\n",
      "Step 4, Merged ('er', ' ') -> er \n",
      "Step 5, Merged ('low', ' ') -> low \n",
      "Merged Corpus: [['low '], ['low', 'er '], ['n', 'e', 'w', 'e', 's', 't', ' '], ['w', 'i', 'd', 'er ']]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_stats(tokenized_corpus):\n",
    "    \"\"\"\n",
    "    Counts how frequently pairs of tokens appear.\n",
    "    :param tokenized_corpus: list of sentences, each sentence is a list of tokens.\n",
    "    :return: dictionary {(token1, token2): count}\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for sentence in tokenized_corpus:\n",
    "        for i in range(len(sentence)-1):\n",
    "            pairs[(sentence[i], sentence[i+1])] += 1\n",
    "    return pairs\n",
    "\n",
    "def merge_most_frequent(tokenized_corpus, pair_to_merge):\n",
    "    \"\"\"\n",
    "    Merge the most frequent pair in the corpus.\n",
    "    \"\"\"\n",
    "    new_tokenized_corpus = []\n",
    "    (a, b) = pair_to_merge\n",
    "    merge_token = a + b\n",
    "    for sentence in tokenized_corpus:\n",
    "        new_sentence = []\n",
    "        skip = False\n",
    "        for i in range(len(sentence)):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if i < len(sentence) - 1 and sentence[i] == a and sentence[i+1] == b:\n",
    "                new_sentence.append(merge_token)\n",
    "                skip = True\n",
    "            else:\n",
    "                new_sentence.append(sentence[i])\n",
    "        new_tokenized_corpus.append(new_sentence)\n",
    "    return new_tokenized_corpus\n",
    "\n",
    "def bpe_training(tokenized_corpus, num_merges=10):\n",
    "    \"\"\"\n",
    "    Trains a simple BPE tokenizer.\n",
    "    :param tokenized_corpus: list of tokenized sentences\n",
    "    :param num_merges: how many merges to perform\n",
    "    :return: The merged corpus after BPE\n",
    "    \"\"\"\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(tokenized_corpus)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        tokenized_corpus = merge_most_frequent(tokenized_corpus, best_pair)\n",
    "        print(f\"Step {i+1}, Merged {best_pair} -> {best_pair[0] + best_pair[1]}\")\n",
    "    return tokenized_corpus\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    list(\"low \"),\n",
    "    list(\"lower \"),\n",
    "    list(\"newest \"),\n",
    "    list(\"wider \")\n",
    "]\n",
    "\n",
    "merged_corpus = bpe_training(corpus, num_merges=5)\n",
    "print(\"Merged Corpus:\", merged_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad38a35-2a70-42c8-a245-aecc072c90a5",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "**Goal**:  \n",
    "1. Take a short text corpus, tokenize it by characters, and then apply the above `bpe_training` function with different values of `num_merges`.  \n",
    "2. Observe how tokens evolve with each merge step.  \n",
    "3. Discuss how subword merges make sense for repeated sequences of characters.  \n",
    "\n",
    "*(Optional)*:  \n",
    "- Modify the code to start with word-level tokens instead of characters, then sub-split words by merges.  \n",
    "- Experiment to see if you can reduce the vocabulary size while still retaining most of the text‚Äôs representational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0550d360-3667-40be-a481-4c3d41dc3aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Merged ('h', 'e') -> he\n",
      "Step 2, Merged ('he', 'l') -> hel\n",
      "Step 3, Merged ('hel', 'l') -> hell\n",
      "Step 4, Merged ('hell', 'o') -> hello\n",
      "Step 5, Merged (' ', 'w') ->  w\n",
      "\n",
      "Merged corpus with 5 merges:\n",
      "['hello', ' ', 'hello']\n",
      "['hello', ' w', 'o', 'r', 'l', 'd']\n",
      "['w', 'o', 'r', 'l', 'd', ' w', 'i', 'd', 'e', ' w', 'e', 'b']\n",
      "Step 1, Merged ('h', 'e') -> he\n",
      "Step 2, Merged ('he', 'l') -> hel\n",
      "\n",
      "Merged corpus with 2 merges:\n",
      "['hel', 'l', 'o', ' ', 'hel', 'l', 'o']\n",
      "['hel', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n",
      "['w', 'o', 'r', 'l', 'd', ' ', 'w', 'i', 'd', 'e', ' ', 'w', 'e', 'b']\n"
     ]
    }
   ],
   "source": [
    "sample_corpus = [\n",
    "    \"hello hello\", \n",
    "    \"hello world\", \n",
    "    \"world wide web\"\n",
    "]\n",
    "\n",
    "# Split each sentence into a list of characters\n",
    "tokenized_corpus = [list(sent) for sent in sample_corpus]\n",
    "\n",
    "# Suppose you do 5 merges\n",
    "merged_corpus_5 = bpe_training(tokenized_corpus, num_merges=5)\n",
    "\n",
    "print(\"\\nMerged corpus with 5 merges:\")\n",
    "for line in merged_corpus_5:\n",
    "    print(line)\n",
    "\n",
    "# Suppose you do 2 merges\n",
    "tokenized_corpus = [list(sent) for sent in sample_corpus]  # reset\n",
    "merged_corpus_2 = bpe_training(tokenized_corpus, num_merges=2)\n",
    "\n",
    "print(\"\\nMerged corpus with 2 merges:\")\n",
    "for line in merged_corpus_2:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f97c58-2cf8-4873-b90d-9506466879a0",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* Frequent pairs like (\"h\", \"e\") or (\"e\", \"l\") being merged first if they appear a lot.\n",
    "* Compare the final tokens for num_merges=5 vs. num_merges=2. With fewer merges, you remain closer to pure character-level. With more merges, you start to see ‚Äúsubwords‚Äù forming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f02914-dbf1-452a-b640-ff9aabceca33",
   "metadata": {},
   "source": [
    "# 5. Using Hugging Face‚Äôs Tokenizers Library\n",
    "<a id=\"hf-tokenizers\"></a>\n",
    "\n",
    "While writing your own tokenizer is educational, for practical tasks, it‚Äôs more convenient (and often more optimized) to rely on well-tested libraries like **Hugging Face‚Äôs `tokenizers`**. This library offers efficient implementations of BPE, WordPiece, Unigram, and more.\n",
    "\n",
    "**Note:** Make sure the `tokenizers` library is installed. You can install it using `pip install -U tokenizers`\n",
    "\n",
    "### Example: Training a BPE Tokenizer with Hugging Face\n",
    "\n",
    "**Key aspects**:\n",
    "- We specify a **`vocab_size`** to control how many merges the tokenizer can learn.\n",
    "- We include **special tokens** like `[UNK]`, `[CLS]`, `[SEP]`, `[PAD]`, `[MASK]` for tasks like language modeling and classification.\n",
    "- After training, we can **encode** new text to see how the tokenizer splits it into subword units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "470b2a1e-3802-4877-ba60-b1dc3198868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Encoded IDs: [36, 65, 0, 25, 15, 34, 34, 10, 25, 13, 24, 25, 0]\n",
      "Encoded Tokens: ['Hello', 'world', '[UNK]', 't', 'h', 'is', 'is', 'a', 't', 'e', 's', 't', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# 1. Initialize an empty BPE tokenizer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# 2. Prepare trainer\n",
    "trainer = BpeTrainer(vocab_size=2000, show_progress=True, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# Suppose we have a list of file paths or text data. For a small example, let's create one text file:\n",
    "sample_text_data = \"\"\"Hello world\n",
    "Hello there\n",
    "This is a small example corpus for BPE Tokenizer\n",
    "\"\"\"\n",
    "\n",
    "with open(\"sample_corpus.txt\", \"w\") as f:\n",
    "    f.write(sample_text_data)\n",
    "\n",
    "# 3. Train the tokenizer\n",
    "tokenizer.train(files=[\"sample_corpus.txt\"], trainer=trainer)\n",
    "\n",
    "# 4. Encode some text\n",
    "encoded = tokenizer.encode(\"Hello world, this is a test!\")\n",
    "print(\"Encoded IDs:\", encoded.ids)\n",
    "print(\"Encoded Tokens:\", encoded.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f52ba5-577b-46b1-8c27-e8e9b05125c1",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "**Goal**:  \n",
    "1. Use Hugging Face‚Äôs tokenizers library to train a tokenizer on a larger corpus (e.g., **Tiny Shakespeare** if you have it, or any small open-source dataset).  \n",
    "2. Experiment with different `vocab_size` values (e.g., 500, 1000, 5000). Compare the resulting tokens for some sample sentences.  \n",
    "3. Make a short summary of how the choice of `vocab_size` affects:\n",
    "   - The granularity of your tokens.  \n",
    "   - The presence of `[UNK]` tokens when encoding new text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5612ad9e-d287-4840-8105-83e33336b874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Vocab Size: 200 ---\n",
      "Tokens: ['Hello', 'world', '[UNK]', 't', 'h', 'is', 'is', 'a', 't', 'es', 't', '[UNK]']\n",
      "Token IDs: [41, 82, 0, 27, 17, 37, 37, 11, 27, 54, 27, 0]\n",
      "\n",
      "--- Vocab Size: 50 ---\n",
      "Tokens: ['Hello', 'w', 'or', 'l', 'd', '[UNK]', 't', 'h', 'is', 'is', 'a', 't', 'e', 's', 't', '[UNK]']\n",
      "Token IDs: [41, 30, 34, 20, 14, 0, 27, 17, 37, 37, 11, 27, 15, 26, 27, 0]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "text_corpus = \"\"\"Hello world\n",
    "Hello there\n",
    "This is a small example corpus for BPE Tokenizer\n",
    "We will compare vocab sizes\n",
    "\"\"\"\n",
    "\n",
    "# Save it to a file\n",
    "with open(\"my_corpus.txt\", \"w\") as f:\n",
    "    f.write(text_corpus)\n",
    "\n",
    "def train_and_test_tokenizer(vocab_size):\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size, \n",
    "        show_progress=False, \n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "    )\n",
    "    tokenizer.train([\"my_corpus.txt\"], trainer)\n",
    "    return tokenizer\n",
    "\n",
    "for vs in [200, 50]:\n",
    "    tokenizer = train_and_test_tokenizer(vs)\n",
    "    print(f\"\\n--- Vocab Size: {vs} ---\")\n",
    "    sample_text = \"Hello world, this is a test!\"\n",
    "    encoding = tokenizer.encode(sample_text)\n",
    "    print(\"Tokens:\", encoding.tokens)\n",
    "    print(\"Token IDs:\", encoding.ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e49bf-eea1-4c4d-b791-4142b7c464ec",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* With a larger vocabulary (e.g., 200), you‚Äôll get fewer [UNK] tokens and generally bigger subwords.\n",
    "* With a smaller vocabulary (e.g., 50), you might see text broken down into more subword pieces or even individual characters in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befac037-533a-452d-afab-603e7c2cb4bc",
   "metadata": {},
   "source": [
    "# 6. Putting It All Together ‚Äì Processing a Small Dataset\n",
    "<a id=\"dataset-processing\"></a>\n",
    "\n",
    "Let‚Äôs walk through a **mini-pipeline** for **Tiny Shakespeare** or any small text dataset you have. The steps are:\n",
    "\n",
    "1. **Load the dataset**: Suppose it‚Äôs a single text file called `tiny_shakespeare.txt`.\n",
    "2. **Clean the text**: Remove unnecessary characters, unify case, etc.\n",
    "3. **(Optionally) Normalize**: Decide if stemming/lemmatization is beneficial for your task.\n",
    "4. **Tokenize**: \n",
    "   - Decide on your method (word-level, char-level, subword).\n",
    "   - Train or load a pre-trained tokenizer if you‚Äôre doing subword.\n",
    "5. **Save the processed data**: Convert tokens to integer IDs (if subword or word-level) and store them in a format that your model can read.\n",
    "\n",
    "### Example Pipeline for Tiny Shakespeare\n",
    "\n",
    "**Note:** In a real project, you‚Äôd store these token IDs in a serialized format (e.g., NumPy array, PyTorch tensor) and then use them for training a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5daedf1-ef1a-4de4-9c8b-4742c92a3c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokens: ['from', 'fairest', 'creatures', 'we', 'desire', 'increase', 'that', 'thereby', 'beautys', 'rose', 'might', 'never', 'die']\n",
      "Token IDs: [63, 64, 68, 52, 61, 65, 57, 69, 70, 56, 66, 67, 62]\n",
      "First 50 IDs of the corpus: [63, 64, 68, 52, 61, 65, 57, 69, 70, 56, 66, 67, 62]\n"
     ]
    }
   ],
   "source": [
    "# Suppose you have 'tiny_shakespeare.txt' in your working directory\n",
    "# with open(\"tiny_shakespeare.txt\", \"r\") as f:\n",
    "#     shakespeare_data = f.read()\n",
    "\n",
    "shakespeare_data = \"\"\"From fairest creatures we desire increase, \n",
    "That thereby beauty's rose might never die...\"\"\"  # Mock sample\n",
    "\n",
    "# 1. Clean the data\n",
    "clean_text = basic_data_cleaning(shakespeare_data)\n",
    "# 2. (Optional) we won't do advanced normalization for a Shakespeare corpus, \n",
    "#    but we might use lemmatization if it suits the task.\n",
    "\n",
    "# 3. Train a BPE tokenizer using Hugging Face‚Äôs tokenizers\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(vocab_size=500, show_progress=True, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# We'll save our cleaned text to a file for training the tokenizer\n",
    "with open(\"shakespeare_cleaned.txt\", \"w\") as f:\n",
    "    f.write(clean_text)\n",
    "\n",
    "tokenizer.train(files=[\"shakespeare_cleaned.txt\"], trainer=trainer)\n",
    "\n",
    "# 4. Encode text\n",
    "encoded = tokenizer.encode(clean_text[:100])  # Just the first 100 chars\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "print(\"Token IDs:\", encoded.ids)\n",
    "\n",
    "# 5. Save or convert processed data\n",
    "# For instance, we can convert the entire text to a list of IDs\n",
    "full_ids = tokenizer.encode(clean_text).ids\n",
    "print(\"First 50 IDs of the corpus:\", full_ids[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c043a7-a655-4bfc-abb1-243646130ce1",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "**Goal**:\n",
    "1. Download a small public-domain text (e.g., from [Project Gutenberg](https://www.gutenberg.org/) or [Tiny Shakespeare](https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt)) and run through this pipeline end-to-end.  \n",
    "2. Report how many tokens you get in total, and how many unique tokens there are.  \n",
    "3. Experiment with adding or removing cleaning steps. For example, keep punctuation vs. remove punctuation. See how it changes your token distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2c9deb6-f31a-49da-b043-70976568c22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 1089k  100 1089k    0     0  4174k      0 --:--:-- --:--:-- --:--:-- 4189k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a5ab4a6-2bb5-4be3-97ac-c0982b1b4853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Total tokens in corpus: 297788\n",
      "Unique token IDs: 972\n"
     ]
    }
   ],
   "source": [
    "tiny_shakespeare = \"input.txt\"  # Download from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "# Step 1: load\n",
    "with open(tiny_shakespeare, \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "# Step 2: clean\n",
    "cleaned_text = basic_data_cleaning(text_data)  # from your earlier function\n",
    "\n",
    "# Optional normalization step:\n",
    "# tokens = word_tokenize(cleaned_text)\n",
    "# lemmatized_tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "# cleaned_text = \" \".join(lemmatized_tokens)\n",
    "\n",
    "# Step 3: Train tokenizer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(vocab_size=1000, show_progress=True, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# We'll write out the cleaned text to a file\n",
    "with open(\"cleaned_gutenberg.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cleaned_text)\n",
    "\n",
    "tokenizer.train([\"cleaned_gutenberg.txt\"], trainer)\n",
    "\n",
    "# Step 4 & 5: Encode, measure total tokens and unique tokens\n",
    "encoded_output = tokenizer.encode(cleaned_text)\n",
    "token_ids = encoded_output.ids\n",
    "print(\"Total tokens in corpus:\", len(token_ids))\n",
    "print(\"Unique token IDs:\", len(set(token_ids)))\n",
    "\n",
    "# (Optional) Compare punctuation removal vs. punctuation kept:\n",
    "# - Rerun the pipeline without removing punctuation in 'basic_data_cleaning'\n",
    "# - Check how the vocabulary distribution changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d46dbd-2ed8-4720-813a-88117032d8b9",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* Vocabulary distribution: Larger vocab ‚Üí fewer merges, more unique tokens.\n",
    "* Removing punctuation might reduce the overall variety in tokens, but at the potential cost of losing punctuation signals.\n",
    "* Comparing final tokenized outputs can show how different cleaning steps drastically alter the final ID sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769125bf-0c1a-4c32-bdb8-49e6768dfa32",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "<a id=\"conclusion\"></a>\n",
    "\n",
    "Congratulations on completing **Session 1**: Data Processing and Tokenization! You have learned:\n",
    "\n",
    "1. **Why cleaning and normalization** are essential to reduce noise and help models focus on relevant patterns.\n",
    "2. **Word-level**, **character-level**, and **subword** tokenization strategies:\n",
    "   - Their pros and cons.\n",
    "   - Practical examples of how to implement or use them.\n",
    "3. **Hands-on with Hugging Face tokenizers** to train a subword tokenizer on your own dataset.\n",
    "\n",
    "**Next Steps**:\n",
    "- In future sessions, we‚Äôll look at building and training language models using these tokenized datasets.\n",
    "- Keep experimenting with different tokenization and normalization techniques, as these can drastically affect model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f737a-76d7-4f77-83a0-8722996a49fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
