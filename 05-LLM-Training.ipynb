{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7302111-9dcd-47fd-82de-2b1e6da907f6",
   "metadata": {},
   "source": [
    "# Session 4, Part 2 – Large-Scale Pretraining and Fine-Tuning\n",
    "\n",
    "Previously, in **Part 1**, we built a small Transformer-based text generator (GPT-like model) and discussed training routines on a **small scale**. Now, we will **scale up** to discuss:\n",
    "\n",
    "1. **Large-Scale Pretraining** strategies (distributed training, pipeline parallelism, model sharding).  \n",
    "2. **Mixed Precision** training to reduce memory usage and speed up computation.  \n",
    "3. **Fine-Tuning** on downstream tasks (using domain data, custom heads).  \n",
    "4. **Leveraging third-party APIs** like **Hugging Face Transformers**, **Hugging Face Accelerate**, and **PyTorch Lightning** to handle complexities of large-scale training.\n",
    "\n",
    "**Note**: Due to resource constraints, we will not run a real large-scale pretraining, but you’ll learn the **conceptual framework** and see **example code** that can be adapted to real HPC environments.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Overview of Large-Scale Transformer Training](#overview)\n",
    "2. [Distributed Training Approaches](#distributed)\n",
    "   - [Data Parallelism](#data-parallel)\n",
    "   - [Model/Pipeline Parallelism](#model-parallel)\n",
    "   - [Sharding (e.g., ZeRO, Megatron-LM)](#sharding)\n",
    "3. [Mixed Precision Training](#mixed-precision)\n",
    "   - [FP16, BF16, and FP8 Overview](#fp16-bf16)\n",
    "   - [Practical Implementation in PyTorch/Hugging Face](#mp-implementation)\n",
    "4. [Fine-Tuning on Downstream Tasks](#finetuning)\n",
    "   - [Task-Adaptive Pretraining vs. Fine-Tuning](#task-adapt)\n",
    "   - [Using Hugging Face Transformers](#hf-transformers)\n",
    "   - [Example: Fine-Tuning GPT-2 with PyTorch Lightning](#lightning-example)\n",
    "5. [Putting It All Together – A Scalable Pretraining/Fine-Tuning Pipeline](#pipeline)\n",
    "6. [Practical Exercises – Part 2](#exercises)\n",
    "7. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb30dc3-6c9f-43ae-9ef7-9d1e4a3dd8a0",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"overview\"></a>1. Overview of Large-Scale Transformer Training\n",
    "\n",
    "Modern large language models (LLMs) like **GPT-3**, **PaLM**, or **LLaMA** are trained on **massive** corpora (hundreds of billions of tokens). The **key** to training such models is **efficiently** parallelizing computations and effectively handling huge parameter counts.\n",
    "\n",
    "**Main Challenges**:\n",
    "1. **Memory**: Single GPU cannot store billions of parameters plus intermediate activations.  \n",
    "2. **Speed**: Large-scale training can take weeks or months if not parallelized.  \n",
    "3. **Cost**: HPC resources must be used effectively; inefficient code can skyrocket costs.\n",
    "\n",
    "**High-Level Solutions**:\n",
    "- **Distributed Training**: multiple GPUs, multiple nodes.  \n",
    "- **Parallelism**: data parallel, model parallel, pipeline parallel.  \n",
    "- **Mixed Precision**: FP16 or BF16 to reduce memory usage.  \n",
    "- **Advanced Libraries**: **PyTorch Lightning**, **Hugging Face Transformers/Accelerate**, **DeepSpeed** or **Megatron-LM** for parallelism, memory optimization, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf87b78-3054-4687-ad72-b86e42cfbd21",
   "metadata": {},
   "source": [
    "## <a id=\"distributed\"></a>2. Distributed Training Approaches\n",
    "\n",
    "### <a id=\"data-parallel\"></a>2.1 Data Parallelism\n",
    "\n",
    "- **Idea**: Replicate the model on each GPU, split the dataset (or mini-batch) across GPUs.  \n",
    "- **Parameter sync**: after computing gradients locally, an all-reduce operation merges them so each GPU keeps consistent weights.\n",
    "\n",
    "**Pros**:\n",
    "- Easiest approach to scale if the model fits in a single GPU.  \n",
    "- Well-supported by frameworks (`torch.nn.DataParallel`, `torch.distributed`, or `Lightning` built-in).\n",
    "\n",
    "**Cons**:\n",
    "- Model size is limited by single-GPU memory.  \n",
    "- Increasing the number of GPUs mostly helps handle bigger batch sizes or speed up the training, but doesn’t solve “model too big for one GPU” problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec30bd8-e85f-4a1a-933b-9a3ffea104ee",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"model-parallel\"></a>2.2 Model Parallelism / Pipeline Parallelism\n",
    "\n",
    "When the model **exceeds** a single GPU’s memory:\n",
    "\n",
    "1. **Model Parallelism**: split model layers or parameters across multiple GPUs.  \n",
    "   - *Tensor Parallelism*: e.g., Megatron-LM splits large matrix multiplications across GPUs.  \n",
    "   - *Layer Parallelism*: each GPU holds a subset of layers.\n",
    "\n",
    "2. **Pipeline Parallelism**: chain layers in a “pipeline” across GPUs:\n",
    "   - GPU0: processes micro-batch 1 on first stage (layers 1–N).\n",
    "   - GPU1: processes micro-batch 1 on next stage, while GPU0 processes micro-batch 2.\n",
    "\n",
    "**Pros**:\n",
    "- Overcomes single-GPU memory constraints.  \n",
    "- Pipeline approach can keep multiple GPUs busy simultaneously.\n",
    "\n",
    "**Cons**:\n",
    "- More complicated to implement than data parallel.  \n",
    "- Potential for “bubble” inefficiencies (waiting for the next stage to finish).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8bf5e0-463e-4547-aafb-04c2b8cf7441",
   "metadata": {},
   "source": [
    "### <a id=\"sharding\"></a>2.3 Sharding (ZeRO, Megatron, etc.)\n",
    "\n",
    "**Sharding** further splits not just layers but also **optimizer states, gradients, parameters** across GPUs. Popular frameworks:\n",
    "\n",
    "- **DeepSpeed ZeRO**: partitions optimizer states, gradients, and parameters across multiple processes.  \n",
    "- **Megatron-LM**: focuses on **tensor parallel** approaches for large matrix multiplications.  \n",
    "- **Fully Sharded Data Parallel (FSDP)** in PyTorch: shards parameters/gradients and consolidates them as needed.\n",
    "\n",
    "**Benefit**: Efficient usage of memory, allowing extremely large models to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2fc75a-608f-41b5-a495-182e28361fb2",
   "metadata": {},
   "source": [
    "## <a id=\"mixed-precision\"></a>3. Mixed Precision Training\n",
    "\n",
    "Large models can benefit greatly from **FP16** or **BF16**:\n",
    "\n",
    "- **Faster** computations using GPU tensor cores.\n",
    "- **Less** memory usage for activations and gradients.\n",
    "\n",
    "### <a id=\"fp16-bf16\"></a>3.1 FP16, BF16, and FP8 Overview\n",
    "\n",
    "1. **FP16 (Half Precision)**:\n",
    "   - 16-bit floating point.  \n",
    "   - Must handle potential **loss of precision** carefully (gradient scaling).\n",
    "2. **BF16 (Brain Float 16)**:\n",
    "   - 16-bit, but larger exponent range.  \n",
    "   - Often doesn’t require explicit gradient scaling.  \n",
    "   - Widely used on Google TPUs and some newer GPUs.\n",
    "3. **FP8**:\n",
    "   - 8-bit float.  \n",
    "   - Experimental, used in cutting-edge research for even more memory savings, with certain hardware (e.g., Hopper GPUs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c0b3f-3d5d-4f2f-acb4-f3c774768fa5",
   "metadata": {},
   "source": [
    "### <a id=\"mp-implementation\"></a>3.2 Practical Implementation in PyTorch/Hugging Face\n",
    "\n",
    "**PyTorch**:\n",
    "\n",
    "```python\n",
    "...\n",
    "scaler = torch.cuda.amp.GradScaler()  # For automatic gradient scaling\n",
    "\n",
    "for x_batch, y_batch in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        logits = model(x_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "```\n",
    "\n",
    "**Hugging Face Transformers**:\n",
    "- Use `Trainer` with `fp16=True` or `bf16=True` in the training arguments.\n",
    "\n",
    "**PyTorch Lightning**:\n",
    "\n",
    "```python\n",
    "...\n",
    "trainer = pl.Trainer(\n",
    "    precision=16,   # for FP16\n",
    "    gpus=2,         # data parallel on 2 GPUs\n",
    "    ...\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5632699-c5ba-48ef-b826-9efbf0e43ab7",
   "metadata": {},
   "source": [
    "## <a id=\"finetuning\"></a>4. Fine-Tuning on Downstream Tasks\n",
    "\n",
    "### <a id=\"task-adapt\"></a>4.1 Task-Adaptive Pretraining vs. Fine-Tuning\n",
    "\n",
    "1. **Task-Adaptive Pretraining**:  \n",
    "   - You continue large-scale language modeling on domain-specific data (e.g., biomedical text) to adapt your general LLM.  \n",
    "2. **Fine-Tuning**:  \n",
    "   - On a specific supervised dataset (classification, QA, summarization), you update weights while focusing on the new objective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e97f36-d606-4051-ac1e-7d7326f7386f",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"hf-transformers\"></a>4.2 Using Hugging Face Transformers\n",
    "\n",
    "Hugging Face provides:\n",
    "\n",
    "- **Pretrained models**: GPT, BERT, T5, etc.\n",
    "- **Trainer API** for easy fine-tuning with built-in features (mixed precision, distributed, etc.).\n",
    "\n",
    "**Example** (fine-tuning a GPT-2 on domain text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6553f14-f060-47a4-b79e-14e926497cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:03, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=2.8256507449679904, metrics={'train_runtime': 3.7269, 'train_samples_per_second': 80.496, 'train_steps_per_second': 9.66, 'total_flos': 6001551360000.0, 'train_loss': 2.8256507449679904, 'epoch': 2.8})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# %pip install -q transformers accelerate\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Suppose we have a custom dataset (your domain text)...\n",
    "class DummyDataset(torch.utils.data.Dataset):\n",
    "    def __len__(self):\n",
    "        return 100\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.arange(idx, idx+42),\n",
    "            \"label_ids\": torch.arange(idx+1, idx+43),\n",
    "        }\n",
    "train_dataset = DummyDataset()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,                        # Mixed precision\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # your custom dataset\n",
    "    # eval_dataset=valid_dataset,   # optional\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10698ab-1058-4a30-b215-61a72ae1661f",
   "metadata": {},
   "source": [
    "**Highlight**: \n",
    "- Setting `fp16=True` automatically enables half-precision.  \n",
    "- `gradient_accumulation_steps` effectively increases the batch size if memory is limited.  \n",
    "- Under the hood, **Accelerate** can handle multi-GPU or multi-node setups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091e6621-8acf-4935-8209-708264950b20",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"lightning-example\"></a>4.3 Example: Fine-Tuning GPT-2 with PyTorch Lightning\n",
    "\n",
    "**Benefits** of Lightning:\n",
    "- Built-in support for multi-GPU, checkpointing, logging, etc.\n",
    "- Precision=16 => automatic mixed precision on GPUs.\n",
    "\n",
    "**PyTorch Lightning** abstracts away boilerplate training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa170df9-2b75-4976-9148-b471adefa436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type            | Params | Mode\n",
      "-------------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 124 M  | eval\n",
      "-------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "497.759   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "164       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20a944d35a749389164fa07f93222f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pytorch-lightning transformers\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "class GPT2FineTuner(pl.LightningModule):\n",
    "    def __init__(self, model_name=\"gpt2\", lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, input_ids, label_ids=None, attention_mask=None):\n",
    "        if label_ids is None:\n",
    "            label_ids = input_ids\n",
    "        return self.model(input_ids, attention_mask=attention_mask, labels=label_ids)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self(**batch)  # GPT2 outputs crossentropy if labels are provided\n",
    "        self.log(\"train_loss\", output.loss)\n",
    "        return output.loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "# Then define a DataModule or Dataloader for your domain dataset\n",
    "class DummyDataModule(pl.LightningDataModule):\n",
    "    # https://lightning.ai/docs/pytorch/stable/data/datamodule.html#what-is-a-datamodule\n",
    "    def __init__(self, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(DummyDataset(), batch_size=self.batch_size)\n",
    "\n",
    "domain_data_module = DummyDataModule()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3, \n",
    "    precision=16,     # FP16\n",
    "    devices=2 if torch.cuda.device_count() >= 2 else 1,  # multi-GPU if available\n",
    "    gradient_clip_val=1.0,\n",
    ")\n",
    "model_module = GPT2FineTuner()\n",
    "trainer.fit(model_module, domain_data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea33220-99d1-440e-a7ca-b6ff290e3311",
   "metadata": {},
   "source": [
    "## <a id=\"pipeline\"></a>5. Putting It All Together – A Scalable Pretraining/Fine-Tuning Pipeline\n",
    "\n",
    "### Step-by-Step (Conceptual)\n",
    "\n",
    "1. **Gather Large Corpus** (billions of tokens) => Shard it into multiple files for streaming.  \n",
    "2. **Tokenizer**: Possibly BPE/WordPiece to build a subword vocabulary, or use a known pretrained tokenizer.  \n",
    "3. **Initialize** your Transformer model or load a partial checkpoint (like GPT-2 medium).  \n",
    "4. **Distributed Setup**:\n",
    "   - `torch.distributed.launch` or **Accelerate** CLI to spawn multiple workers.  \n",
    "   - Decide if you need **data parallel** or **model/pipeline parallel** (for extremely large models).  \n",
    "5. **Mixed Precision** to reduce memory usage (FP16/BF16) with gradient scaling.  \n",
    "6. **Train**: \n",
    "   - Use *gradient accumulation* if you can’t fit your desired batch size per device.  \n",
    "   - Monitor *loss*, *perplexity*, etc.  \n",
    "7. **Checkpointing**: Save model weights periodically, especially crucial in HPC environments.  \n",
    "8. **Fine-Tuning**:\n",
    "   - Switch to your domain or task data. Possibly freeze some layers or do full fine-tuning.  \n",
    "   - Evaluate with appropriate metrics (e.g., perplexity, BLEU, ROUGE, etc.).  \n",
    "9. **Inference**: \n",
    "   - For generation tasks, apply sampling strategies (temperature, top-k).  \n",
    "   - Use high-level libraries (Transformers `pipeline` or custom code) for final deployment.\n",
    "\n",
    "**Libraries** that ease the pipeline:\n",
    "- **DeepSpeed** / **Megatron-LM**: advanced distributed/memory optimization.  \n",
    "- **PyTorch Lightning**: structure your training loops with minimal boilerplate.  \n",
    "- **Hugging Face Accelerate**: easily convert your single-GPU script to multi-GPU/multi-node.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea1c6b-36bb-445d-b2d9-6797a9da7050",
   "metadata": {},
   "source": [
    "## <a id=\"exercises\"></a>6. Practical Exercises\n",
    "\n",
    "1. **Scalable Pretraining Design (Conceptual)**  \n",
    "   - Outline a training plan for a 1–2 billion parameter model on 8 GPUs.  \n",
    "   - Specify how you’d shard the dataset, which parallelism approach you’d use, and how to incorporate mixed precision.\n",
    "\n",
    "2. **Fine-Tuning GPT-2**  \n",
    "   - Take a small domain dataset (could be a subset of domain-specific text).  \n",
    "   - Fine-tune GPT-2 using either the **Hugging Face Trainer** or **PyTorch Lightning**.  \n",
    "   - Evaluate the generated text vs. the original GPT-2 output.\n",
    "\n",
    "3. **Experiment with Parallelism**  \n",
    "   - (If you have multiple GPUs) attempt a minimal data parallel run using PyTorch Lightning or Accelerate.  \n",
    "   - Observe the speed-up and memory usage changes.  \n",
    "   - Discuss how you’d handle a bigger model that doesn’t fit on one GPU.\n",
    "\n",
    "4. **Mixed Precision Pitfalls**  \n",
    "   - Write a short note on potential issues with FP16 (e.g., overflow, underflow, gradient scaling).  \n",
    "   - How might BF16 mitigate some of these problems?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a41b4f-6d7b-4ed4-b6eb-bb6d92d28f97",
   "metadata": {},
   "source": [
    "## <a id=\"conclusion\"></a>7. Conclusion\n",
    "\n",
    "**Recap** of Session 4, Part 2:\n",
    "- **Large-scale Transformer training** requires advanced **parallelism** (data, model, pipeline) and memory optimization.  \n",
    "- **Mixed precision** (FP16, BF16) is a standard approach to speed up training and reduce memory usage.  \n",
    "- **Fine-tuning** large pretrained models is often more efficient than training from scratch; *domain adaptation* and *task-specific heads* can drastically improve performance.\n",
    "- **Third-party APIs**—like **Hugging Face Transformers/Accelerate** or **PyTorch Lightning**—provide robust solutions for distributed training, checkpointing, logging, and more, saving developers from writing low-level distributed code.\n",
    "\n",
    "**Key Takeaways**:\n",
    "- Deciding on a parallel strategy (data parallel vs. model parallel vs. pipeline parallel) depends on model size and hardware resources.  \n",
    "- Tools such as **Accelerate** and **Lightning** simplify multi-GPU setups and mix precision usage.  \n",
    "- Fine-tuning modern LLMs can yield strong results on domain tasks without incurring massive training costs from scratch.\n",
    "\n",
    "**Next Steps**:\n",
    "- Explore advanced optimization libraries like **DeepSpeed** or **FSDP** in PyTorch for extreme model scales.  \n",
    "- Try actual HPC or cloud environments to test your pipeline.  \n",
    "- Investigate advanced topics like **sparsity**, **quantization**, or **RLHF** for large language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9d04f-1955-4582-a80e-d778aea35f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
